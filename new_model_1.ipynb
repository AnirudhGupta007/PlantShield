{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "import wandb\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Resize, Normalize, Compose\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or Compose([\n",
    "            Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.images, self.captions = self.load_dataset()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def load_dataset(self):\n",
    "        images = []\n",
    "        captions = []\n",
    "        for class_dir in os.listdir(self.root_dir):\n",
    "            class_path = os.path.join(self.root_dir, class_dir)\n",
    "            for image_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, image_name)\n",
    "                caption = self.get_caption(class_dir, image_name)\n",
    "                images.append(img_path)\n",
    "                captions.append(caption)\n",
    "        return images, captions\n",
    "\n",
    "    def get_caption(self, class_dir, image_name):\n",
    "        if class_dir == \"_BrownSpot\":\n",
    "            return \"Rice plant with brown spot disease\"\n",
    "        elif class_dir == \"_Hispa\":\n",
    "            return \"Rice plant with Hispa disease\"\n",
    "        elif class_dir == \"_LeafBlast\":\n",
    "            return \"Rice plant with Leaf Blast disease\"\n",
    "        else:\n",
    "            return \"Healthy Rice Plant\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        caption = self.captions[idx]\n",
    "        input_ids = self.tokenizer.encode(caption, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
    "        return img, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Ensure input_ids and attention_mask are 2D\n",
    "        if input_ids.dim() == 3:\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "        if attention_mask.dim() == 3:\n",
    "            attention_mask = attention_mask.squeeze(1)\n",
    "        \n",
    "        output = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return output.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, text_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.init_size = 8  # Initial size of feature maps (256 = 8 * 2^5)\n",
    "        self.l1 = nn.Linear(latent_dim + text_dim, 1024 * self.init_size ** 2)\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(1024, 512, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        input_tensor = torch.cat([noise, text], dim=1)\n",
    "        out = self.l1(input_tensor)\n",
    "        out = out.view(out.shape[0], 1024, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, text_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "        self.image_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "        \n",
    "        # Calculate the output size of the image_model\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 256, 256)\n",
    "            dummy_output = self.image_model(dummy_input)\n",
    "            self.image_features_size = dummy_output.view(1, -1).size(1)\n",
    "        \n",
    "        print(f\"Image features size: {self.image_features_size}\")\n",
    "        \n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(self.image_features_size + text_dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        if input_ids.dim() == 3:\n",
    "            input_ids = input_ids.squeeze(1)\n",
    "        if attention_mask.dim() == 3:\n",
    "            attention_mask = attention_mask.squeeze(1)\n",
    "        \n",
    "        text_features = self.text_encoder(input_ids, attention_mask)\n",
    "        \n",
    "        image_features = self.image_model(image)\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "        \n",
    "        fused_features = torch.cat([image_features, text_features], dim=1)\n",
    "        \n",
    "        validity = self.fusion_layer(fused_features)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "text_dim = 768\n",
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features size: 16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (text_encoder): TextEncoder(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (image_model): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): Dropout2d(p=0.25, inplace=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (10): Dropout2d(p=0.25, inplace=False)\n",
       "    (11): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (14): Dropout2d(p=0.25, inplace=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (18): Dropout2d(p=0.25, inplace=False)\n",
       "    (19): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (20): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (22): Dropout2d(p=0.25, inplace=False)\n",
       "  )\n",
       "  (fusion_layer): Sequential(\n",
       "    (0): Linear(in_features=17152, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout(p=0.4, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(latent_dim, text_dim)\n",
    "discriminator = Discriminator(text_dim)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator.to(device)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image if needed\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RiceDataset(r\"D:\\GuruGobindSinghIndrapasthaUniversity-Jatin\\harjot_data\\rice_images\\rice_images\")\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, input_ids, attention_mask):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # Ensure input_ids and attention_mask are 2D\n",
    "    if input_ids.dim() == 3:\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "    if attention_mask.dim() == 3:\n",
    "        attention_mask = attention_mask.squeeze(1)\n",
    "\n",
    "    # Train with real images\n",
    "    real_validity = discriminator(real_images, input_ids, attention_mask)\n",
    "    d_real_loss = criterion(real_validity, torch.ones_like(real_validity))\n",
    "\n",
    "    # Train with fake images\n",
    "    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "    text_embedding = discriminator.text_encoder(input_ids, attention_mask)\n",
    "    fake_images = generator(z, text_embedding)\n",
    "    fake_validity = discriminator(fake_images.detach(), input_ids, attention_mask)\n",
    "    d_fake_loss = criterion(fake_validity, torch.zeros_like(fake_validity))\n",
    "\n",
    "    # Total discriminator loss\n",
    "    d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "    return d_loss.item()\n",
    "\n",
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion, input_ids, attention_mask):\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "    # Ensure input_ids and attention_mask are 2D\n",
    "    if input_ids.dim() == 3:\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "    if attention_mask.dim() == 3:\n",
    "        attention_mask = attention_mask.squeeze(1)\n",
    "\n",
    "    # Generate fake images\n",
    "    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "    text_embedding = discriminator.text_encoder(input_ids, attention_mask)\n",
    "    fake_images = generator(z, text_embedding)\n",
    "\n",
    "    # Try to fool the discriminator\n",
    "    validity = discriminator(fake_images, input_ids, attention_mask)\n",
    "    g_loss = criterion(validity, torch.ones_like(validity))\n",
    "\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    return g_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"saved_models_1\", exist_ok=True)\n",
    "os.makedirs(\"generated_images_1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "def save_models(epoch, generator, discriminator):\n",
    "    torch.save(generator.state_dict(), f\"saved_models/generator_epoch_{epoch}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"saved_models/discriminator_epoch_{epoch}.pth\")\n",
    "    print(f\"Models saved for epoch {epoch}\")\n",
    "\n",
    "def log_metrics(epoch, d_loss, g_loss):\n",
    "    with open('training_log.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch, d_loss, g_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for batch_idx, (real_images, input_ids, attention_mask) in enumerate(train_loader):\n",
    "        real_images, input_ids, attention_mask = real_images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        d_loss = discriminator_train_step(real_images.size(0), discriminator, generator, d_optimizer, criterion, real_images, input_ids, attention_mask)\n",
    "        g_loss = generator_train_step(real_images.size(0), discriminator, generator, g_optimizer, criterion, input_ids, attention_mask)\n",
    "\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        if (batch_idx + 1) % 2== 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], D_loss: {d_loss:.4f}, G_loss: {g_loss:.4f}\")\n",
    "\n",
    "    avg_d_loss = sum(d_losses) / len(d_losses)\n",
    "    avg_g_loss = sum(g_losses) / len(g_losses)\n",
    "    log_metrics(epoch + 1, avg_d_loss, avg_g_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg D_loss: {avg_d_loss:.4f}, Avg G_loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "    # Generate and save images after every 5 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            batch_size = 64  # Or whatever your current batch size is\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            \n",
    "            # Create a batch of the same text\n",
    "            text = [\"A healthy rice plant\"] * batch_size\n",
    "            input_ids = dataset.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt').input_ids.to(device)\n",
    "            attention_mask = (input_ids != dataset.tokenizer.pad_token_id).float().to(device)\n",
    "            \n",
    "            text_embedding = discriminator.text_encoder(input_ids, attention_mask)\n",
    "            generated_images = generator(z, text_embedding)\n",
    "            save_image(generated_images, f\"generated_images/epoch_{epoch+1}.png\", nrow=8, normalize=True)\n",
    "            print(f\"Generated images saved as generated_images/epoch_{epoch+1}.png\")\n",
    "\n",
    "    # Save models every 10 epochs\n",
    "    if (epoch + 1) % 50  == 0:\n",
    "        save_models(epoch + 1, generator, discriminator)\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_text(text, filename):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = dataset.tokenizer.encode(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        attention_mask = (input_ids != dataset.tokenizer.pad_token_id).float().to(device)\n",
    "        z = torch.randn(1, latent_dim, device=device)\n",
    "        text_embedding = discriminator.text_encoder(input_ids, attention_mask)\n",
    "        generated_image = generator(z, text_embedding)\n",
    "        save_image(generated_image, f\"generated_images/{filename}.png\", normalize=True)\n",
    "\n",
    "# Example usage\n",
    "generate_image_from_text(\"A healthy rice plant\", \"healthy_rice_plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(generator, discriminator, epoch):\n",
    "    generator.load_state_dict(torch.load(f\"saved_models/generator_epoch_{epoch}.pth\"))\n",
    "    discriminator.load_state_dict(torch.load(f\"saved_models/discriminator_epoch_{epoch}.pth\"))\n",
    "    print(f\"Models loaded from epoch {epoch}\")\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded from epoch 300\n",
      "Epoch [301/800], Step [2/53], D_loss: 0.0032, G_loss: 6.1806\n",
      "Epoch [301/800], Step [4/53], D_loss: 0.0116, G_loss: 6.0997\n",
      "Epoch [301/800], Step [6/53], D_loss: 0.0758, G_loss: 6.7526\n",
      "Epoch [301/800], Step [8/53], D_loss: 0.3016, G_loss: 8.6380\n",
      "Epoch [301/800], Step [10/53], D_loss: 0.1457, G_loss: 8.1033\n",
      "Epoch [301/800], Step [12/53], D_loss: 0.0762, G_loss: 7.1117\n",
      "Epoch [301/800], Step [14/53], D_loss: 0.0200, G_loss: 6.8384\n",
      "Epoch [301/800], Step [16/53], D_loss: 0.0085, G_loss: 8.7462\n",
      "Epoch [301/800], Step [18/53], D_loss: 0.0246, G_loss: 6.0262\n",
      "Epoch [301/800], Step [20/53], D_loss: 0.0467, G_loss: 9.0268\n",
      "Epoch [301/800], Step [22/53], D_loss: 0.0012, G_loss: 9.3924\n",
      "Epoch [301/800], Step [24/53], D_loss: 0.0020, G_loss: 8.3505\n",
      "Epoch [301/800], Step [26/53], D_loss: 0.0224, G_loss: 7.4477\n",
      "Epoch [301/800], Step [28/53], D_loss: 0.0225, G_loss: 12.0038\n",
      "Epoch [301/800], Step [30/53], D_loss: 0.0111, G_loss: 1.8972\n",
      "Epoch [301/800], Step [32/53], D_loss: 0.1640, G_loss: 15.2108\n",
      "Epoch [301/800], Step [34/53], D_loss: 0.0884, G_loss: 3.7082\n",
      "Epoch [301/800], Step [36/53], D_loss: 0.0828, G_loss: 3.3963\n",
      "Epoch [301/800], Step [38/53], D_loss: 0.0033, G_loss: 7.1077\n",
      "Epoch [301/800], Step [40/53], D_loss: 0.0063, G_loss: 4.8660\n",
      "Epoch [301/800], Step [42/53], D_loss: 0.0083, G_loss: 5.4024\n",
      "Epoch [301/800], Step [44/53], D_loss: 0.2357, G_loss: 6.1747\n",
      "Epoch [301/800], Step [46/53], D_loss: 0.0117, G_loss: 6.8436\n",
      "Epoch [301/800], Step [48/53], D_loss: 0.0372, G_loss: 7.1328\n",
      "Epoch [301/800], Step [50/53], D_loss: 0.0273, G_loss: 6.9807\n",
      "Epoch [301/800], Step [52/53], D_loss: 0.0079, G_loss: 6.6615\n",
      "Epoch [301/800], Avg D_loss: 0.0702, Avg G_loss: 7.1514\n",
      "Epoch [302/800], Step [2/53], D_loss: 2.6821, G_loss: 16.8087\n",
      "Epoch [302/800], Step [4/53], D_loss: 2.9638, G_loss: 3.3764\n",
      "Epoch [302/800], Step [6/53], D_loss: 0.2614, G_loss: 1.1716\n",
      "Epoch [302/800], Step [8/53], D_loss: 0.1190, G_loss: 2.6913\n",
      "Epoch [302/800], Step [10/53], D_loss: 0.2314, G_loss: 2.6075\n",
      "Epoch [302/800], Step [12/53], D_loss: 0.1486, G_loss: 3.1076\n",
      "Epoch [302/800], Step [14/53], D_loss: 0.3810, G_loss: 2.3128\n",
      "Epoch [302/800], Step [16/53], D_loss: 0.1286, G_loss: 3.4091\n",
      "Epoch [302/800], Step [18/53], D_loss: 0.1780, G_loss: 1.7537\n",
      "Epoch [302/800], Step [20/53], D_loss: 0.1825, G_loss: 3.3951\n",
      "Epoch [302/800], Step [22/53], D_loss: 0.3488, G_loss: 2.3220\n",
      "Epoch [302/800], Step [24/53], D_loss: 0.1020, G_loss: 4.3544\n",
      "Epoch [302/800], Step [26/53], D_loss: 0.0545, G_loss: 3.2089\n",
      "Epoch [302/800], Step [28/53], D_loss: 0.0780, G_loss: 3.1318\n",
      "Epoch [302/800], Step [30/53], D_loss: 0.2960, G_loss: 3.1285\n",
      "Epoch [302/800], Step [32/53], D_loss: 0.3312, G_loss: 2.3118\n",
      "Epoch [302/800], Step [34/53], D_loss: 0.3192, G_loss: 5.6735\n",
      "Epoch [302/800], Step [36/53], D_loss: 0.2255, G_loss: 2.5942\n",
      "Epoch [302/800], Step [38/53], D_loss: 0.2505, G_loss: 3.4154\n",
      "Epoch [302/800], Step [40/53], D_loss: 0.1070, G_loss: 4.0171\n",
      "Epoch [302/800], Step [42/53], D_loss: 0.1111, G_loss: 2.0608\n",
      "Epoch [302/800], Step [44/53], D_loss: 0.0860, G_loss: 2.6686\n",
      "Epoch [302/800], Step [46/53], D_loss: 0.1055, G_loss: 5.2786\n",
      "Epoch [302/800], Step [48/53], D_loss: 0.4016, G_loss: 3.3657\n",
      "Epoch [302/800], Step [50/53], D_loss: 0.0587, G_loss: 4.1219\n",
      "Epoch [302/800], Step [52/53], D_loss: 0.0585, G_loss: 4.4662\n",
      "Epoch [302/800], Avg D_loss: 0.3253, Avg G_loss: 3.7971\n",
      "Epoch [303/800], Step [2/53], D_loss: 0.1671, G_loss: 3.2541\n",
      "Epoch [303/800], Step [4/53], D_loss: 0.2730, G_loss: 7.9344\n",
      "Epoch [303/800], Step [6/53], D_loss: 0.4924, G_loss: 3.3055\n",
      "Epoch [303/800], Step [8/53], D_loss: 0.2851, G_loss: 3.1679\n",
      "Epoch [303/800], Step [10/53], D_loss: 0.0570, G_loss: 5.1709\n",
      "Epoch [303/800], Step [12/53], D_loss: 0.0525, G_loss: 6.1199\n",
      "Epoch [303/800], Step [14/53], D_loss: 0.0443, G_loss: 3.5354\n",
      "Epoch [303/800], Step [16/53], D_loss: 0.0285, G_loss: 2.3797\n",
      "Epoch [303/800], Step [18/53], D_loss: 0.0243, G_loss: 3.8247\n",
      "Epoch [303/800], Step [20/53], D_loss: 0.0143, G_loss: 5.0769\n",
      "Epoch [303/800], Step [22/53], D_loss: 0.0693, G_loss: 3.5261\n",
      "Epoch [303/800], Step [24/53], D_loss: 0.3164, G_loss: 8.1345\n",
      "Epoch [303/800], Step [26/53], D_loss: 0.0977, G_loss: 2.2228\n",
      "Epoch [303/800], Step [28/53], D_loss: 1.2846, G_loss: 9.3567\n",
      "Epoch [303/800], Step [30/53], D_loss: 0.3768, G_loss: 4.2307\n",
      "Epoch [303/800], Step [32/53], D_loss: 0.0466, G_loss: 2.8011\n",
      "Epoch [303/800], Step [34/53], D_loss: 0.0574, G_loss: 2.7252\n",
      "Epoch [303/800], Step [36/53], D_loss: 0.1556, G_loss: 5.8538\n",
      "Epoch [303/800], Step [38/53], D_loss: 0.2175, G_loss: 2.5759\n",
      "Epoch [303/800], Step [40/53], D_loss: 0.0392, G_loss: 2.2292\n",
      "Epoch [303/800], Step [42/53], D_loss: 0.2918, G_loss: 4.9655\n",
      "Epoch [303/800], Step [44/53], D_loss: 0.0499, G_loss: 3.9175\n",
      "Epoch [303/800], Step [46/53], D_loss: 0.0329, G_loss: 4.6912\n",
      "Epoch [303/800], Step [48/53], D_loss: 0.0623, G_loss: 3.1087\n",
      "Epoch [303/800], Step [50/53], D_loss: 0.1591, G_loss: 3.2798\n",
      "Epoch [303/800], Step [52/53], D_loss: 0.1472, G_loss: 5.1061\n",
      "Epoch [303/800], Avg D_loss: 0.1745, Avg G_loss: 4.5159\n",
      "Epoch [304/800], Step [2/53], D_loss: 0.2144, G_loss: 2.9214\n",
      "Epoch [304/800], Step [4/53], D_loss: 0.0628, G_loss: 5.1597\n",
      "Epoch [304/800], Step [6/53], D_loss: 0.0206, G_loss: 3.5368\n",
      "Epoch [304/800], Step [8/53], D_loss: 0.0450, G_loss: 5.7675\n",
      "Epoch [304/800], Step [10/53], D_loss: 0.0198, G_loss: 3.6556\n",
      "Epoch [304/800], Step [12/53], D_loss: 0.0133, G_loss: 7.1118\n",
      "Epoch [304/800], Step [14/53], D_loss: 0.0278, G_loss: 3.1061\n",
      "Epoch [304/800], Step [16/53], D_loss: 0.3175, G_loss: 7.6853\n",
      "Epoch [304/800], Step [18/53], D_loss: 0.0999, G_loss: 5.7120\n",
      "Epoch [304/800], Step [20/53], D_loss: 0.1697, G_loss: 4.0379\n",
      "Epoch [304/800], Step [22/53], D_loss: 0.0178, G_loss: 7.4451\n",
      "Epoch [304/800], Step [24/53], D_loss: 0.7383, G_loss: 6.9215\n",
      "Epoch [304/800], Step [26/53], D_loss: 0.1337, G_loss: 4.6973\n",
      "Epoch [304/800], Step [28/53], D_loss: 0.0830, G_loss: 4.4336\n",
      "Epoch [304/800], Step [30/53], D_loss: 0.1391, G_loss: 3.9866\n",
      "Epoch [304/800], Step [32/53], D_loss: 0.0514, G_loss: 6.3745\n",
      "Epoch [304/800], Step [34/53], D_loss: 0.0456, G_loss: 4.0764\n",
      "Epoch [304/800], Step [36/53], D_loss: 0.0779, G_loss: 3.8162\n",
      "Epoch [304/800], Step [38/53], D_loss: 0.0956, G_loss: 6.3262\n",
      "Epoch [304/800], Step [40/53], D_loss: 0.1874, G_loss: 3.3026\n",
      "Epoch [304/800], Step [42/53], D_loss: 0.2800, G_loss: 6.9297\n",
      "Epoch [304/800], Step [44/53], D_loss: 0.0493, G_loss: 9.0004\n",
      "Epoch [304/800], Step [46/53], D_loss: 0.0138, G_loss: 8.2525\n",
      "Epoch [304/800], Step [48/53], D_loss: 0.0292, G_loss: 2.0423\n",
      "Epoch [304/800], Step [50/53], D_loss: 0.2409, G_loss: 3.1950\n",
      "Epoch [304/800], Step [52/53], D_loss: 0.0271, G_loss: 4.3769\n",
      "Epoch [304/800], Avg D_loss: 0.1244, Avg G_loss: 5.0391\n",
      "Epoch [305/800], Step [2/53], D_loss: 0.0240, G_loss: 4.8820\n",
      "Epoch [305/800], Step [4/53], D_loss: 0.0087, G_loss: 4.1040\n",
      "Epoch [305/800], Step [6/53], D_loss: 0.1298, G_loss: 4.6605\n",
      "Epoch [305/800], Step [8/53], D_loss: 0.0903, G_loss: 4.2691\n",
      "Epoch [305/800], Step [10/53], D_loss: 0.0380, G_loss: 2.2052\n",
      "Epoch [305/800], Step [12/53], D_loss: 0.0172, G_loss: 5.7136\n",
      "Epoch [305/800], Step [14/53], D_loss: 0.0069, G_loss: 5.4497\n",
      "Epoch [305/800], Step [16/53], D_loss: 0.0202, G_loss: 6.5394\n",
      "Epoch [305/800], Step [18/53], D_loss: 0.0071, G_loss: 5.9893\n",
      "Epoch [305/800], Step [20/53], D_loss: 0.0135, G_loss: 4.0434\n",
      "Epoch [305/800], Step [22/53], D_loss: 0.0263, G_loss: 6.3385\n",
      "Epoch [305/800], Step [24/53], D_loss: 0.0817, G_loss: 7.9200\n",
      "Epoch [305/800], Step [26/53], D_loss: 0.0057, G_loss: 5.7272\n",
      "Epoch [305/800], Step [28/53], D_loss: 0.0725, G_loss: 7.6318\n",
      "Epoch [305/800], Step [30/53], D_loss: 0.0203, G_loss: 7.6330\n",
      "Epoch [305/800], Step [32/53], D_loss: 0.0746, G_loss: 7.6278\n",
      "Epoch [305/800], Step [34/53], D_loss: 0.0137, G_loss: 7.5033\n",
      "Epoch [305/800], Step [36/53], D_loss: 0.0118, G_loss: 7.9971\n",
      "Epoch [305/800], Step [38/53], D_loss: 0.1781, G_loss: 6.0916\n",
      "Epoch [305/800], Step [40/53], D_loss: 0.2129, G_loss: 6.9547\n",
      "Epoch [305/800], Step [42/53], D_loss: 0.3443, G_loss: 5.5609\n",
      "Epoch [305/800], Step [44/53], D_loss: 0.2202, G_loss: 7.6261\n",
      "Epoch [305/800], Step [46/53], D_loss: 0.0018, G_loss: 4.4290\n",
      "Epoch [305/800], Step [48/53], D_loss: 0.0313, G_loss: 3.6391\n",
      "Epoch [305/800], Step [50/53], D_loss: 0.0057, G_loss: 5.5864\n",
      "Epoch [305/800], Step [52/53], D_loss: 0.3672, G_loss: 11.9034\n",
      "Epoch [305/800], Avg D_loss: 0.0930, Avg G_loss: 5.9364\n",
      "Epoch [306/800], Step [2/53], D_loss: 0.4193, G_loss: 5.7982\n",
      "Epoch [306/800], Step [4/53], D_loss: 0.3019, G_loss: 4.3157\n",
      "Epoch [306/800], Step [6/53], D_loss: 0.0756, G_loss: 5.1842\n",
      "Epoch [306/800], Step [8/53], D_loss: 0.1056, G_loss: 4.6513\n",
      "Epoch [306/800], Step [10/53], D_loss: 0.0208, G_loss: 3.4565\n",
      "Epoch [306/800], Step [12/53], D_loss: 0.0328, G_loss: 4.5443\n",
      "Epoch [306/800], Step [14/53], D_loss: 0.0565, G_loss: 2.6660\n",
      "Epoch [306/800], Step [16/53], D_loss: 0.0115, G_loss: 7.0424\n",
      "Epoch [306/800], Step [18/53], D_loss: 0.2541, G_loss: 0.5925\n",
      "Epoch [306/800], Step [20/53], D_loss: 0.0730, G_loss: 5.4020\n",
      "Epoch [306/800], Step [22/53], D_loss: 0.0568, G_loss: 4.5929\n",
      "Epoch [306/800], Step [24/53], D_loss: 0.2525, G_loss: 8.6317\n",
      "Epoch [306/800], Step [26/53], D_loss: 0.1792, G_loss: 2.6417\n",
      "Epoch [306/800], Step [28/53], D_loss: 0.0847, G_loss: 6.0680\n",
      "Epoch [306/800], Step [30/53], D_loss: 0.0094, G_loss: 7.8140\n",
      "Epoch [306/800], Step [32/53], D_loss: 0.0571, G_loss: 3.0428\n",
      "Epoch [306/800], Step [34/53], D_loss: 0.0934, G_loss: 3.0244\n",
      "Epoch [306/800], Step [36/53], D_loss: 0.0868, G_loss: 2.4836\n",
      "Epoch [306/800], Step [38/53], D_loss: 0.0924, G_loss: 4.5065\n",
      "Epoch [306/800], Step [40/53], D_loss: 0.0486, G_loss: 4.1271\n",
      "Epoch [306/800], Step [42/53], D_loss: 0.3132, G_loss: 9.5752\n",
      "Epoch [306/800], Step [44/53], D_loss: 0.2972, G_loss: 3.2091\n",
      "Epoch [306/800], Step [46/53], D_loss: 0.1136, G_loss: 5.8184\n",
      "Epoch [306/800], Step [48/53], D_loss: 0.1025, G_loss: 1.9094\n",
      "Epoch [306/800], Step [50/53], D_loss: 0.1990, G_loss: 4.6854\n",
      "Epoch [306/800], Step [52/53], D_loss: 0.0115, G_loss: 2.3409\n",
      "Epoch [306/800], Avg D_loss: 0.1551, Avg G_loss: 4.5532\n",
      "Epoch [307/800], Step [2/53], D_loss: 0.0224, G_loss: 7.8654\n",
      "Epoch [307/800], Step [4/53], D_loss: 0.0163, G_loss: 4.3858\n",
      "Epoch [307/800], Step [6/53], D_loss: 0.2162, G_loss: 4.2703\n",
      "Epoch [307/800], Step [8/53], D_loss: 0.1008, G_loss: 3.9898\n",
      "Epoch [307/800], Step [10/53], D_loss: 0.0242, G_loss: 3.4644\n",
      "Epoch [307/800], Step [12/53], D_loss: 0.0323, G_loss: 6.5024\n",
      "Epoch [307/800], Step [14/53], D_loss: 0.1167, G_loss: 2.0596\n",
      "Epoch [307/800], Step [16/53], D_loss: 0.0156, G_loss: 5.2763\n",
      "Epoch [307/800], Step [18/53], D_loss: 0.0341, G_loss: 4.8472\n",
      "Epoch [307/800], Step [20/53], D_loss: 0.0996, G_loss: 4.5811\n",
      "Epoch [307/800], Step [22/53], D_loss: 0.0026, G_loss: 5.0498\n",
      "Epoch [307/800], Step [24/53], D_loss: 0.1044, G_loss: 4.2107\n",
      "Epoch [307/800], Step [26/53], D_loss: 0.1657, G_loss: 4.6246\n",
      "Epoch [307/800], Step [28/53], D_loss: 0.2608, G_loss: 7.6665\n",
      "Epoch [307/800], Step [30/53], D_loss: 0.1451, G_loss: 6.5393\n",
      "Epoch [307/800], Step [32/53], D_loss: 0.0139, G_loss: 4.8020\n",
      "Epoch [307/800], Step [34/53], D_loss: 0.0026, G_loss: 9.8537\n",
      "Epoch [307/800], Step [36/53], D_loss: 0.0021, G_loss: 4.3215\n",
      "Epoch [307/800], Step [38/53], D_loss: 0.1305, G_loss: 6.5754\n",
      "Epoch [307/800], Step [40/53], D_loss: 0.0139, G_loss: 1.6031\n",
      "Epoch [307/800], Step [42/53], D_loss: 0.0504, G_loss: 2.2476\n",
      "Epoch [307/800], Step [44/53], D_loss: 0.1628, G_loss: 4.1897\n",
      "Epoch [307/800], Step [46/53], D_loss: 0.1088, G_loss: 6.5193\n",
      "Epoch [307/800], Step [48/53], D_loss: 0.0142, G_loss: 8.6769\n",
      "Epoch [307/800], Step [50/53], D_loss: 0.0439, G_loss: 7.7227\n",
      "Epoch [307/800], Step [52/53], D_loss: 0.1330, G_loss: 6.7942\n",
      "Epoch [307/800], Avg D_loss: 0.0939, Avg G_loss: 5.0431\n",
      "Epoch [308/800], Step [2/53], D_loss: 1.3371, G_loss: 11.6823\n",
      "Epoch [308/800], Step [4/53], D_loss: 0.8429, G_loss: 3.4092\n",
      "Epoch [308/800], Step [6/53], D_loss: 0.0663, G_loss: 1.3146\n",
      "Epoch [308/800], Step [8/53], D_loss: 0.2101, G_loss: 3.5088\n",
      "Epoch [308/800], Step [10/53], D_loss: 0.1315, G_loss: 4.0312\n",
      "Epoch [308/800], Step [12/53], D_loss: 0.1254, G_loss: 4.3592\n",
      "Epoch [308/800], Step [14/53], D_loss: 0.1629, G_loss: 3.5602\n",
      "Epoch [308/800], Step [16/53], D_loss: 0.1198, G_loss: 2.1548\n",
      "Epoch [308/800], Step [18/53], D_loss: 0.2593, G_loss: 5.5525\n",
      "Epoch [308/800], Step [20/53], D_loss: 0.0809, G_loss: 3.6806\n",
      "Epoch [308/800], Step [22/53], D_loss: 0.1109, G_loss: 4.7996\n",
      "Epoch [308/800], Step [24/53], D_loss: 0.0889, G_loss: 5.1393\n",
      "Epoch [308/800], Step [26/53], D_loss: 0.0550, G_loss: 5.8392\n",
      "Epoch [308/800], Step [28/53], D_loss: 0.0759, G_loss: 5.3977\n",
      "Epoch [308/800], Step [30/53], D_loss: 0.0637, G_loss: 4.8888\n",
      "Epoch [308/800], Step [32/53], D_loss: 0.0103, G_loss: 4.1781\n",
      "Epoch [308/800], Step [34/53], D_loss: 0.0202, G_loss: 2.9795\n",
      "Epoch [308/800], Step [36/53], D_loss: 0.0396, G_loss: 3.8793\n",
      "Epoch [308/800], Step [38/53], D_loss: 0.0236, G_loss: 3.6295\n",
      "Epoch [308/800], Step [40/53], D_loss: 0.1865, G_loss: 6.0262\n",
      "Epoch [308/800], Step [42/53], D_loss: 0.0678, G_loss: 2.8136\n",
      "Epoch [308/800], Step [44/53], D_loss: 0.0555, G_loss: 7.2693\n",
      "Epoch [308/800], Step [46/53], D_loss: 0.0876, G_loss: 2.3002\n",
      "Epoch [308/800], Step [48/53], D_loss: 0.0524, G_loss: 8.9161\n",
      "Epoch [308/800], Step [50/53], D_loss: 0.0694, G_loss: 4.4141\n",
      "Epoch [308/800], Step [52/53], D_loss: 0.1296, G_loss: 2.8703\n",
      "Epoch [308/800], Avg D_loss: 0.1532, Avg G_loss: 4.7244\n",
      "Epoch [309/800], Step [2/53], D_loss: 0.0188, G_loss: 4.2260\n",
      "Epoch [309/800], Step [4/53], D_loss: 0.0928, G_loss: 8.7833\n",
      "Epoch [309/800], Step [6/53], D_loss: 0.0298, G_loss: 8.7259\n",
      "Epoch [309/800], Step [8/53], D_loss: 0.0398, G_loss: 7.2931\n",
      "Epoch [309/800], Step [10/53], D_loss: 0.1884, G_loss: 6.6992\n",
      "Epoch [309/800], Step [12/53], D_loss: 0.0101, G_loss: 8.5123\n",
      "Epoch [309/800], Step [14/53], D_loss: 0.0027, G_loss: 9.0987\n",
      "Epoch [309/800], Step [16/53], D_loss: 0.0186, G_loss: 4.0362\n",
      "Epoch [309/800], Step [18/53], D_loss: 0.0042, G_loss: 6.0653\n",
      "Epoch [309/800], Step [20/53], D_loss: 0.0125, G_loss: 8.0685\n",
      "Epoch [309/800], Step [22/53], D_loss: 0.3412, G_loss: 10.8951\n",
      "Epoch [309/800], Step [24/53], D_loss: 0.0275, G_loss: 9.6856\n",
      "Epoch [309/800], Step [26/53], D_loss: 0.0355, G_loss: 0.7098\n",
      "Epoch [309/800], Step [28/53], D_loss: 0.2357, G_loss: 10.1712\n",
      "Epoch [309/800], Step [30/53], D_loss: 0.1247, G_loss: 4.0995\n",
      "Epoch [309/800], Step [32/53], D_loss: 0.1515, G_loss: 4.4592\n",
      "Epoch [309/800], Step [34/53], D_loss: 0.0775, G_loss: 7.4138\n",
      "Epoch [309/800], Step [36/53], D_loss: 0.0422, G_loss: 5.0494\n",
      "Epoch [309/800], Step [38/53], D_loss: 0.1017, G_loss: 4.8945\n",
      "Epoch [309/800], Step [40/53], D_loss: 0.1052, G_loss: 4.9626\n",
      "Epoch [309/800], Step [42/53], D_loss: 0.0125, G_loss: 3.4802\n",
      "Epoch [309/800], Step [44/53], D_loss: 0.1419, G_loss: 5.4657\n",
      "Epoch [309/800], Step [46/53], D_loss: 0.0973, G_loss: 5.2049\n",
      "Epoch [309/800], Step [48/53], D_loss: 0.0875, G_loss: 4.9886\n",
      "Epoch [309/800], Step [50/53], D_loss: 0.0413, G_loss: 5.3104\n",
      "Epoch [309/800], Step [52/53], D_loss: 0.7195, G_loss: 5.1315\n",
      "Epoch [309/800], Avg D_loss: 0.1154, Avg G_loss: 6.1493\n",
      "Epoch [310/800], Step [2/53], D_loss: 0.0457, G_loss: 3.2592\n",
      "Epoch [310/800], Step [4/53], D_loss: 0.0632, G_loss: 4.3535\n",
      "Epoch [310/800], Step [6/53], D_loss: 0.0555, G_loss: 9.4896\n",
      "Epoch [310/800], Step [8/53], D_loss: 0.1516, G_loss: 3.2680\n",
      "Epoch [310/800], Step [10/53], D_loss: 0.5763, G_loss: 8.8964\n",
      "Epoch [310/800], Step [12/53], D_loss: 0.0087, G_loss: 6.4756\n",
      "Epoch [310/800], Step [14/53], D_loss: 0.0175, G_loss: 6.7578\n",
      "Epoch [310/800], Step [16/53], D_loss: 0.0506, G_loss: 2.0458\n",
      "Epoch [310/800], Step [18/53], D_loss: 0.0750, G_loss: 4.1478\n",
      "Epoch [310/800], Step [20/53], D_loss: 0.0797, G_loss: 5.7200\n",
      "Epoch [310/800], Step [22/53], D_loss: 0.0565, G_loss: 5.7523\n",
      "Epoch [310/800], Step [24/53], D_loss: 0.7710, G_loss: 10.0023\n",
      "Epoch [310/800], Step [26/53], D_loss: 0.5433, G_loss: 2.9825\n",
      "Epoch [310/800], Step [28/53], D_loss: 0.5133, G_loss: 4.5429\n",
      "Epoch [310/800], Step [30/53], D_loss: 0.1115, G_loss: 5.2353\n",
      "Epoch [310/800], Step [32/53], D_loss: 0.0278, G_loss: 4.9411\n",
      "Epoch [310/800], Step [34/53], D_loss: 0.0390, G_loss: 4.8561\n",
      "Epoch [310/800], Step [36/53], D_loss: 0.0207, G_loss: 5.3934\n",
      "Epoch [310/800], Step [38/53], D_loss: 0.1006, G_loss: 4.8361\n",
      "Epoch [310/800], Step [40/53], D_loss: 0.0484, G_loss: 1.7349\n",
      "Epoch [310/800], Step [42/53], D_loss: 0.0473, G_loss: 6.0822\n",
      "Epoch [310/800], Step [44/53], D_loss: 0.0304, G_loss: 4.6385\n",
      "Epoch [310/800], Step [46/53], D_loss: 0.2405, G_loss: 8.6907\n",
      "Epoch [310/800], Step [48/53], D_loss: 0.2646, G_loss: 2.8043\n",
      "Epoch [310/800], Step [50/53], D_loss: 0.0641, G_loss: 4.1021\n",
      "Epoch [310/800], Step [52/53], D_loss: 0.0275, G_loss: 3.6028\n",
      "Epoch [310/800], Avg D_loss: 0.1381, Avg G_loss: 5.2542\n",
      "Epoch [311/800], Step [2/53], D_loss: 0.0095, G_loss: 6.6126\n",
      "Epoch [311/800], Step [4/53], D_loss: 0.5360, G_loss: 10.5102\n",
      "Epoch [311/800], Step [6/53], D_loss: 0.4289, G_loss: 3.4292\n",
      "Epoch [311/800], Step [8/53], D_loss: 0.0488, G_loss: 2.9514\n",
      "Epoch [311/800], Step [10/53], D_loss: 0.4426, G_loss: 8.5128\n",
      "Epoch [311/800], Step [12/53], D_loss: 0.0571, G_loss: 6.7987\n",
      "Epoch [311/800], Step [14/53], D_loss: 0.1259, G_loss: 3.3776\n",
      "Epoch [311/800], Step [16/53], D_loss: 0.0850, G_loss: 3.4099\n",
      "Epoch [311/800], Step [18/53], D_loss: 0.0985, G_loss: 5.2806\n",
      "Epoch [311/800], Step [20/53], D_loss: 0.1095, G_loss: 7.5353\n",
      "Epoch [311/800], Step [22/53], D_loss: 0.1080, G_loss: 7.7145\n",
      "Epoch [311/800], Step [24/53], D_loss: 0.0152, G_loss: 2.2742\n",
      "Epoch [311/800], Step [26/53], D_loss: 0.1645, G_loss: 7.6768\n",
      "Epoch [311/800], Step [28/53], D_loss: 0.8331, G_loss: 8.7029\n",
      "Epoch [311/800], Step [30/53], D_loss: 0.9484, G_loss: 6.6060\n",
      "Epoch [311/800], Step [32/53], D_loss: 0.2692, G_loss: 2.0059\n",
      "Epoch [311/800], Step [34/53], D_loss: 0.2664, G_loss: 8.0147\n",
      "Epoch [311/800], Step [36/53], D_loss: 0.0758, G_loss: 3.8179\n",
      "Epoch [311/800], Step [38/53], D_loss: 0.3569, G_loss: 5.5338\n",
      "Epoch [311/800], Step [40/53], D_loss: 0.1840, G_loss: 3.4502\n",
      "Epoch [311/800], Step [42/53], D_loss: 0.0671, G_loss: 3.9260\n",
      "Epoch [311/800], Step [44/53], D_loss: 0.0137, G_loss: 5.8559\n",
      "Epoch [311/800], Step [46/53], D_loss: 0.2819, G_loss: 4.0663\n",
      "Epoch [311/800], Step [48/53], D_loss: 0.0379, G_loss: 4.2312\n",
      "Epoch [311/800], Step [50/53], D_loss: 0.1022, G_loss: 4.1397\n",
      "Epoch [311/800], Step [52/53], D_loss: 0.0212, G_loss: 3.7845\n",
      "Epoch [311/800], Avg D_loss: 0.1644, Avg G_loss: 5.1664\n",
      "Epoch [312/800], Step [2/53], D_loss: 0.0117, G_loss: 4.8680\n",
      "Epoch [312/800], Step [4/53], D_loss: 0.0233, G_loss: 3.8680\n",
      "Epoch [312/800], Step [6/53], D_loss: 0.0585, G_loss: 6.5927\n",
      "Epoch [312/800], Step [8/53], D_loss: 0.1628, G_loss: 7.5456\n",
      "Epoch [312/800], Step [10/53], D_loss: 0.1419, G_loss: 5.5752\n",
      "Epoch [312/800], Step [12/53], D_loss: 0.0509, G_loss: 6.5315\n",
      "Epoch [312/800], Step [14/53], D_loss: 0.0054, G_loss: 7.0412\n",
      "Epoch [312/800], Step [16/53], D_loss: 0.0150, G_loss: 8.0843\n",
      "Epoch [312/800], Step [18/53], D_loss: 0.0742, G_loss: 8.2711\n",
      "Epoch [312/800], Step [20/53], D_loss: 0.0190, G_loss: 9.5701\n",
      "Epoch [312/800], Step [22/53], D_loss: 0.0083, G_loss: 6.5373\n",
      "Epoch [312/800], Step [24/53], D_loss: 0.2303, G_loss: 8.3297\n",
      "Epoch [312/800], Step [26/53], D_loss: 0.1573, G_loss: 7.6008\n",
      "Epoch [312/800], Step [28/53], D_loss: 0.0507, G_loss: 3.4696\n",
      "Epoch [312/800], Step [30/53], D_loss: 0.1391, G_loss: 6.2653\n",
      "Epoch [312/800], Step [32/53], D_loss: 0.1640, G_loss: 6.1405\n",
      "Epoch [312/800], Step [34/53], D_loss: 0.1124, G_loss: 5.3669\n",
      "Epoch [312/800], Step [36/53], D_loss: 0.0421, G_loss: 6.5699\n",
      "Epoch [312/800], Step [38/53], D_loss: 0.1821, G_loss: 6.1380\n",
      "Epoch [312/800], Step [40/53], D_loss: 0.0002, G_loss: 9.0388\n",
      "Epoch [312/800], Step [42/53], D_loss: 0.0031, G_loss: 8.7833\n",
      "Epoch [312/800], Step [44/53], D_loss: 0.1920, G_loss: 5.8871\n",
      "Epoch [312/800], Step [46/53], D_loss: 0.0135, G_loss: 10.1458\n",
      "Epoch [312/800], Step [48/53], D_loss: 0.0586, G_loss: 2.0493\n",
      "Epoch [312/800], Step [50/53], D_loss: 0.0225, G_loss: 9.5360\n",
      "Epoch [312/800], Step [52/53], D_loss: 0.0358, G_loss: 4.0462\n",
      "Epoch [312/800], Avg D_loss: 0.0814, Avg G_loss: 6.5560\n",
      "Epoch [313/800], Step [2/53], D_loss: 0.1068, G_loss: 6.3864\n",
      "Epoch [313/800], Step [4/53], D_loss: 0.0446, G_loss: 6.9339\n",
      "Epoch [313/800], Step [6/53], D_loss: 0.0895, G_loss: 4.8151\n",
      "Epoch [313/800], Step [8/53], D_loss: 0.0487, G_loss: 5.9472\n",
      "Epoch [313/800], Step [10/53], D_loss: 0.1306, G_loss: 3.1518\n",
      "Epoch [313/800], Step [12/53], D_loss: 0.0417, G_loss: 3.4751\n",
      "Epoch [313/800], Step [14/53], D_loss: 0.0469, G_loss: 5.6675\n",
      "Epoch [313/800], Step [16/53], D_loss: 0.0961, G_loss: 7.9339\n",
      "Epoch [313/800], Step [18/53], D_loss: 0.0118, G_loss: 6.4918\n",
      "Epoch [313/800], Step [20/53], D_loss: 0.0063, G_loss: 2.2356\n",
      "Epoch [313/800], Step [22/53], D_loss: 0.0301, G_loss: 4.9395\n",
      "Epoch [313/800], Step [24/53], D_loss: 0.0459, G_loss: 6.2819\n",
      "Epoch [313/800], Step [26/53], D_loss: 0.0133, G_loss: 6.7815\n",
      "Epoch [313/800], Step [28/53], D_loss: 0.0138, G_loss: 8.9248\n",
      "Epoch [313/800], Step [30/53], D_loss: 0.0790, G_loss: 7.6530\n",
      "Epoch [313/800], Step [32/53], D_loss: 0.0287, G_loss: 4.5305\n",
      "Epoch [313/800], Step [34/53], D_loss: 0.0046, G_loss: 7.1135\n",
      "Epoch [313/800], Step [36/53], D_loss: 0.3160, G_loss: 7.0822\n",
      "Epoch [313/800], Step [38/53], D_loss: 0.8447, G_loss: 12.2219\n",
      "Epoch [313/800], Step [40/53], D_loss: 0.0039, G_loss: 5.0068\n",
      "Epoch [313/800], Step [42/53], D_loss: 0.0209, G_loss: 3.9420\n",
      "Epoch [313/800], Step [44/53], D_loss: 0.0443, G_loss: 4.4315\n",
      "Epoch [313/800], Step [46/53], D_loss: 0.0021, G_loss: 5.7282\n",
      "Epoch [313/800], Step [48/53], D_loss: 0.0555, G_loss: 3.7097\n",
      "Epoch [313/800], Step [50/53], D_loss: 0.0593, G_loss: 4.6594\n",
      "Epoch [313/800], Step [52/53], D_loss: 0.0763, G_loss: 5.8234\n",
      "Epoch [313/800], Avg D_loss: 0.1065, Avg G_loss: 5.8046\n",
      "Epoch [314/800], Step [2/53], D_loss: 1.2141, G_loss: 6.9916\n",
      "Epoch [314/800], Step [4/53], D_loss: 0.6961, G_loss: 3.6953\n",
      "Epoch [314/800], Step [6/53], D_loss: 0.1691, G_loss: 1.4706\n",
      "Epoch [314/800], Step [8/53], D_loss: 0.0578, G_loss: 4.4947\n",
      "Epoch [314/800], Step [10/53], D_loss: 0.2856, G_loss: 5.2809\n",
      "Epoch [314/800], Step [12/53], D_loss: 0.1927, G_loss: 1.2647\n",
      "Epoch [314/800], Step [14/53], D_loss: 0.1118, G_loss: 4.7984\n",
      "Epoch [314/800], Step [16/53], D_loss: 0.1160, G_loss: 3.4889\n",
      "Epoch [314/800], Step [18/53], D_loss: 0.0860, G_loss: 5.6076\n",
      "Epoch [314/800], Step [20/53], D_loss: 0.1757, G_loss: 4.6253\n",
      "Epoch [314/800], Step [22/53], D_loss: 0.0289, G_loss: 6.0504\n",
      "Epoch [314/800], Step [24/53], D_loss: 0.1939, G_loss: 4.1302\n",
      "Epoch [314/800], Step [26/53], D_loss: 0.4107, G_loss: 7.4799\n",
      "Epoch [314/800], Step [28/53], D_loss: 0.6711, G_loss: 2.9485\n",
      "Epoch [314/800], Step [30/53], D_loss: 0.1523, G_loss: 4.2559\n",
      "Epoch [314/800], Step [32/53], D_loss: 0.2204, G_loss: 3.8079\n",
      "Epoch [314/800], Step [34/53], D_loss: 0.0644, G_loss: 6.2321\n",
      "Epoch [314/800], Step [36/53], D_loss: 0.4240, G_loss: 2.4567\n",
      "Epoch [314/800], Step [38/53], D_loss: 0.2169, G_loss: 4.4567\n",
      "Epoch [314/800], Step [40/53], D_loss: 0.0931, G_loss: 2.3043\n",
      "Epoch [314/800], Step [42/53], D_loss: 0.3991, G_loss: 5.3135\n",
      "Epoch [314/800], Step [44/53], D_loss: 0.1053, G_loss: 3.3221\n",
      "Epoch [314/800], Step [46/53], D_loss: 0.0398, G_loss: 4.4602\n",
      "Epoch [314/800], Step [48/53], D_loss: 0.0237, G_loss: 4.5224\n",
      "Epoch [314/800], Step [50/53], D_loss: 0.2930, G_loss: 6.6177\n",
      "Epoch [314/800], Step [52/53], D_loss: 0.2420, G_loss: 4.1223\n",
      "Epoch [314/800], Avg D_loss: 0.2117, Avg G_loss: 4.2767\n",
      "Epoch [315/800], Step [2/53], D_loss: 0.0568, G_loss: 4.5230\n",
      "Epoch [315/800], Step [4/53], D_loss: 0.0389, G_loss: 6.1206\n",
      "Epoch [315/800], Step [6/53], D_loss: 0.0359, G_loss: 5.2887\n",
      "Epoch [315/800], Step [8/53], D_loss: 0.0350, G_loss: 2.8697\n",
      "Epoch [315/800], Step [10/53], D_loss: 0.0748, G_loss: 1.8757\n",
      "Epoch [315/800], Step [12/53], D_loss: 0.3248, G_loss: 5.8158\n",
      "Epoch [315/800], Step [14/53], D_loss: 0.2848, G_loss: 8.1318\n",
      "Epoch [315/800], Step [16/53], D_loss: 0.1649, G_loss: 4.8880\n",
      "Epoch [315/800], Step [18/53], D_loss: 0.0090, G_loss: 5.3912\n",
      "Epoch [315/800], Step [20/53], D_loss: 0.1007, G_loss: 4.2010\n",
      "Epoch [315/800], Step [22/53], D_loss: 0.0078, G_loss: 3.7929\n",
      "Epoch [315/800], Step [24/53], D_loss: 0.0100, G_loss: 3.8149\n",
      "Epoch [315/800], Step [26/53], D_loss: 0.0590, G_loss: 6.3012\n",
      "Epoch [315/800], Step [28/53], D_loss: 0.1344, G_loss: 8.9726\n",
      "Epoch [315/800], Step [30/53], D_loss: 0.4136, G_loss: 6.8080\n",
      "Epoch [315/800], Step [32/53], D_loss: 0.0206, G_loss: 5.3745\n",
      "Epoch [315/800], Step [34/53], D_loss: 0.0203, G_loss: 7.2980\n",
      "Epoch [315/800], Step [36/53], D_loss: 0.0219, G_loss: 5.2834\n",
      "Epoch [315/800], Step [38/53], D_loss: 0.0421, G_loss: 5.1294\n",
      "Epoch [315/800], Step [40/53], D_loss: 0.1820, G_loss: 5.9141\n",
      "Epoch [315/800], Step [42/53], D_loss: 0.0126, G_loss: 3.4987\n",
      "Epoch [315/800], Step [44/53], D_loss: 0.0443, G_loss: 9.7384\n",
      "Epoch [315/800], Step [46/53], D_loss: 0.0441, G_loss: 8.3280\n",
      "Epoch [315/800], Step [48/53], D_loss: 0.0062, G_loss: 5.0441\n",
      "Epoch [315/800], Step [50/53], D_loss: 0.0870, G_loss: 4.3801\n",
      "Epoch [315/800], Step [52/53], D_loss: 0.0058, G_loss: 2.5859\n",
      "Epoch [315/800], Avg D_loss: 0.0905, Avg G_loss: 5.7471\n",
      "Epoch [316/800], Step [2/53], D_loss: 0.0289, G_loss: 6.3627\n",
      "Epoch [316/800], Step [4/53], D_loss: 0.0160, G_loss: 7.2852\n",
      "Epoch [316/800], Step [6/53], D_loss: 0.0092, G_loss: 9.8983\n",
      "Epoch [316/800], Step [8/53], D_loss: 0.0341, G_loss: 2.6393\n",
      "Epoch [316/800], Step [10/53], D_loss: 0.0575, G_loss: 9.3452\n",
      "Epoch [316/800], Step [12/53], D_loss: 0.5235, G_loss: 3.1884\n",
      "Epoch [316/800], Step [14/53], D_loss: 0.0612, G_loss: 3.5239\n",
      "Epoch [316/800], Step [16/53], D_loss: 0.0707, G_loss: 4.4248\n",
      "Epoch [316/800], Step [18/53], D_loss: 0.0266, G_loss: 5.0147\n",
      "Epoch [316/800], Step [20/53], D_loss: 0.0688, G_loss: 4.7763\n",
      "Epoch [316/800], Step [22/53], D_loss: 0.0269, G_loss: 5.4810\n",
      "Epoch [316/800], Step [24/53], D_loss: 0.2780, G_loss: 3.7981\n",
      "Epoch [316/800], Step [26/53], D_loss: 0.0549, G_loss: 5.9264\n",
      "Epoch [316/800], Step [28/53], D_loss: 0.0691, G_loss: 4.6387\n",
      "Epoch [316/800], Step [30/53], D_loss: 0.0310, G_loss: 4.7317\n",
      "Epoch [316/800], Step [32/53], D_loss: 0.1987, G_loss: 6.6137\n",
      "Epoch [316/800], Step [34/53], D_loss: 0.1145, G_loss: 5.5569\n",
      "Epoch [316/800], Step [36/53], D_loss: 0.1237, G_loss: 0.9574\n",
      "Epoch [316/800], Step [38/53], D_loss: 0.0806, G_loss: 4.4478\n",
      "Epoch [316/800], Step [40/53], D_loss: 0.1807, G_loss: 7.5777\n",
      "Epoch [316/800], Step [42/53], D_loss: 0.2119, G_loss: 5.7641\n",
      "Epoch [316/800], Step [44/53], D_loss: 0.0240, G_loss: 4.0894\n",
      "Epoch [316/800], Step [46/53], D_loss: 0.0365, G_loss: 7.5471\n",
      "Epoch [316/800], Step [48/53], D_loss: 0.0132, G_loss: 9.3115\n",
      "Epoch [316/800], Step [50/53], D_loss: 0.0303, G_loss: 6.1540\n",
      "Epoch [316/800], Step [52/53], D_loss: 0.0982, G_loss: 3.0538\n",
      "Epoch [316/800], Avg D_loss: 0.1158, Avg G_loss: 5.6329\n",
      "Epoch [317/800], Step [2/53], D_loss: 0.0404, G_loss: 7.4927\n",
      "Epoch [317/800], Step [4/53], D_loss: 0.0433, G_loss: 3.8939\n",
      "Epoch [317/800], Step [6/53], D_loss: 1.3515, G_loss: 5.4903\n",
      "Epoch [317/800], Step [8/53], D_loss: 0.0894, G_loss: 2.1857\n",
      "Epoch [317/800], Step [10/53], D_loss: 0.1969, G_loss: 4.8560\n",
      "Epoch [317/800], Step [12/53], D_loss: 0.1004, G_loss: 1.8216\n",
      "Epoch [317/800], Step [14/53], D_loss: 0.2739, G_loss: 5.9712\n",
      "Epoch [317/800], Step [16/53], D_loss: 0.1707, G_loss: 5.2964\n",
      "Epoch [317/800], Step [18/53], D_loss: 0.0330, G_loss: 4.1826\n",
      "Epoch [317/800], Step [20/53], D_loss: 0.0769, G_loss: 6.0735\n",
      "Epoch [317/800], Step [22/53], D_loss: 0.0307, G_loss: 2.8873\n",
      "Epoch [317/800], Step [24/53], D_loss: 0.0157, G_loss: 1.0067\n",
      "Epoch [317/800], Step [26/53], D_loss: 0.0595, G_loss: 6.8553\n",
      "Epoch [317/800], Step [28/53], D_loss: 0.0371, G_loss: 5.4060\n",
      "Epoch [317/800], Step [30/53], D_loss: 0.0328, G_loss: 3.4133\n",
      "Epoch [317/800], Step [32/53], D_loss: 0.1569, G_loss: 3.7576\n",
      "Epoch [317/800], Step [34/53], D_loss: 0.1643, G_loss: 1.7527\n",
      "Epoch [317/800], Step [36/53], D_loss: 0.0780, G_loss: 7.6233\n",
      "Epoch [317/800], Step [38/53], D_loss: 0.0362, G_loss: 7.8569\n",
      "Epoch [317/800], Step [40/53], D_loss: 0.0434, G_loss: 5.4323\n",
      "Epoch [317/800], Step [42/53], D_loss: 0.2080, G_loss: 5.0310\n",
      "Epoch [317/800], Step [44/53], D_loss: 0.1160, G_loss: 4.5423\n",
      "Epoch [317/800], Step [46/53], D_loss: 0.0261, G_loss: 8.0025\n",
      "Epoch [317/800], Step [48/53], D_loss: 0.0279, G_loss: 6.8925\n",
      "Epoch [317/800], Step [50/53], D_loss: 0.0678, G_loss: 4.8742\n",
      "Epoch [317/800], Step [52/53], D_loss: 0.1074, G_loss: 7.8771\n",
      "Epoch [317/800], Avg D_loss: 0.1371, Avg G_loss: 4.9798\n",
      "Epoch [318/800], Step [2/53], D_loss: 0.0160, G_loss: 7.5807\n",
      "Epoch [318/800], Step [4/53], D_loss: 0.1529, G_loss: 7.0114\n",
      "Epoch [318/800], Step [6/53], D_loss: 0.0048, G_loss: 9.6295\n",
      "Epoch [318/800], Step [8/53], D_loss: 0.0040, G_loss: 4.7636\n",
      "Epoch [318/800], Step [10/53], D_loss: 0.0561, G_loss: 3.7645\n",
      "Epoch [318/800], Step [12/53], D_loss: 0.0433, G_loss: 5.3400\n",
      "Epoch [318/800], Step [14/53], D_loss: 0.0023, G_loss: 4.9816\n",
      "Epoch [318/800], Step [16/53], D_loss: 0.1355, G_loss: 7.5583\n",
      "Epoch [318/800], Step [18/53], D_loss: 0.1026, G_loss: 2.3291\n",
      "Epoch [318/800], Step [20/53], D_loss: 0.4921, G_loss: 9.6997\n",
      "Epoch [318/800], Step [22/53], D_loss: 0.1276, G_loss: 7.7676\n",
      "Epoch [318/800], Step [24/53], D_loss: 0.0054, G_loss: 2.0677\n",
      "Epoch [318/800], Step [26/53], D_loss: 0.6550, G_loss: 12.6170\n",
      "Epoch [318/800], Step [28/53], D_loss: 0.0724, G_loss: 1.6718\n",
      "Epoch [318/800], Step [30/53], D_loss: 0.5710, G_loss: 6.9745\n",
      "Epoch [318/800], Step [32/53], D_loss: 0.9151, G_loss: 0.5853\n",
      "Epoch [318/800], Step [34/53], D_loss: 0.6229, G_loss: 3.6839\n",
      "Epoch [318/800], Step [36/53], D_loss: 0.1212, G_loss: 4.8831\n",
      "Epoch [318/800], Step [38/53], D_loss: 0.1443, G_loss: 3.6951\n",
      "Epoch [318/800], Step [40/53], D_loss: 0.0955, G_loss: 2.3743\n",
      "Epoch [318/800], Step [42/53], D_loss: 0.1492, G_loss: 3.9925\n",
      "Epoch [318/800], Step [44/53], D_loss: 0.1239, G_loss: 3.0391\n",
      "Epoch [318/800], Step [46/53], D_loss: 0.1320, G_loss: 3.2936\n",
      "Epoch [318/800], Step [48/53], D_loss: 0.0939, G_loss: 4.2916\n",
      "Epoch [318/800], Step [50/53], D_loss: 0.0391, G_loss: 4.7884\n",
      "Epoch [318/800], Step [52/53], D_loss: 0.0833, G_loss: 3.4935\n",
      "Epoch [318/800], Avg D_loss: 0.1859, Avg G_loss: 4.7558\n",
      "Epoch [319/800], Step [2/53], D_loss: 0.0288, G_loss: 4.8211\n",
      "Epoch [319/800], Step [4/53], D_loss: 0.0416, G_loss: 5.8332\n",
      "Epoch [319/800], Step [6/53], D_loss: 0.1323, G_loss: 5.4346\n",
      "Epoch [319/800], Step [8/53], D_loss: 0.0527, G_loss: 4.3418\n",
      "Epoch [319/800], Step [10/53], D_loss: 0.0518, G_loss: 4.3548\n",
      "Epoch [319/800], Step [12/53], D_loss: 0.0152, G_loss: 5.2259\n",
      "Epoch [319/800], Step [14/53], D_loss: 0.0811, G_loss: 1.8436\n",
      "Epoch [319/800], Step [16/53], D_loss: 0.0611, G_loss: 3.5280\n",
      "Epoch [319/800], Step [18/53], D_loss: 0.0319, G_loss: 8.3372\n",
      "Epoch [319/800], Step [20/53], D_loss: 0.0427, G_loss: 7.8640\n",
      "Epoch [319/800], Step [22/53], D_loss: 0.0236, G_loss: 5.4078\n",
      "Epoch [319/800], Step [24/53], D_loss: 0.0650, G_loss: 5.2544\n",
      "Epoch [319/800], Step [26/53], D_loss: 0.0758, G_loss: 5.3339\n",
      "Epoch [319/800], Step [28/53], D_loss: 0.0346, G_loss: 4.1425\n",
      "Epoch [319/800], Step [30/53], D_loss: 0.1030, G_loss: 9.1353\n",
      "Epoch [319/800], Step [32/53], D_loss: 0.0402, G_loss: 7.6810\n",
      "Epoch [319/800], Step [34/53], D_loss: 0.0035, G_loss: 4.8101\n",
      "Epoch [319/800], Step [36/53], D_loss: 0.0407, G_loss: 5.3991\n",
      "Epoch [319/800], Step [38/53], D_loss: 0.0506, G_loss: 5.3406\n",
      "Epoch [319/800], Step [40/53], D_loss: 0.0530, G_loss: 7.1689\n",
      "Epoch [319/800], Step [42/53], D_loss: 0.1290, G_loss: 9.3744\n",
      "Epoch [319/800], Step [44/53], D_loss: 0.1934, G_loss: 7.5318\n",
      "Epoch [319/800], Step [46/53], D_loss: 0.0096, G_loss: 8.4995\n",
      "Epoch [319/800], Step [48/53], D_loss: 0.0315, G_loss: 5.6523\n",
      "Epoch [319/800], Step [50/53], D_loss: 0.0167, G_loss: 3.9108\n",
      "Epoch [319/800], Step [52/53], D_loss: 0.0261, G_loss: 7.3767\n",
      "Epoch [319/800], Avg D_loss: 0.0606, Avg G_loss: 5.8729\n",
      "Epoch [320/800], Step [2/53], D_loss: 0.0109, G_loss: 5.7613\n",
      "Epoch [320/800], Step [4/53], D_loss: 0.0678, G_loss: 6.1141\n",
      "Epoch [320/800], Step [6/53], D_loss: 0.0039, G_loss: 7.2489\n",
      "Epoch [320/800], Step [8/53], D_loss: 0.0735, G_loss: 6.4354\n",
      "Epoch [320/800], Step [10/53], D_loss: 0.0315, G_loss: 6.8701\n",
      "Epoch [320/800], Step [12/53], D_loss: 0.0365, G_loss: 7.0733\n",
      "Epoch [320/800], Step [14/53], D_loss: 0.0822, G_loss: 8.8283\n",
      "Epoch [320/800], Step [16/53], D_loss: 0.0033, G_loss: 6.8503\n",
      "Epoch [320/800], Step [18/53], D_loss: 0.0633, G_loss: 4.0795\n",
      "Epoch [320/800], Step [20/53], D_loss: 0.0618, G_loss: 7.5358\n",
      "Epoch [320/800], Step [22/53], D_loss: 0.1392, G_loss: 10.9029\n",
      "Epoch [320/800], Step [24/53], D_loss: 0.0128, G_loss: 10.5210\n",
      "Epoch [320/800], Step [26/53], D_loss: 0.0033, G_loss: 9.3152\n",
      "Epoch [320/800], Step [28/53], D_loss: 0.0302, G_loss: 9.9370\n",
      "Epoch [320/800], Step [30/53], D_loss: 0.0195, G_loss: 5.6684\n",
      "Epoch [320/800], Step [32/53], D_loss: 0.4579, G_loss: 15.4177\n",
      "Epoch [320/800], Step [34/53], D_loss: 0.0756, G_loss: 6.2076\n",
      "Epoch [320/800], Step [36/53], D_loss: 0.2912, G_loss: 6.4565\n",
      "Epoch [320/800], Step [38/53], D_loss: 0.0027, G_loss: 3.1021\n",
      "Epoch [320/800], Step [40/53], D_loss: 0.0308, G_loss: 3.6141\n",
      "Epoch [320/800], Step [42/53], D_loss: 0.0315, G_loss: 5.4525\n",
      "Epoch [320/800], Step [44/53], D_loss: 0.0458, G_loss: 6.2956\n",
      "Epoch [320/800], Step [46/53], D_loss: 0.1207, G_loss: 8.0378\n",
      "Epoch [320/800], Step [48/53], D_loss: 0.4499, G_loss: 2.8203\n",
      "Epoch [320/800], Step [50/53], D_loss: 0.4760, G_loss: 4.8569\n",
      "Epoch [320/800], Step [52/53], D_loss: 0.1826, G_loss: 9.4955\n",
      "Epoch [320/800], Avg D_loss: 0.1061, Avg G_loss: 6.7590\n",
      "Generated images saved as generated_images/epoch_320.png\n",
      "Epoch [321/800], Step [2/53], D_loss: 0.1546, G_loss: 1.5513\n",
      "Epoch [321/800], Step [4/53], D_loss: 0.0526, G_loss: 5.9933\n",
      "Epoch [321/800], Step [6/53], D_loss: 0.0311, G_loss: 7.2780\n",
      "Epoch [321/800], Step [8/53], D_loss: 0.0429, G_loss: 8.6801\n",
      "Epoch [321/800], Step [10/53], D_loss: 0.0211, G_loss: 6.0506\n",
      "Epoch [321/800], Step [12/53], D_loss: 0.0725, G_loss: 2.1568\n",
      "Epoch [321/800], Step [14/53], D_loss: 0.0623, G_loss: 5.8112\n",
      "Epoch [321/800], Step [16/53], D_loss: 1.5410, G_loss: 10.3628\n",
      "Epoch [321/800], Step [18/53], D_loss: 0.1969, G_loss: 4.1497\n",
      "Epoch [321/800], Step [20/53], D_loss: 0.1134, G_loss: 2.9886\n",
      "Epoch [321/800], Step [22/53], D_loss: 0.1700, G_loss: 3.8826\n",
      "Epoch [321/800], Step [24/53], D_loss: 0.0345, G_loss: 3.3545\n",
      "Epoch [321/800], Step [26/53], D_loss: 0.1100, G_loss: 3.7645\n",
      "Epoch [321/800], Step [28/53], D_loss: 0.1700, G_loss: 5.0494\n",
      "Epoch [321/800], Step [30/53], D_loss: 0.2483, G_loss: 2.4692\n",
      "Epoch [321/800], Step [32/53], D_loss: 0.1363, G_loss: 5.1438\n",
      "Epoch [321/800], Step [34/53], D_loss: 0.0616, G_loss: 5.5632\n",
      "Epoch [321/800], Step [36/53], D_loss: 0.1062, G_loss: 4.7533\n",
      "Epoch [321/800], Step [38/53], D_loss: 0.4317, G_loss: 1.5060\n",
      "Epoch [321/800], Step [40/53], D_loss: 0.0191, G_loss: 5.2455\n",
      "Epoch [321/800], Step [42/53], D_loss: 0.2652, G_loss: 4.2504\n",
      "Epoch [321/800], Step [44/53], D_loss: 0.1369, G_loss: 4.6786\n",
      "Epoch [321/800], Step [46/53], D_loss: 0.1315, G_loss: 2.0174\n",
      "Epoch [321/800], Step [48/53], D_loss: 0.0269, G_loss: 5.6095\n",
      "Epoch [321/800], Step [50/53], D_loss: 0.0285, G_loss: 4.3927\n",
      "Epoch [321/800], Step [52/53], D_loss: 0.0042, G_loss: 7.1381\n",
      "Epoch [321/800], Avg D_loss: 0.1597, Avg G_loss: 4.7843\n",
      "Epoch [322/800], Step [2/53], D_loss: 0.0177, G_loss: 7.2789\n",
      "Epoch [322/800], Step [4/53], D_loss: 0.0197, G_loss: 4.8524\n",
      "Epoch [322/800], Step [6/53], D_loss: 0.0648, G_loss: 5.4657\n",
      "Epoch [322/800], Step [8/53], D_loss: 0.0530, G_loss: 7.5451\n",
      "Epoch [322/800], Step [10/53], D_loss: 0.0274, G_loss: 4.3305\n",
      "Epoch [322/800], Step [12/53], D_loss: 0.1207, G_loss: 5.1613\n",
      "Epoch [322/800], Step [14/53], D_loss: 0.0225, G_loss: 5.3307\n",
      "Epoch [322/800], Step [16/53], D_loss: 0.0056, G_loss: 7.3916\n",
      "Epoch [322/800], Step [18/53], D_loss: 0.0712, G_loss: 5.0446\n",
      "Epoch [322/800], Step [20/53], D_loss: 0.0524, G_loss: 7.0032\n",
      "Epoch [322/800], Step [22/53], D_loss: 0.0077, G_loss: 6.2577\n",
      "Epoch [322/800], Step [24/53], D_loss: 0.0158, G_loss: 8.4013\n",
      "Epoch [322/800], Step [26/53], D_loss: 0.0046, G_loss: 7.4824\n",
      "Epoch [322/800], Step [28/53], D_loss: 0.0974, G_loss: 6.1049\n",
      "Epoch [322/800], Step [30/53], D_loss: 0.0128, G_loss: 5.7518\n",
      "Epoch [322/800], Step [32/53], D_loss: 0.0020, G_loss: 8.0102\n",
      "Epoch [322/800], Step [34/53], D_loss: 0.0043, G_loss: 8.1671\n",
      "Epoch [322/800], Step [36/53], D_loss: 0.0612, G_loss: 5.2963\n",
      "Epoch [322/800], Step [38/53], D_loss: 0.0447, G_loss: 5.0931\n",
      "Epoch [322/800], Step [40/53], D_loss: 0.0499, G_loss: 7.8042\n",
      "Epoch [322/800], Step [42/53], D_loss: 0.0011, G_loss: 5.0066\n",
      "Epoch [322/800], Step [44/53], D_loss: 0.0318, G_loss: 8.5110\n",
      "Epoch [322/800], Step [46/53], D_loss: 0.0370, G_loss: 7.3740\n",
      "Epoch [322/800], Step [48/53], D_loss: 0.0190, G_loss: 6.9693\n",
      "Epoch [322/800], Step [50/53], D_loss: 0.0201, G_loss: 7.3682\n",
      "Epoch [322/800], Step [52/53], D_loss: 0.0266, G_loss: 6.7592\n",
      "Epoch [322/800], Avg D_loss: 0.0342, Avg G_loss: 6.2415\n",
      "Epoch [323/800], Step [2/53], D_loss: 0.0100, G_loss: 3.5762\n",
      "Epoch [323/800], Step [4/53], D_loss: 0.0050, G_loss: 7.7278\n",
      "Epoch [323/800], Step [6/53], D_loss: 0.0073, G_loss: 5.6466\n",
      "Epoch [323/800], Step [8/53], D_loss: 0.0334, G_loss: 6.8684\n",
      "Epoch [323/800], Step [10/53], D_loss: 0.0052, G_loss: 9.8311\n",
      "Epoch [323/800], Step [12/53], D_loss: 0.5330, G_loss: 9.4375\n",
      "Epoch [323/800], Step [14/53], D_loss: 0.0152, G_loss: 9.3694\n",
      "Epoch [323/800], Step [16/53], D_loss: 0.0171, G_loss: 6.9765\n",
      "Epoch [323/800], Step [18/53], D_loss: 0.0373, G_loss: 4.0420\n",
      "Epoch [323/800], Step [20/53], D_loss: 0.0234, G_loss: 2.7090\n",
      "Epoch [323/800], Step [22/53], D_loss: 0.0313, G_loss: 8.0700\n",
      "Epoch [323/800], Step [24/53], D_loss: 0.0193, G_loss: 7.3398\n",
      "Epoch [323/800], Step [26/53], D_loss: 0.0411, G_loss: 6.5570\n",
      "Epoch [323/800], Step [28/53], D_loss: 0.0466, G_loss: 7.4367\n",
      "Epoch [323/800], Step [30/53], D_loss: 0.1222, G_loss: 9.6566\n",
      "Epoch [323/800], Step [32/53], D_loss: 0.2565, G_loss: 4.5699\n",
      "Epoch [323/800], Step [34/53], D_loss: 0.8064, G_loss: 12.9452\n",
      "Epoch [323/800], Step [36/53], D_loss: 0.0611, G_loss: 4.9438\n",
      "Epoch [323/800], Step [38/53], D_loss: 0.1465, G_loss: 5.9455\n",
      "Epoch [323/800], Step [40/53], D_loss: 0.1239, G_loss: 9.2489\n",
      "Epoch [323/800], Step [42/53], D_loss: 0.0778, G_loss: 3.8730\n",
      "Epoch [323/800], Step [44/53], D_loss: 0.0934, G_loss: 7.8991\n",
      "Epoch [323/800], Step [46/53], D_loss: 0.0573, G_loss: 6.9794\n",
      "Epoch [323/800], Step [48/53], D_loss: 0.0630, G_loss: 7.3108\n",
      "Epoch [323/800], Step [50/53], D_loss: 0.2870, G_loss: 8.6452\n",
      "Epoch [323/800], Step [52/53], D_loss: 0.0312, G_loss: 3.2668\n",
      "Epoch [323/800], Avg D_loss: 0.1105, Avg G_loss: 6.9052\n",
      "Epoch [324/800], Step [2/53], D_loss: 0.0502, G_loss: 7.7352\n",
      "Epoch [324/800], Step [4/53], D_loss: 0.0258, G_loss: 5.5526\n",
      "Epoch [324/800], Step [6/53], D_loss: 0.0503, G_loss: 6.5454\n",
      "Epoch [324/800], Step [8/53], D_loss: 0.0188, G_loss: 8.1721\n",
      "Epoch [324/800], Step [10/53], D_loss: 1.2565, G_loss: 1.5715\n",
      "Epoch [324/800], Step [12/53], D_loss: 0.4898, G_loss: 5.1817\n",
      "Epoch [324/800], Step [14/53], D_loss: 0.3464, G_loss: 4.9252\n",
      "Epoch [324/800], Step [16/53], D_loss: 0.0456, G_loss: 5.5502\n",
      "Epoch [324/800], Step [18/53], D_loss: 0.2363, G_loss: 3.9234\n",
      "Epoch [324/800], Step [20/53], D_loss: 0.0831, G_loss: 5.0836\n",
      "Epoch [324/800], Step [22/53], D_loss: 0.0107, G_loss: 5.1787\n",
      "Epoch [324/800], Step [24/53], D_loss: 0.0988, G_loss: 3.6164\n",
      "Epoch [324/800], Step [26/53], D_loss: 0.0218, G_loss: 6.2487\n",
      "Epoch [324/800], Step [28/53], D_loss: 0.0156, G_loss: 6.2897\n",
      "Epoch [324/800], Step [30/53], D_loss: 0.3484, G_loss: 2.7324\n",
      "Epoch [324/800], Step [32/53], D_loss: 0.0466, G_loss: 7.5699\n",
      "Epoch [324/800], Step [34/53], D_loss: 0.0300, G_loss: 5.6998\n",
      "Epoch [324/800], Step [36/53], D_loss: 0.0352, G_loss: 3.5352\n",
      "Epoch [324/800], Step [38/53], D_loss: 0.2059, G_loss: 3.8643\n",
      "Epoch [324/800], Step [40/53], D_loss: 0.1201, G_loss: 6.3419\n",
      "Epoch [324/800], Step [42/53], D_loss: 0.0080, G_loss: 5.9651\n",
      "Epoch [324/800], Step [44/53], D_loss: 0.0086, G_loss: 3.6379\n",
      "Epoch [324/800], Step [46/53], D_loss: 0.0255, G_loss: 4.9487\n",
      "Epoch [324/800], Step [48/53], D_loss: 0.2104, G_loss: 7.9301\n",
      "Epoch [324/800], Step [50/53], D_loss: 0.2743, G_loss: 3.5365\n",
      "Epoch [324/800], Step [52/53], D_loss: 0.0099, G_loss: 7.2372\n",
      "Epoch [324/800], Avg D_loss: 0.1260, Avg G_loss: 5.4831\n",
      "Epoch [325/800], Step [2/53], D_loss: 0.0953, G_loss: 3.4495\n",
      "Epoch [325/800], Step [4/53], D_loss: 0.0045, G_loss: 4.1739\n",
      "Epoch [325/800], Step [6/53], D_loss: 0.0047, G_loss: 6.6673\n",
      "Epoch [325/800], Step [8/53], D_loss: 0.0765, G_loss: 6.2986\n",
      "Epoch [325/800], Step [10/53], D_loss: 0.0182, G_loss: 7.3600\n",
      "Epoch [325/800], Step [12/53], D_loss: 0.0176, G_loss: 6.8407\n",
      "Epoch [325/800], Step [14/53], D_loss: 0.0303, G_loss: 7.7814\n",
      "Epoch [325/800], Step [16/53], D_loss: 0.0569, G_loss: 7.3386\n",
      "Epoch [325/800], Step [18/53], D_loss: 0.0222, G_loss: 6.4150\n",
      "Epoch [325/800], Step [20/53], D_loss: 0.0099, G_loss: 6.6319\n",
      "Epoch [325/800], Step [22/53], D_loss: 0.0107, G_loss: 8.7672\n",
      "Epoch [325/800], Step [24/53], D_loss: 0.0370, G_loss: 4.9362\n",
      "Epoch [325/800], Step [26/53], D_loss: 0.3001, G_loss: 10.0075\n",
      "Epoch [325/800], Step [28/53], D_loss: 0.0800, G_loss: 9.3418\n",
      "Epoch [325/800], Step [30/53], D_loss: 0.1029, G_loss: 8.5758\n",
      "Epoch [325/800], Step [32/53], D_loss: 1.7657, G_loss: 15.2356\n",
      "Epoch [325/800], Step [34/53], D_loss: 0.2230, G_loss: 5.7189\n",
      "Epoch [325/800], Step [36/53], D_loss: 0.0644, G_loss: 1.1996\n",
      "Epoch [325/800], Step [38/53], D_loss: 0.2885, G_loss: 3.4132\n",
      "Epoch [325/800], Step [40/53], D_loss: 0.0992, G_loss: 4.3501\n",
      "Epoch [325/800], Step [42/53], D_loss: 0.3561, G_loss: 5.2144\n",
      "Epoch [325/800], Step [44/53], D_loss: 0.0342, G_loss: 3.7705\n",
      "Epoch [325/800], Step [46/53], D_loss: 0.0644, G_loss: 6.6977\n",
      "Epoch [325/800], Step [48/53], D_loss: 0.1220, G_loss: 3.0698\n",
      "Epoch [325/800], Step [50/53], D_loss: 0.0728, G_loss: 6.0070\n",
      "Epoch [325/800], Step [52/53], D_loss: 0.1799, G_loss: 5.1810\n",
      "Epoch [325/800], Avg D_loss: 0.1727, Avg G_loss: 6.2908\n",
      "Epoch [326/800], Step [2/53], D_loss: 0.0402, G_loss: 5.4978\n",
      "Epoch [326/800], Step [4/53], D_loss: 0.0087, G_loss: 3.6842\n",
      "Epoch [326/800], Step [6/53], D_loss: 0.0134, G_loss: 3.0403\n",
      "Epoch [326/800], Step [8/53], D_loss: 0.3405, G_loss: 3.5809\n",
      "Epoch [326/800], Step [10/53], D_loss: 0.0854, G_loss: 5.2469\n",
      "Epoch [326/800], Step [12/53], D_loss: 0.0139, G_loss: 3.4796\n",
      "Epoch [326/800], Step [14/53], D_loss: 0.0731, G_loss: 4.6085\n",
      "Epoch [326/800], Step [16/53], D_loss: 0.3112, G_loss: 3.0205\n",
      "Epoch [326/800], Step [18/53], D_loss: 0.1844, G_loss: 6.1570\n",
      "Epoch [326/800], Step [20/53], D_loss: 0.1734, G_loss: 1.5794\n",
      "Epoch [326/800], Step [22/53], D_loss: 0.0460, G_loss: 7.0040\n",
      "Epoch [326/800], Step [24/53], D_loss: 0.2225, G_loss: 1.5918\n",
      "Epoch [326/800], Step [26/53], D_loss: 0.1065, G_loss: 2.8930\n",
      "Epoch [326/800], Step [28/53], D_loss: 0.0772, G_loss: 4.5291\n",
      "Epoch [326/800], Step [30/53], D_loss: 0.3752, G_loss: 5.9498\n",
      "Epoch [326/800], Step [32/53], D_loss: 0.0629, G_loss: 0.9599\n",
      "Epoch [326/800], Step [34/53], D_loss: 0.1007, G_loss: 4.5669\n",
      "Epoch [326/800], Step [36/53], D_loss: 0.0361, G_loss: 4.9704\n",
      "Epoch [326/800], Step [38/53], D_loss: 0.1316, G_loss: 4.7593\n",
      "Epoch [326/800], Step [40/53], D_loss: 0.0318, G_loss: 3.7871\n",
      "Epoch [326/800], Step [42/53], D_loss: 0.0127, G_loss: 4.8596\n",
      "Epoch [326/800], Step [44/53], D_loss: 0.0962, G_loss: 4.4382\n",
      "Epoch [326/800], Step [46/53], D_loss: 0.1138, G_loss: 4.9923\n",
      "Epoch [326/800], Step [48/53], D_loss: 0.0131, G_loss: 6.8001\n",
      "Epoch [326/800], Step [50/53], D_loss: 0.1842, G_loss: 5.0674\n",
      "Epoch [326/800], Step [52/53], D_loss: 1.0644, G_loss: 6.8948\n",
      "Epoch [326/800], Avg D_loss: 0.1429, Avg G_loss: 4.6284\n",
      "Epoch [327/800], Step [2/53], D_loss: 0.3519, G_loss: 4.6316\n",
      "Epoch [327/800], Step [4/53], D_loss: 0.2356, G_loss: 4.6499\n",
      "Epoch [327/800], Step [6/53], D_loss: 0.0734, G_loss: 4.3945\n",
      "Epoch [327/800], Step [8/53], D_loss: 0.1988, G_loss: 5.7948\n",
      "Epoch [327/800], Step [10/53], D_loss: 0.1444, G_loss: 2.8664\n",
      "Epoch [327/800], Step [12/53], D_loss: 0.0440, G_loss: 4.1946\n",
      "Epoch [327/800], Step [14/53], D_loss: 0.0896, G_loss: 4.8740\n",
      "Epoch [327/800], Step [16/53], D_loss: 0.0829, G_loss: 4.9321\n",
      "Epoch [327/800], Step [18/53], D_loss: 0.0673, G_loss: 4.1650\n",
      "Epoch [327/800], Step [20/53], D_loss: 0.1290, G_loss: 3.7481\n",
      "Epoch [327/800], Step [22/53], D_loss: 0.0642, G_loss: 6.3122\n",
      "Epoch [327/800], Step [24/53], D_loss: 0.0377, G_loss: 5.9012\n",
      "Epoch [327/800], Step [26/53], D_loss: 0.0127, G_loss: 5.1994\n",
      "Epoch [327/800], Step [28/53], D_loss: 0.0185, G_loss: 5.3552\n",
      "Epoch [327/800], Step [30/53], D_loss: 0.6889, G_loss: 7.5919\n",
      "Epoch [327/800], Step [32/53], D_loss: 0.1665, G_loss: 7.1129\n",
      "Epoch [327/800], Step [34/53], D_loss: 0.0147, G_loss: 6.9751\n",
      "Epoch [327/800], Step [36/53], D_loss: 0.0547, G_loss: 5.3853\n",
      "Epoch [327/800], Step [38/53], D_loss: 0.0324, G_loss: 6.2418\n",
      "Epoch [327/800], Step [40/53], D_loss: 0.2982, G_loss: 5.9599\n",
      "Epoch [327/800], Step [42/53], D_loss: 0.7043, G_loss: 7.5985\n",
      "Epoch [327/800], Step [44/53], D_loss: 0.1587, G_loss: 7.3461\n",
      "Epoch [327/800], Step [46/53], D_loss: 0.3183, G_loss: 1.7634\n",
      "Epoch [327/800], Step [48/53], D_loss: 0.2149, G_loss: 5.1037\n",
      "Epoch [327/800], Step [50/53], D_loss: 0.0252, G_loss: 4.9151\n",
      "Epoch [327/800], Step [52/53], D_loss: 0.1252, G_loss: 5.4843\n",
      "Epoch [327/800], Avg D_loss: 0.1329, Avg G_loss: 4.9879\n",
      "Epoch [328/800], Step [2/53], D_loss: 0.0040, G_loss: 4.7891\n",
      "Epoch [328/800], Step [4/53], D_loss: 0.1044, G_loss: 2.6463\n",
      "Epoch [328/800], Step [6/53], D_loss: 0.0092, G_loss: 2.3614\n",
      "Epoch [328/800], Step [8/53], D_loss: 0.0099, G_loss: 7.6207\n",
      "Epoch [328/800], Step [10/53], D_loss: 0.0504, G_loss: 6.1143\n",
      "Epoch [328/800], Step [12/53], D_loss: 0.0182, G_loss: 7.2570\n",
      "Epoch [328/800], Step [14/53], D_loss: 0.0228, G_loss: 3.1885\n",
      "Epoch [328/800], Step [16/53], D_loss: 0.1126, G_loss: 3.4226\n",
      "Epoch [328/800], Step [18/53], D_loss: 0.0051, G_loss: 5.6861\n",
      "Epoch [328/800], Step [20/53], D_loss: 0.0499, G_loss: 5.3848\n",
      "Epoch [328/800], Step [22/53], D_loss: 0.3080, G_loss: 3.4978\n",
      "Epoch [328/800], Step [24/53], D_loss: 0.3570, G_loss: 6.6694\n",
      "Epoch [328/800], Step [26/53], D_loss: 0.1942, G_loss: 7.3017\n",
      "Epoch [328/800], Step [28/53], D_loss: 0.1629, G_loss: 8.0696\n",
      "Epoch [328/800], Step [30/53], D_loss: 0.0137, G_loss: 6.4639\n",
      "Epoch [328/800], Step [32/53], D_loss: 0.1497, G_loss: 2.1831\n",
      "Epoch [328/800], Step [34/53], D_loss: 0.2444, G_loss: 2.7843\n",
      "Epoch [328/800], Step [36/53], D_loss: 0.3804, G_loss: 4.3167\n",
      "Epoch [328/800], Step [38/53], D_loss: 0.0298, G_loss: 5.0144\n",
      "Epoch [328/800], Step [40/53], D_loss: 0.0449, G_loss: 4.4745\n",
      "Epoch [328/800], Step [42/53], D_loss: 0.0562, G_loss: 2.2814\n",
      "Epoch [328/800], Step [44/53], D_loss: 0.1179, G_loss: 7.4245\n",
      "Epoch [328/800], Step [46/53], D_loss: 0.4234, G_loss: 2.2453\n",
      "Epoch [328/800], Step [48/53], D_loss: 0.4635, G_loss: 7.2549\n",
      "Epoch [328/800], Step [50/53], D_loss: 0.1269, G_loss: 6.4140\n",
      "Epoch [328/800], Step [52/53], D_loss: 0.0654, G_loss: 5.9509\n",
      "Epoch [328/800], Avg D_loss: 0.1367, Avg G_loss: 5.3033\n",
      "Epoch [329/800], Step [2/53], D_loss: 0.1745, G_loss: 3.4495\n",
      "Epoch [329/800], Step [4/53], D_loss: 0.1438, G_loss: 1.8294\n",
      "Epoch [329/800], Step [6/53], D_loss: 0.3705, G_loss: 2.2052\n",
      "Epoch [329/800], Step [8/53], D_loss: 0.0591, G_loss: 1.1418\n",
      "Epoch [329/800], Step [10/53], D_loss: 0.0588, G_loss: 2.4846\n",
      "Epoch [329/800], Step [12/53], D_loss: 0.3217, G_loss: 3.0653\n",
      "Epoch [329/800], Step [14/53], D_loss: 0.0633, G_loss: 4.1213\n",
      "Epoch [329/800], Step [16/53], D_loss: 0.0763, G_loss: 3.0833\n",
      "Epoch [329/800], Step [18/53], D_loss: 0.1018, G_loss: 3.0377\n",
      "Epoch [329/800], Step [20/53], D_loss: 0.1236, G_loss: 4.7347\n",
      "Epoch [329/800], Step [22/53], D_loss: 0.0622, G_loss: 5.3008\n",
      "Epoch [329/800], Step [24/53], D_loss: 0.0250, G_loss: 5.9205\n",
      "Epoch [329/800], Step [26/53], D_loss: 0.0321, G_loss: 4.6356\n",
      "Epoch [329/800], Step [28/53], D_loss: 0.0924, G_loss: 5.9899\n",
      "Epoch [329/800], Step [30/53], D_loss: 0.0695, G_loss: 4.9087\n",
      "Epoch [329/800], Step [32/53], D_loss: 0.0537, G_loss: 6.6800\n",
      "Epoch [329/800], Step [34/53], D_loss: 0.1758, G_loss: 4.2231\n",
      "Epoch [329/800], Step [36/53], D_loss: 0.0215, G_loss: 4.6462\n",
      "Epoch [329/800], Step [38/53], D_loss: 0.1920, G_loss: 6.5361\n",
      "Epoch [329/800], Step [40/53], D_loss: 0.0624, G_loss: 6.0653\n",
      "Epoch [329/800], Step [42/53], D_loss: 0.0147, G_loss: 4.3189\n",
      "Epoch [329/800], Step [44/53], D_loss: 0.0175, G_loss: 4.5476\n",
      "Epoch [329/800], Step [46/53], D_loss: 0.0224, G_loss: 8.8505\n",
      "Epoch [329/800], Step [48/53], D_loss: 0.1938, G_loss: 6.0828\n",
      "Epoch [329/800], Step [50/53], D_loss: 0.1703, G_loss: 7.5854\n",
      "Epoch [329/800], Step [52/53], D_loss: 0.0536, G_loss: 6.7316\n",
      "Epoch [329/800], Avg D_loss: 0.1362, Avg G_loss: 4.9526\n",
      "Epoch [330/800], Step [2/53], D_loss: 0.1784, G_loss: 4.8608\n",
      "Epoch [330/800], Step [4/53], D_loss: 0.0056, G_loss: 7.3986\n",
      "Epoch [330/800], Step [6/53], D_loss: 0.0148, G_loss: 8.0884\n",
      "Epoch [330/800], Step [8/53], D_loss: 0.0528, G_loss: 3.5768\n",
      "Epoch [330/800], Step [10/53], D_loss: 0.0703, G_loss: 4.9584\n",
      "Epoch [330/800], Step [12/53], D_loss: 0.0214, G_loss: 5.2006\n",
      "Epoch [330/800], Step [14/53], D_loss: 0.1669, G_loss: 6.8458\n",
      "Epoch [330/800], Step [16/53], D_loss: 0.0094, G_loss: 4.6568\n",
      "Epoch [330/800], Step [18/53], D_loss: 0.0181, G_loss: 6.8422\n",
      "Epoch [330/800], Step [20/53], D_loss: 0.0279, G_loss: 2.5801\n",
      "Epoch [330/800], Step [22/53], D_loss: 0.0706, G_loss: 4.0722\n",
      "Epoch [330/800], Step [24/53], D_loss: 0.0667, G_loss: 7.6015\n",
      "Epoch [330/800], Step [26/53], D_loss: 0.0329, G_loss: 6.1844\n",
      "Epoch [330/800], Step [28/53], D_loss: 0.0781, G_loss: 6.4155\n",
      "Epoch [330/800], Step [30/53], D_loss: 0.0332, G_loss: 4.5943\n",
      "Epoch [330/800], Step [32/53], D_loss: 0.0590, G_loss: 5.4735\n",
      "Epoch [330/800], Step [34/53], D_loss: 0.0030, G_loss: 7.7179\n",
      "Epoch [330/800], Step [36/53], D_loss: 0.0149, G_loss: 7.2840\n",
      "Epoch [330/800], Step [38/53], D_loss: 0.0084, G_loss: 7.0152\n",
      "Epoch [330/800], Step [40/53], D_loss: 0.0079, G_loss: 8.1216\n",
      "Epoch [330/800], Step [42/53], D_loss: 0.0243, G_loss: 7.5127\n",
      "Epoch [330/800], Step [44/53], D_loss: 0.0716, G_loss: 2.1915\n",
      "Epoch [330/800], Step [46/53], D_loss: 0.0014, G_loss: 9.3606\n",
      "Epoch [330/800], Step [48/53], D_loss: 0.0080, G_loss: 7.5952\n",
      "Epoch [330/800], Step [50/53], D_loss: 0.0066, G_loss: 10.0463\n",
      "Epoch [330/800], Step [52/53], D_loss: 0.0377, G_loss: 7.8681\n",
      "Epoch [330/800], Avg D_loss: 0.0507, Avg G_loss: 6.2555\n",
      "Epoch [331/800], Step [2/53], D_loss: 0.2468, G_loss: 7.9663\n",
      "Epoch [331/800], Step [4/53], D_loss: 0.0461, G_loss: 7.9033\n",
      "Epoch [331/800], Step [6/53], D_loss: 0.0472, G_loss: 6.9827\n",
      "Epoch [331/800], Step [8/53], D_loss: 0.0548, G_loss: 8.7159\n",
      "Epoch [331/800], Step [10/53], D_loss: 0.0013, G_loss: 6.1463\n",
      "Epoch [331/800], Step [12/53], D_loss: 0.1340, G_loss: 10.8936\n",
      "Epoch [331/800], Step [14/53], D_loss: 0.0033, G_loss: 2.7722\n",
      "Epoch [331/800], Step [16/53], D_loss: 0.1574, G_loss: 5.7888\n",
      "Epoch [331/800], Step [18/53], D_loss: 0.0487, G_loss: 3.3196\n",
      "Epoch [331/800], Step [20/53], D_loss: 0.0234, G_loss: 7.7419\n",
      "Epoch [331/800], Step [22/53], D_loss: 0.0161, G_loss: 3.6126\n",
      "Epoch [331/800], Step [24/53], D_loss: 0.0354, G_loss: 7.7611\n",
      "Epoch [331/800], Step [26/53], D_loss: 0.1315, G_loss: 5.8228\n",
      "Epoch [331/800], Step [28/53], D_loss: 0.0570, G_loss: 8.0972\n",
      "Epoch [331/800], Step [30/53], D_loss: 0.0045, G_loss: 10.4721\n",
      "Epoch [331/800], Step [32/53], D_loss: 0.0028, G_loss: 5.9005\n",
      "Epoch [331/800], Step [34/53], D_loss: 2.4583, G_loss: 7.6282\n",
      "Epoch [331/800], Step [36/53], D_loss: 0.0303, G_loss: 2.4962\n",
      "Epoch [331/800], Step [38/53], D_loss: 0.0277, G_loss: 2.6623\n",
      "Epoch [331/800], Step [40/53], D_loss: 0.0394, G_loss: 4.4713\n",
      "Epoch [331/800], Step [42/53], D_loss: 0.0190, G_loss: 4.1323\n",
      "Epoch [331/800], Step [44/53], D_loss: 0.0348, G_loss: 2.6330\n",
      "Epoch [331/800], Step [46/53], D_loss: 0.0887, G_loss: 6.0914\n",
      "Epoch [331/800], Step [48/53], D_loss: 0.0628, G_loss: 3.1146\n",
      "Epoch [331/800], Step [50/53], D_loss: 0.0636, G_loss: 4.4905\n",
      "Epoch [331/800], Step [52/53], D_loss: 0.0285, G_loss: 6.4182\n",
      "Epoch [331/800], Avg D_loss: 0.1480, Avg G_loss: 5.9370\n",
      "Epoch [332/800], Step [2/53], D_loss: 0.0228, G_loss: 3.8424\n",
      "Epoch [332/800], Step [4/53], D_loss: 0.0545, G_loss: 4.3372\n",
      "Epoch [332/800], Step [6/53], D_loss: 0.3660, G_loss: 1.0227\n",
      "Epoch [332/800], Step [8/53], D_loss: 0.0597, G_loss: 3.3487\n",
      "Epoch [332/800], Step [10/53], D_loss: 0.0348, G_loss: 2.5818\n",
      "Epoch [332/800], Step [12/53], D_loss: 0.4620, G_loss: 5.1303\n",
      "Epoch [332/800], Step [14/53], D_loss: 0.2613, G_loss: 8.7030\n",
      "Epoch [332/800], Step [16/53], D_loss: 0.0237, G_loss: 8.1922\n",
      "Epoch [332/800], Step [18/53], D_loss: 0.1462, G_loss: 1.5680\n",
      "Epoch [332/800], Step [20/53], D_loss: 0.0879, G_loss: 3.9881\n",
      "Epoch [332/800], Step [22/53], D_loss: 0.1788, G_loss: 0.7608\n",
      "Epoch [332/800], Step [24/53], D_loss: 0.0568, G_loss: 3.1357\n",
      "Epoch [332/800], Step [26/53], D_loss: 0.0212, G_loss: 5.6728\n",
      "Epoch [332/800], Step [28/53], D_loss: 0.5476, G_loss: 0.4401\n",
      "Epoch [332/800], Step [30/53], D_loss: 0.1941, G_loss: 4.0408\n",
      "Epoch [332/800], Step [32/53], D_loss: 0.0619, G_loss: 5.8489\n",
      "Epoch [332/800], Step [34/53], D_loss: 0.3077, G_loss: 1.9891\n",
      "Epoch [332/800], Step [36/53], D_loss: 0.0973, G_loss: 5.0573\n",
      "Epoch [332/800], Step [38/53], D_loss: 0.0751, G_loss: 2.0037\n",
      "Epoch [332/800], Step [40/53], D_loss: 0.0968, G_loss: 4.7479\n",
      "Epoch [332/800], Step [42/53], D_loss: 0.0899, G_loss: 2.5475\n",
      "Epoch [332/800], Step [44/53], D_loss: 0.2012, G_loss: 6.3920\n",
      "Epoch [332/800], Step [46/53], D_loss: 0.1266, G_loss: 4.6240\n",
      "Epoch [332/800], Step [48/53], D_loss: 0.0098, G_loss: 4.8625\n",
      "Epoch [332/800], Step [50/53], D_loss: 0.1331, G_loss: 6.0228\n",
      "Epoch [332/800], Step [52/53], D_loss: 0.3735, G_loss: 4.0659\n",
      "Epoch [332/800], Avg D_loss: 0.1621, Avg G_loss: 4.0102\n",
      "Epoch [333/800], Step [2/53], D_loss: 0.0162, G_loss: 6.9220\n",
      "Epoch [333/800], Step [4/53], D_loss: 0.0279, G_loss: 6.6521\n",
      "Epoch [333/800], Step [6/53], D_loss: 0.1048, G_loss: 5.2132\n",
      "Epoch [333/800], Step [8/53], D_loss: 0.0679, G_loss: 3.9257\n",
      "Epoch [333/800], Step [10/53], D_loss: 0.2217, G_loss: 7.4357\n",
      "Epoch [333/800], Step [12/53], D_loss: 0.0454, G_loss: 3.1755\n",
      "Epoch [333/800], Step [14/53], D_loss: 0.0339, G_loss: 3.8926\n",
      "Epoch [333/800], Step [16/53], D_loss: 0.2086, G_loss: 7.0163\n",
      "Epoch [333/800], Step [18/53], D_loss: 0.0123, G_loss: 3.5431\n",
      "Epoch [333/800], Step [20/53], D_loss: 0.0179, G_loss: 8.3278\n",
      "Epoch [333/800], Step [22/53], D_loss: 0.0477, G_loss: 4.2131\n",
      "Epoch [333/800], Step [24/53], D_loss: 0.4810, G_loss: 7.4461\n",
      "Epoch [333/800], Step [26/53], D_loss: 0.1005, G_loss: 2.4237\n",
      "Epoch [333/800], Step [28/53], D_loss: 0.1023, G_loss: 4.8536\n",
      "Epoch [333/800], Step [30/53], D_loss: 0.0442, G_loss: 3.8749\n",
      "Epoch [333/800], Step [32/53], D_loss: 0.2228, G_loss: 4.5842\n",
      "Epoch [333/800], Step [34/53], D_loss: 0.0269, G_loss: 4.4671\n",
      "Epoch [333/800], Step [36/53], D_loss: 0.0228, G_loss: 3.2518\n",
      "Epoch [333/800], Step [38/53], D_loss: 0.1073, G_loss: 5.3442\n",
      "Epoch [333/800], Step [40/53], D_loss: 0.1046, G_loss: 5.1777\n",
      "Epoch [333/800], Step [42/53], D_loss: 0.1261, G_loss: 5.5685\n",
      "Epoch [333/800], Step [44/53], D_loss: 0.0022, G_loss: 6.9736\n",
      "Epoch [333/800], Step [46/53], D_loss: 0.0842, G_loss: 5.3707\n",
      "Epoch [333/800], Step [48/53], D_loss: 0.1196, G_loss: 8.4266\n",
      "Epoch [333/800], Step [50/53], D_loss: 0.0617, G_loss: 4.6290\n",
      "Epoch [333/800], Step [52/53], D_loss: 0.2290, G_loss: 6.3053\n",
      "Epoch [333/800], Avg D_loss: 0.1333, Avg G_loss: 5.2101\n",
      "Epoch [334/800], Step [2/53], D_loss: 0.0144, G_loss: 3.8423\n",
      "Epoch [334/800], Step [4/53], D_loss: 0.0867, G_loss: 4.7086\n",
      "Epoch [334/800], Step [6/53], D_loss: 0.0479, G_loss: 4.7776\n",
      "Epoch [334/800], Step [8/53], D_loss: 0.2009, G_loss: 6.9721\n",
      "Epoch [334/800], Step [10/53], D_loss: 0.8402, G_loss: 1.8229\n",
      "Epoch [334/800], Step [12/53], D_loss: 0.3115, G_loss: 6.7148\n",
      "Epoch [334/800], Step [14/53], D_loss: 0.0772, G_loss: 1.3050\n",
      "Epoch [334/800], Step [16/53], D_loss: 0.2285, G_loss: 5.1863\n",
      "Epoch [334/800], Step [18/53], D_loss: 0.0975, G_loss: 3.5757\n",
      "Epoch [334/800], Step [20/53], D_loss: 0.1408, G_loss: 4.5152\n",
      "Epoch [334/800], Step [22/53], D_loss: 0.0464, G_loss: 5.6835\n",
      "Epoch [334/800], Step [24/53], D_loss: 0.0485, G_loss: 3.5584\n",
      "Epoch [334/800], Step [26/53], D_loss: 0.0274, G_loss: 3.8207\n",
      "Epoch [334/800], Step [28/53], D_loss: 0.0461, G_loss: 4.5211\n",
      "Epoch [334/800], Step [30/53], D_loss: 0.1737, G_loss: 4.8695\n",
      "Epoch [334/800], Step [32/53], D_loss: 0.0774, G_loss: 6.3751\n",
      "Epoch [334/800], Step [34/53], D_loss: 0.1220, G_loss: 3.9266\n",
      "Epoch [334/800], Step [36/53], D_loss: 0.0557, G_loss: 7.2040\n",
      "Epoch [334/800], Step [38/53], D_loss: 0.1678, G_loss: 5.7449\n",
      "Epoch [334/800], Step [40/53], D_loss: 0.0211, G_loss: 7.0629\n",
      "Epoch [334/800], Step [42/53], D_loss: 0.0008, G_loss: 4.3671\n",
      "Epoch [334/800], Step [44/53], D_loss: 0.1125, G_loss: 4.9136\n",
      "Epoch [334/800], Step [46/53], D_loss: 0.0194, G_loss: 3.5712\n",
      "Epoch [334/800], Step [48/53], D_loss: 0.0476, G_loss: 9.7031\n",
      "Epoch [334/800], Step [50/53], D_loss: 0.4474, G_loss: 2.6566\n",
      "Epoch [334/800], Step [52/53], D_loss: 0.0300, G_loss: 7.4561\n",
      "Epoch [334/800], Avg D_loss: 0.1432, Avg G_loss: 5.1149\n",
      "Epoch [335/800], Step [2/53], D_loss: 0.1730, G_loss: 7.9270\n",
      "Epoch [335/800], Step [4/53], D_loss: 0.1745, G_loss: 2.4720\n",
      "Epoch [335/800], Step [6/53], D_loss: 0.0426, G_loss: 10.0453\n",
      "Epoch [335/800], Step [8/53], D_loss: 0.0751, G_loss: 2.7036\n",
      "Epoch [335/800], Step [10/53], D_loss: 0.0279, G_loss: 3.9329\n",
      "Epoch [335/800], Step [12/53], D_loss: 0.0533, G_loss: 5.0435\n",
      "Epoch [335/800], Step [14/53], D_loss: 0.2540, G_loss: 4.7005\n",
      "Epoch [335/800], Step [16/53], D_loss: 0.1129, G_loss: 4.4742\n",
      "Epoch [335/800], Step [18/53], D_loss: 0.0184, G_loss: 3.7535\n",
      "Epoch [335/800], Step [20/53], D_loss: 0.2242, G_loss: 4.1577\n",
      "Epoch [335/800], Step [22/53], D_loss: 0.2215, G_loss: 3.7929\n",
      "Epoch [335/800], Step [24/53], D_loss: 0.0300, G_loss: 6.6132\n",
      "Epoch [335/800], Step [26/53], D_loss: 0.1139, G_loss: 6.2456\n",
      "Epoch [335/800], Step [28/53], D_loss: 0.0007, G_loss: 7.6737\n",
      "Epoch [335/800], Step [30/53], D_loss: 0.8142, G_loss: 6.8960\n",
      "Epoch [335/800], Step [32/53], D_loss: 0.5347, G_loss: 4.2666\n",
      "Epoch [335/800], Step [34/53], D_loss: 0.2180, G_loss: 3.0152\n",
      "Epoch [335/800], Step [36/53], D_loss: 0.8758, G_loss: 2.1016\n",
      "Epoch [335/800], Step [38/53], D_loss: 0.0868, G_loss: 3.5704\n",
      "Epoch [335/800], Step [40/53], D_loss: 0.0613, G_loss: 0.9926\n",
      "Epoch [335/800], Step [42/53], D_loss: 0.0834, G_loss: 4.3842\n",
      "Epoch [335/800], Step [44/53], D_loss: 0.1059, G_loss: 5.1610\n",
      "Epoch [335/800], Step [46/53], D_loss: 0.0836, G_loss: 2.7719\n",
      "Epoch [335/800], Step [48/53], D_loss: 0.0786, G_loss: 5.2388\n",
      "Epoch [335/800], Step [50/53], D_loss: 0.0438, G_loss: 5.3090\n",
      "Epoch [335/800], Step [52/53], D_loss: 0.1559, G_loss: 6.3627\n",
      "Epoch [335/800], Avg D_loss: 0.1804, Avg G_loss: 4.6415\n",
      "Epoch [336/800], Step [2/53], D_loss: 0.0225, G_loss: 6.9365\n",
      "Epoch [336/800], Step [4/53], D_loss: 0.1185, G_loss: 6.0504\n",
      "Epoch [336/800], Step [6/53], D_loss: 0.0596, G_loss: 3.6895\n",
      "Epoch [336/800], Step [8/53], D_loss: 0.0672, G_loss: 4.8707\n",
      "Epoch [336/800], Step [10/53], D_loss: 0.1898, G_loss: 2.5491\n",
      "Epoch [336/800], Step [12/53], D_loss: 0.0427, G_loss: 3.8149\n",
      "Epoch [336/800], Step [14/53], D_loss: 0.4780, G_loss: 2.5758\n",
      "Epoch [336/800], Step [16/53], D_loss: 0.0265, G_loss: 3.9389\n",
      "Epoch [336/800], Step [18/53], D_loss: 0.1522, G_loss: 6.1656\n",
      "Epoch [336/800], Step [20/53], D_loss: 0.0401, G_loss: 2.9957\n",
      "Epoch [336/800], Step [22/53], D_loss: 0.3713, G_loss: 4.6097\n",
      "Epoch [336/800], Step [24/53], D_loss: 0.3216, G_loss: 1.3205\n",
      "Epoch [336/800], Step [26/53], D_loss: 0.0675, G_loss: 3.4519\n",
      "Epoch [336/800], Step [28/53], D_loss: 0.0049, G_loss: 4.6190\n",
      "Epoch [336/800], Step [30/53], D_loss: 0.1103, G_loss: 2.5789\n",
      "Epoch [336/800], Step [32/53], D_loss: 0.3900, G_loss: 4.7626\n",
      "Epoch [336/800], Step [34/53], D_loss: 0.0439, G_loss: 2.3087\n",
      "Epoch [336/800], Step [36/53], D_loss: 0.0728, G_loss: 9.5482\n",
      "Epoch [336/800], Step [38/53], D_loss: 0.0765, G_loss: 2.8819\n",
      "Epoch [336/800], Step [40/53], D_loss: 0.0689, G_loss: 5.9909\n",
      "Epoch [336/800], Step [42/53], D_loss: 0.0470, G_loss: 1.2257\n",
      "Epoch [336/800], Step [44/53], D_loss: 0.1726, G_loss: 3.4969\n",
      "Epoch [336/800], Step [46/53], D_loss: 0.4004, G_loss: 2.6695\n",
      "Epoch [336/800], Step [48/53], D_loss: 0.6045, G_loss: 4.8179\n",
      "Epoch [336/800], Step [50/53], D_loss: 0.1151, G_loss: 5.9793\n",
      "Epoch [336/800], Step [52/53], D_loss: 0.0624, G_loss: 1.4957\n",
      "Epoch [336/800], Avg D_loss: 0.2002, Avg G_loss: 4.0359\n",
      "Epoch [337/800], Step [2/53], D_loss: 0.1595, G_loss: 3.5029\n",
      "Epoch [337/800], Step [4/53], D_loss: 0.0815, G_loss: 4.8649\n",
      "Epoch [337/800], Step [6/53], D_loss: 0.0411, G_loss: 2.0792\n",
      "Epoch [337/800], Step [8/53], D_loss: 0.1058, G_loss: 5.2683\n",
      "Epoch [337/800], Step [10/53], D_loss: 0.0251, G_loss: 3.7076\n",
      "Epoch [337/800], Step [12/53], D_loss: 0.1198, G_loss: 5.8599\n",
      "Epoch [337/800], Step [14/53], D_loss: 0.0244, G_loss: 2.9021\n",
      "Epoch [337/800], Step [16/53], D_loss: 0.0395, G_loss: 2.8804\n",
      "Epoch [337/800], Step [18/53], D_loss: 0.0368, G_loss: 6.2950\n",
      "Epoch [337/800], Step [20/53], D_loss: 0.0096, G_loss: 3.4619\n",
      "Epoch [337/800], Step [22/53], D_loss: 0.0187, G_loss: 5.5806\n",
      "Epoch [337/800], Step [24/53], D_loss: 0.0107, G_loss: 5.5776\n",
      "Epoch [337/800], Step [26/53], D_loss: 0.0689, G_loss: 4.3511\n",
      "Epoch [337/800], Step [28/53], D_loss: 0.1326, G_loss: 5.6952\n",
      "Epoch [337/800], Step [30/53], D_loss: 0.3167, G_loss: 6.8199\n",
      "Epoch [337/800], Step [32/53], D_loss: 0.0714, G_loss: 4.9232\n",
      "Epoch [337/800], Step [34/53], D_loss: 0.0541, G_loss: 5.3150\n",
      "Epoch [337/800], Step [36/53], D_loss: 0.1731, G_loss: 8.5098\n",
      "Epoch [337/800], Step [38/53], D_loss: 0.0283, G_loss: 7.1851\n",
      "Epoch [337/800], Step [40/53], D_loss: 0.0090, G_loss: 6.8958\n",
      "Epoch [337/800], Step [42/53], D_loss: 0.0328, G_loss: 6.6087\n",
      "Epoch [337/800], Step [44/53], D_loss: 0.0190, G_loss: 4.2193\n",
      "Epoch [337/800], Step [46/53], D_loss: 0.0237, G_loss: 3.6801\n",
      "Epoch [337/800], Step [48/53], D_loss: 0.1808, G_loss: 4.5340\n",
      "Epoch [337/800], Step [50/53], D_loss: 0.1425, G_loss: 5.7076\n",
      "Epoch [337/800], Step [52/53], D_loss: 0.0853, G_loss: 6.7641\n",
      "Epoch [337/800], Avg D_loss: 0.0761, Avg G_loss: 5.1464\n",
      "Epoch [338/800], Step [2/53], D_loss: 0.0699, G_loss: 3.7342\n",
      "Epoch [338/800], Step [4/53], D_loss: 0.0626, G_loss: 6.5357\n",
      "Epoch [338/800], Step [6/53], D_loss: 0.0108, G_loss: 8.8340\n",
      "Epoch [338/800], Step [8/53], D_loss: 0.1012, G_loss: 6.1031\n",
      "Epoch [338/800], Step [10/53], D_loss: 0.1609, G_loss: 4.8810\n",
      "Epoch [338/800], Step [12/53], D_loss: 0.1460, G_loss: 10.3955\n",
      "Epoch [338/800], Step [14/53], D_loss: 0.0229, G_loss: 5.8073\n",
      "Epoch [338/800], Step [16/53], D_loss: 0.0527, G_loss: 3.1799\n",
      "Epoch [338/800], Step [18/53], D_loss: 0.0871, G_loss: 5.8654\n",
      "Epoch [338/800], Step [20/53], D_loss: 0.0207, G_loss: 7.7496\n",
      "Epoch [338/800], Step [22/53], D_loss: 0.0216, G_loss: 5.5985\n",
      "Epoch [338/800], Step [24/53], D_loss: 0.0254, G_loss: 5.4702\n",
      "Epoch [338/800], Step [26/53], D_loss: 0.6994, G_loss: 6.5073\n",
      "Epoch [338/800], Step [28/53], D_loss: 0.0355, G_loss: 1.5639\n",
      "Epoch [338/800], Step [30/53], D_loss: 0.0734, G_loss: 2.8143\n",
      "Epoch [338/800], Step [32/53], D_loss: 0.0527, G_loss: 7.7181\n",
      "Epoch [338/800], Step [34/53], D_loss: 0.0121, G_loss: 6.6279\n",
      "Epoch [338/800], Step [36/53], D_loss: 0.0424, G_loss: 3.3406\n",
      "Epoch [338/800], Step [38/53], D_loss: 0.1965, G_loss: 5.2851\n",
      "Epoch [338/800], Step [40/53], D_loss: 0.0074, G_loss: 7.5894\n",
      "Epoch [338/800], Step [42/53], D_loss: 0.0072, G_loss: 2.9321\n",
      "Epoch [338/800], Step [44/53], D_loss: 0.0405, G_loss: 3.2655\n",
      "Epoch [338/800], Step [46/53], D_loss: 0.0228, G_loss: 5.6469\n",
      "Epoch [338/800], Step [48/53], D_loss: 0.0312, G_loss: 10.9087\n",
      "Epoch [338/800], Step [50/53], D_loss: 0.1272, G_loss: 1.7198\n",
      "Epoch [338/800], Step [52/53], D_loss: 0.0893, G_loss: 5.0923\n",
      "Epoch [338/800], Avg D_loss: 0.1091, Avg G_loss: 5.7523\n",
      "Epoch [339/800], Step [2/53], D_loss: 0.0696, G_loss: 5.5820\n",
      "Epoch [339/800], Step [4/53], D_loss: 0.0315, G_loss: 4.0777\n",
      "Epoch [339/800], Step [6/53], D_loss: 0.0727, G_loss: 6.8438\n",
      "Epoch [339/800], Step [8/53], D_loss: 0.0457, G_loss: 7.2666\n",
      "Epoch [339/800], Step [10/53], D_loss: 0.2705, G_loss: 5.2199\n",
      "Epoch [339/800], Step [12/53], D_loss: 0.0191, G_loss: 5.3575\n",
      "Epoch [339/800], Step [14/53], D_loss: 0.0851, G_loss: 5.0308\n",
      "Epoch [339/800], Step [16/53], D_loss: 0.0665, G_loss: 6.1696\n",
      "Epoch [339/800], Step [18/53], D_loss: 0.0159, G_loss: 4.3287\n",
      "Epoch [339/800], Step [20/53], D_loss: 0.2317, G_loss: 1.6209\n",
      "Epoch [339/800], Step [22/53], D_loss: 0.4538, G_loss: 6.4105\n",
      "Epoch [339/800], Step [24/53], D_loss: 0.7461, G_loss: 6.3343\n",
      "Epoch [339/800], Step [26/53], D_loss: 0.0618, G_loss: 4.0210\n",
      "Epoch [339/800], Step [28/53], D_loss: 0.2783, G_loss: 2.6602\n",
      "Epoch [339/800], Step [30/53], D_loss: 0.1006, G_loss: 3.2373\n",
      "Epoch [339/800], Step [32/53], D_loss: 0.1612, G_loss: 5.8664\n",
      "Epoch [339/800], Step [34/53], D_loss: 0.0671, G_loss: 3.5043\n",
      "Epoch [339/800], Step [36/53], D_loss: 0.1207, G_loss: 6.1695\n",
      "Epoch [339/800], Step [38/53], D_loss: 0.0060, G_loss: 6.1732\n",
      "Epoch [339/800], Step [40/53], D_loss: 0.0103, G_loss: 3.3526\n",
      "Epoch [339/800], Step [42/53], D_loss: 0.0641, G_loss: 4.7751\n",
      "Epoch [339/800], Step [44/53], D_loss: 0.0328, G_loss: 4.4879\n",
      "Epoch [339/800], Step [46/53], D_loss: 0.1304, G_loss: 6.5184\n",
      "Epoch [339/800], Step [48/53], D_loss: 0.0494, G_loss: 4.6510\n",
      "Epoch [339/800], Step [50/53], D_loss: 0.0474, G_loss: 6.8449\n",
      "Epoch [339/800], Step [52/53], D_loss: 0.0606, G_loss: 5.4982\n",
      "Epoch [339/800], Avg D_loss: 0.1039, Avg G_loss: 5.1862\n",
      "Epoch [340/800], Step [2/53], D_loss: 0.0447, G_loss: 7.0270\n",
      "Epoch [340/800], Step [4/53], D_loss: 0.0182, G_loss: 8.4026\n",
      "Epoch [340/800], Step [6/53], D_loss: 0.0061, G_loss: 3.0520\n",
      "Epoch [340/800], Step [8/53], D_loss: 0.1982, G_loss: 10.0066\n",
      "Epoch [340/800], Step [10/53], D_loss: 0.4513, G_loss: 2.7710\n",
      "Epoch [340/800], Step [12/53], D_loss: 0.0071, G_loss: 8.2319\n",
      "Epoch [340/800], Step [14/53], D_loss: 0.0448, G_loss: 2.1029\n",
      "Epoch [340/800], Step [16/53], D_loss: 0.0162, G_loss: 4.0262\n",
      "Epoch [340/800], Step [18/53], D_loss: 0.0440, G_loss: 3.6665\n",
      "Epoch [340/800], Step [20/53], D_loss: 0.1652, G_loss: 5.1532\n",
      "Epoch [340/800], Step [22/53], D_loss: 0.0597, G_loss: 2.4894\n",
      "Epoch [340/800], Step [24/53], D_loss: 0.1264, G_loss: 5.2206\n",
      "Epoch [340/800], Step [26/53], D_loss: 0.6804, G_loss: 2.5915\n",
      "Epoch [340/800], Step [28/53], D_loss: 0.0326, G_loss: 2.9492\n",
      "Epoch [340/800], Step [30/53], D_loss: 0.0477, G_loss: 2.5787\n",
      "Epoch [340/800], Step [32/53], D_loss: 0.1186, G_loss: 3.9372\n",
      "Epoch [340/800], Step [34/53], D_loss: 0.0149, G_loss: 3.1213\n",
      "Epoch [340/800], Step [36/53], D_loss: 0.0662, G_loss: 3.0381\n",
      "Epoch [340/800], Step [38/53], D_loss: 0.1190, G_loss: 3.3526\n",
      "Epoch [340/800], Step [40/53], D_loss: 0.0766, G_loss: 4.9651\n",
      "Epoch [340/800], Step [42/53], D_loss: 0.1144, G_loss: 2.2218\n",
      "Epoch [340/800], Step [44/53], D_loss: 0.0442, G_loss: 3.5917\n",
      "Epoch [340/800], Step [46/53], D_loss: 0.1215, G_loss: 2.8909\n",
      "Epoch [340/800], Step [48/53], D_loss: 0.0652, G_loss: 4.3099\n",
      "Epoch [340/800], Step [50/53], D_loss: 0.0191, G_loss: 5.4978\n",
      "Epoch [340/800], Step [52/53], D_loss: 0.0481, G_loss: 4.0126\n",
      "Epoch [340/800], Avg D_loss: 0.1482, Avg G_loss: 4.4507\n",
      "Generated images saved as generated_images/epoch_340.png\n",
      "Epoch [341/800], Step [2/53], D_loss: 0.0061, G_loss: 2.8936\n",
      "Epoch [341/800], Step [4/53], D_loss: 0.0106, G_loss: 7.3790\n",
      "Epoch [341/800], Step [6/53], D_loss: 0.0028, G_loss: 3.5908\n",
      "Epoch [341/800], Step [8/53], D_loss: 0.1156, G_loss: 4.9450\n",
      "Epoch [341/800], Step [10/53], D_loss: 0.0070, G_loss: 7.3661\n",
      "Epoch [341/800], Step [12/53], D_loss: 0.1367, G_loss: 6.0072\n",
      "Epoch [341/800], Step [14/53], D_loss: 0.0194, G_loss: 5.9579\n",
      "Epoch [341/800], Step [16/53], D_loss: 0.0301, G_loss: 7.8980\n",
      "Epoch [341/800], Step [18/53], D_loss: 0.1363, G_loss: 6.5250\n",
      "Epoch [341/800], Step [20/53], D_loss: 0.0251, G_loss: 6.8848\n",
      "Epoch [341/800], Step [22/53], D_loss: 0.0192, G_loss: 7.4589\n",
      "Epoch [341/800], Step [24/53], D_loss: 0.0609, G_loss: 5.0501\n",
      "Epoch [341/800], Step [26/53], D_loss: 0.0085, G_loss: 6.5678\n",
      "Epoch [341/800], Step [28/53], D_loss: 0.0255, G_loss: 10.1633\n",
      "Epoch [341/800], Step [30/53], D_loss: 0.0705, G_loss: 5.8838\n",
      "Epoch [341/800], Step [32/53], D_loss: 0.2384, G_loss: 7.0851\n",
      "Epoch [341/800], Step [34/53], D_loss: 0.2396, G_loss: 8.9043\n",
      "Epoch [341/800], Step [36/53], D_loss: 0.3120, G_loss: 3.0755\n",
      "Epoch [341/800], Step [38/53], D_loss: 0.0262, G_loss: 3.9958\n",
      "Epoch [341/800], Step [40/53], D_loss: 0.0924, G_loss: 6.2196\n",
      "Epoch [341/800], Step [42/53], D_loss: 0.0081, G_loss: 8.3613\n",
      "Epoch [341/800], Step [44/53], D_loss: 0.0488, G_loss: 5.8415\n",
      "Epoch [341/800], Step [46/53], D_loss: 0.0272, G_loss: 5.3976\n",
      "Epoch [341/800], Step [48/53], D_loss: 0.1959, G_loss: 6.1118\n",
      "Epoch [341/800], Step [50/53], D_loss: 0.0175, G_loss: 6.5020\n",
      "Epoch [341/800], Step [52/53], D_loss: 0.0034, G_loss: 5.1078\n",
      "Epoch [341/800], Avg D_loss: 0.0811, Avg G_loss: 6.2155\n",
      "Epoch [342/800], Step [2/53], D_loss: 0.0103, G_loss: 4.6688\n",
      "Epoch [342/800], Step [4/53], D_loss: 0.0522, G_loss: 3.8689\n",
      "Epoch [342/800], Step [6/53], D_loss: 0.0152, G_loss: 3.9079\n",
      "Epoch [342/800], Step [8/53], D_loss: 0.0325, G_loss: 5.3802\n",
      "Epoch [342/800], Step [10/53], D_loss: 0.2225, G_loss: 5.9571\n",
      "Epoch [342/800], Step [12/53], D_loss: 0.1107, G_loss: 4.4696\n",
      "Epoch [342/800], Step [14/53], D_loss: 0.0656, G_loss: 6.0834\n",
      "Epoch [342/800], Step [16/53], D_loss: 0.0288, G_loss: 2.3119\n",
      "Epoch [342/800], Step [18/53], D_loss: 0.0509, G_loss: 4.7058\n",
      "Epoch [342/800], Step [20/53], D_loss: 0.0561, G_loss: 5.3690\n",
      "Epoch [342/800], Step [22/53], D_loss: 0.0241, G_loss: 4.2143\n",
      "Epoch [342/800], Step [24/53], D_loss: 0.0202, G_loss: 6.8168\n",
      "Epoch [342/800], Step [26/53], D_loss: 0.4067, G_loss: 1.3223\n",
      "Epoch [342/800], Step [28/53], D_loss: 0.2508, G_loss: 7.8900\n",
      "Epoch [342/800], Step [30/53], D_loss: 0.0556, G_loss: 1.3630\n",
      "Epoch [342/800], Step [32/53], D_loss: 0.2329, G_loss: 6.1241\n",
      "Epoch [342/800], Step [34/53], D_loss: 0.2589, G_loss: 2.5148\n",
      "Epoch [342/800], Step [36/53], D_loss: 0.1425, G_loss: 2.3621\n",
      "Epoch [342/800], Step [38/53], D_loss: 0.1324, G_loss: 2.6777\n",
      "Epoch [342/800], Step [40/53], D_loss: 0.1267, G_loss: 2.5527\n",
      "Epoch [342/800], Step [42/53], D_loss: 0.1808, G_loss: 2.9443\n",
      "Epoch [342/800], Step [44/53], D_loss: 0.2407, G_loss: 3.8789\n",
      "Epoch [342/800], Step [46/53], D_loss: 0.2378, G_loss: 3.4893\n",
      "Epoch [342/800], Step [48/53], D_loss: 0.0700, G_loss: 3.7208\n",
      "Epoch [342/800], Step [50/53], D_loss: 0.7335, G_loss: 7.5365\n",
      "Epoch [342/800], Step [52/53], D_loss: 0.5011, G_loss: 2.8349\n",
      "Epoch [342/800], Avg D_loss: 0.2518, Avg G_loss: 4.1367\n",
      "Epoch [343/800], Step [2/53], D_loss: 0.0879, G_loss: 2.2504\n",
      "Epoch [343/800], Step [4/53], D_loss: 0.0897, G_loss: 1.9917\n",
      "Epoch [343/800], Step [6/53], D_loss: 0.0581, G_loss: 4.4929\n",
      "Epoch [343/800], Step [8/53], D_loss: 0.3002, G_loss: 6.1796\n",
      "Epoch [343/800], Step [10/53], D_loss: 0.0729, G_loss: 2.8765\n",
      "Epoch [343/800], Step [12/53], D_loss: 0.9563, G_loss: 6.2185\n",
      "Epoch [343/800], Step [14/53], D_loss: 1.0208, G_loss: 3.5751\n",
      "Epoch [343/800], Step [16/53], D_loss: 0.2169, G_loss: 1.1521\n",
      "Epoch [343/800], Step [18/53], D_loss: 0.1342, G_loss: 2.8555\n",
      "Epoch [343/800], Step [20/53], D_loss: 0.1916, G_loss: 3.3694\n",
      "Epoch [343/800], Step [22/53], D_loss: 0.2067, G_loss: 1.8287\n",
      "Epoch [343/800], Step [24/53], D_loss: 0.1190, G_loss: 4.4777\n",
      "Epoch [343/800], Step [26/53], D_loss: 0.1297, G_loss: 3.5743\n",
      "Epoch [343/800], Step [28/53], D_loss: 0.3384, G_loss: 3.4765\n",
      "Epoch [343/800], Step [30/53], D_loss: 0.0497, G_loss: 3.1503\n",
      "Epoch [343/800], Step [32/53], D_loss: 0.1033, G_loss: 3.9237\n",
      "Epoch [343/800], Step [34/53], D_loss: 0.0932, G_loss: 0.9562\n",
      "Epoch [343/800], Step [36/53], D_loss: 0.0569, G_loss: 5.4409\n",
      "Epoch [343/800], Step [38/53], D_loss: 0.1568, G_loss: 3.4752\n",
      "Epoch [343/800], Step [40/53], D_loss: 0.3064, G_loss: 2.3609\n",
      "Epoch [343/800], Step [42/53], D_loss: 0.0450, G_loss: 5.2590\n",
      "Epoch [343/800], Step [44/53], D_loss: 0.1115, G_loss: 6.5150\n",
      "Epoch [343/800], Step [46/53], D_loss: 0.0346, G_loss: 4.2417\n",
      "Epoch [343/800], Step [48/53], D_loss: 0.1074, G_loss: 3.9158\n",
      "Epoch [343/800], Step [50/53], D_loss: 0.0947, G_loss: 5.0852\n",
      "Epoch [343/800], Step [52/53], D_loss: 0.0305, G_loss: 4.1494\n",
      "Epoch [343/800], Avg D_loss: 0.2098, Avg G_loss: 3.7970\n",
      "Epoch [344/800], Step [2/53], D_loss: 0.0391, G_loss: 5.2837\n",
      "Epoch [344/800], Step [4/53], D_loss: 0.0364, G_loss: 7.4982\n",
      "Epoch [344/800], Step [6/53], D_loss: 0.0771, G_loss: 4.2678\n",
      "Epoch [344/800], Step [8/53], D_loss: 0.1566, G_loss: 5.6419\n",
      "Epoch [344/800], Step [10/53], D_loss: 0.0439, G_loss: 6.9571\n",
      "Epoch [344/800], Step [12/53], D_loss: 0.1960, G_loss: 6.6664\n",
      "Epoch [344/800], Step [14/53], D_loss: 0.0358, G_loss: 5.0948\n",
      "Epoch [344/800], Step [16/53], D_loss: 0.0189, G_loss: 2.7300\n",
      "Epoch [344/800], Step [18/53], D_loss: 0.0315, G_loss: 8.0571\n",
      "Epoch [344/800], Step [20/53], D_loss: 0.0188, G_loss: 2.7628\n",
      "Epoch [344/800], Step [22/53], D_loss: 0.0587, G_loss: 5.3634\n",
      "Epoch [344/800], Step [24/53], D_loss: 0.0834, G_loss: 7.9155\n",
      "Epoch [344/800], Step [26/53], D_loss: 0.1162, G_loss: 5.4441\n",
      "Epoch [344/800], Step [28/53], D_loss: 0.0084, G_loss: 6.4526\n",
      "Epoch [344/800], Step [30/53], D_loss: 0.0166, G_loss: 4.6429\n",
      "Epoch [344/800], Step [32/53], D_loss: 0.0158, G_loss: 4.8638\n",
      "Epoch [344/800], Step [34/53], D_loss: 0.0356, G_loss: 8.0055\n",
      "Epoch [344/800], Step [36/53], D_loss: 0.0212, G_loss: 4.4639\n",
      "Epoch [344/800], Step [38/53], D_loss: 0.0200, G_loss: 6.3105\n",
      "Epoch [344/800], Step [40/53], D_loss: 0.0386, G_loss: 7.9841\n",
      "Epoch [344/800], Step [42/53], D_loss: 0.1005, G_loss: 4.6262\n",
      "Epoch [344/800], Step [44/53], D_loss: 0.0283, G_loss: 5.5257\n",
      "Epoch [344/800], Step [46/53], D_loss: 0.0164, G_loss: 6.9341\n",
      "Epoch [344/800], Step [48/53], D_loss: 0.0948, G_loss: 6.4552\n",
      "Epoch [344/800], Step [50/53], D_loss: 0.0159, G_loss: 7.3277\n",
      "Epoch [344/800], Step [52/53], D_loss: 0.0050, G_loss: 4.8551\n",
      "Epoch [344/800], Avg D_loss: 0.0817, Avg G_loss: 6.0422\n",
      "Epoch [345/800], Step [2/53], D_loss: 0.0672, G_loss: 4.3918\n",
      "Epoch [345/800], Step [4/53], D_loss: 0.0081, G_loss: 8.1128\n",
      "Epoch [345/800], Step [6/53], D_loss: 0.0061, G_loss: 8.3280\n",
      "Epoch [345/800], Step [8/53], D_loss: 0.0189, G_loss: 8.3903\n",
      "Epoch [345/800], Step [10/53], D_loss: 0.0102, G_loss: 5.1922\n",
      "Epoch [345/800], Step [12/53], D_loss: 0.0048, G_loss: 6.3242\n",
      "Epoch [345/800], Step [14/53], D_loss: 0.0021, G_loss: 8.1058\n",
      "Epoch [345/800], Step [16/53], D_loss: 0.0019, G_loss: 3.5760\n",
      "Epoch [345/800], Step [18/53], D_loss: 0.0433, G_loss: 5.1907\n",
      "Epoch [345/800], Step [20/53], D_loss: 0.0023, G_loss: 7.0309\n",
      "Epoch [345/800], Step [22/53], D_loss: 0.0253, G_loss: 9.0213\n",
      "Epoch [345/800], Step [24/53], D_loss: 0.0695, G_loss: 5.0035\n",
      "Epoch [345/800], Step [26/53], D_loss: 0.0027, G_loss: 6.6643\n",
      "Epoch [345/800], Step [28/53], D_loss: 0.0019, G_loss: 8.8984\n",
      "Epoch [345/800], Step [30/53], D_loss: 0.0345, G_loss: 6.6340\n",
      "Epoch [345/800], Step [32/53], D_loss: 0.0050, G_loss: 0.9856\n",
      "Epoch [345/800], Step [34/53], D_loss: 0.4036, G_loss: 11.8854\n",
      "Epoch [345/800], Step [36/53], D_loss: 3.2765, G_loss: 4.6491\n",
      "Epoch [345/800], Step [38/53], D_loss: 0.0675, G_loss: 0.7770\n",
      "Epoch [345/800], Step [40/53], D_loss: 0.1761, G_loss: 3.8822\n",
      "Epoch [345/800], Step [42/53], D_loss: 0.0490, G_loss: 3.1063\n",
      "Epoch [345/800], Step [44/53], D_loss: 0.0622, G_loss: 2.9671\n",
      "Epoch [345/800], Step [46/53], D_loss: 0.0334, G_loss: 4.5541\n",
      "Epoch [345/800], Step [48/53], D_loss: 0.0384, G_loss: 3.1349\n",
      "Epoch [345/800], Step [50/53], D_loss: 0.1141, G_loss: 5.1048\n",
      "Epoch [345/800], Step [52/53], D_loss: 0.0823, G_loss: 3.9826\n",
      "Epoch [345/800], Avg D_loss: 0.1276, Avg G_loss: 5.6979\n",
      "Epoch [346/800], Step [2/53], D_loss: 0.0165, G_loss: 4.3573\n",
      "Epoch [346/800], Step [4/53], D_loss: 0.2884, G_loss: 4.5357\n",
      "Epoch [346/800], Step [6/53], D_loss: 0.0375, G_loss: 4.2513\n",
      "Epoch [346/800], Step [8/53], D_loss: 0.0094, G_loss: 4.8147\n",
      "Epoch [346/800], Step [10/53], D_loss: 0.1450, G_loss: 6.7805\n",
      "Epoch [346/800], Step [12/53], D_loss: 0.0844, G_loss: 4.7458\n",
      "Epoch [346/800], Step [14/53], D_loss: 0.0167, G_loss: 5.7843\n",
      "Epoch [346/800], Step [16/53], D_loss: 0.0243, G_loss: 4.8628\n",
      "Epoch [346/800], Step [18/53], D_loss: 0.0268, G_loss: 7.5924\n",
      "Epoch [346/800], Step [20/53], D_loss: 0.2310, G_loss: 2.9353\n",
      "Epoch [346/800], Step [22/53], D_loss: 0.0915, G_loss: 5.3668\n",
      "Epoch [346/800], Step [24/53], D_loss: 0.0349, G_loss: 5.1716\n",
      "Epoch [346/800], Step [26/53], D_loss: 0.0194, G_loss: 4.6587\n",
      "Epoch [346/800], Step [28/53], D_loss: 2.3711, G_loss: 5.3831\n",
      "Epoch [346/800], Step [30/53], D_loss: 0.0411, G_loss: 2.2449\n",
      "Epoch [346/800], Step [32/53], D_loss: 0.1027, G_loss: 3.1014\n",
      "Epoch [346/800], Step [34/53], D_loss: 0.4202, G_loss: 2.0587\n",
      "Epoch [346/800], Step [36/53], D_loss: 0.0521, G_loss: 3.1007\n",
      "Epoch [346/800], Step [38/53], D_loss: 0.0692, G_loss: 3.0951\n",
      "Epoch [346/800], Step [40/53], D_loss: 0.2245, G_loss: 3.2035\n",
      "Epoch [346/800], Step [42/53], D_loss: 0.1369, G_loss: 2.5472\n",
      "Epoch [346/800], Step [44/53], D_loss: 0.0254, G_loss: 4.2828\n",
      "Epoch [346/800], Step [46/53], D_loss: 0.1485, G_loss: 3.4063\n",
      "Epoch [346/800], Step [48/53], D_loss: 0.1467, G_loss: 4.2794\n",
      "Epoch [346/800], Step [50/53], D_loss: 0.1461, G_loss: 5.6758\n",
      "Epoch [346/800], Step [52/53], D_loss: 0.0305, G_loss: 4.5339\n",
      "Epoch [346/800], Avg D_loss: 0.1571, Avg G_loss: 4.4266\n",
      "Epoch [347/800], Step [2/53], D_loss: 0.0528, G_loss: 4.3813\n",
      "Epoch [347/800], Step [4/53], D_loss: 0.2134, G_loss: 4.8053\n",
      "Epoch [347/800], Step [6/53], D_loss: 0.0537, G_loss: 4.3334\n",
      "Epoch [347/800], Step [8/53], D_loss: 0.0397, G_loss: 4.3646\n",
      "Epoch [347/800], Step [10/53], D_loss: 0.1056, G_loss: 5.9583\n",
      "Epoch [347/800], Step [12/53], D_loss: 0.3779, G_loss: 3.9055\n",
      "Epoch [347/800], Step [14/53], D_loss: 0.0487, G_loss: 5.3059\n",
      "Epoch [347/800], Step [16/53], D_loss: 0.0407, G_loss: 5.4774\n",
      "Epoch [347/800], Step [18/53], D_loss: 0.0377, G_loss: 1.7376\n",
      "Epoch [347/800], Step [20/53], D_loss: 0.0140, G_loss: 6.0245\n",
      "Epoch [347/800], Step [22/53], D_loss: 0.1967, G_loss: 7.6734\n",
      "Epoch [347/800], Step [24/53], D_loss: 0.1320, G_loss: 2.8450\n",
      "Epoch [347/800], Step [26/53], D_loss: 0.1948, G_loss: 6.6102\n",
      "Epoch [347/800], Step [28/53], D_loss: 0.0450, G_loss: 1.3699\n",
      "Epoch [347/800], Step [30/53], D_loss: 0.7653, G_loss: 6.6065\n",
      "Epoch [347/800], Step [32/53], D_loss: 0.3367, G_loss: 4.7585\n",
      "Epoch [347/800], Step [34/53], D_loss: 0.0743, G_loss: 2.7391\n",
      "Epoch [347/800], Step [36/53], D_loss: 0.0734, G_loss: 3.4075\n",
      "Epoch [347/800], Step [38/53], D_loss: 0.1905, G_loss: 3.0354\n",
      "Epoch [347/800], Step [40/53], D_loss: 0.0483, G_loss: 4.4170\n",
      "Epoch [347/800], Step [42/53], D_loss: 0.0819, G_loss: 4.3013\n",
      "Epoch [347/800], Step [44/53], D_loss: 0.1093, G_loss: 4.1402\n",
      "Epoch [347/800], Step [46/53], D_loss: 0.2841, G_loss: 3.2460\n",
      "Epoch [347/800], Step [48/53], D_loss: 0.0691, G_loss: 4.3829\n",
      "Epoch [347/800], Step [50/53], D_loss: 0.0783, G_loss: 6.1560\n",
      "Epoch [347/800], Step [52/53], D_loss: 0.0363, G_loss: 5.9892\n",
      "Epoch [347/800], Avg D_loss: 0.1630, Avg G_loss: 4.4163\n",
      "Epoch [348/800], Step [2/53], D_loss: 0.0427, G_loss: 4.3476\n",
      "Epoch [348/800], Step [4/53], D_loss: 0.1520, G_loss: 5.6942\n",
      "Epoch [348/800], Step [6/53], D_loss: 0.0985, G_loss: 4.8167\n",
      "Epoch [348/800], Step [8/53], D_loss: 0.0449, G_loss: 5.0660\n",
      "Epoch [348/800], Step [10/53], D_loss: 0.0113, G_loss: 5.3094\n",
      "Epoch [348/800], Step [12/53], D_loss: 0.1381, G_loss: 4.7352\n",
      "Epoch [348/800], Step [14/53], D_loss: 0.1131, G_loss: 3.4541\n",
      "Epoch [348/800], Step [16/53], D_loss: 0.0078, G_loss: 6.2528\n",
      "Epoch [348/800], Step [18/53], D_loss: 0.0662, G_loss: 8.4160\n",
      "Epoch [348/800], Step [20/53], D_loss: 0.0111, G_loss: 4.3800\n",
      "Epoch [348/800], Step [22/53], D_loss: 0.0166, G_loss: 6.0994\n",
      "Epoch [348/800], Step [24/53], D_loss: 0.0082, G_loss: 6.7247\n",
      "Epoch [348/800], Step [26/53], D_loss: 0.0096, G_loss: 5.7952\n",
      "Epoch [348/800], Step [28/53], D_loss: 0.0448, G_loss: 2.5740\n",
      "Epoch [348/800], Step [30/53], D_loss: 0.0121, G_loss: 4.2095\n",
      "Epoch [348/800], Step [32/53], D_loss: 0.0066, G_loss: 3.5904\n",
      "Epoch [348/800], Step [34/53], D_loss: 0.0638, G_loss: 7.0057\n",
      "Epoch [348/800], Step [36/53], D_loss: 0.0602, G_loss: 3.8310\n",
      "Epoch [348/800], Step [38/53], D_loss: 0.2371, G_loss: 6.7042\n",
      "Epoch [348/800], Step [40/53], D_loss: 0.9361, G_loss: 8.2743\n",
      "Epoch [348/800], Step [42/53], D_loss: 0.0604, G_loss: 5.8115\n",
      "Epoch [348/800], Step [44/53], D_loss: 0.1418, G_loss: 3.8973\n",
      "Epoch [348/800], Step [46/53], D_loss: 0.0463, G_loss: 3.9684\n",
      "Epoch [348/800], Step [48/53], D_loss: 0.0531, G_loss: 4.7136\n",
      "Epoch [348/800], Step [50/53], D_loss: 0.0852, G_loss: 4.3545\n",
      "Epoch [348/800], Step [52/53], D_loss: 0.0935, G_loss: 3.6725\n",
      "Epoch [348/800], Avg D_loss: 0.1113, Avg G_loss: 5.0207\n",
      "Epoch [349/800], Step [2/53], D_loss: 0.0433, G_loss: 6.8515\n",
      "Epoch [349/800], Step [4/53], D_loss: 0.0062, G_loss: 4.7003\n",
      "Epoch [349/800], Step [6/53], D_loss: 0.1078, G_loss: 4.9841\n",
      "Epoch [349/800], Step [8/53], D_loss: 0.1095, G_loss: 5.1608\n",
      "Epoch [349/800], Step [10/53], D_loss: 0.1078, G_loss: 3.3289\n",
      "Epoch [349/800], Step [12/53], D_loss: 0.3727, G_loss: 7.1509\n",
      "Epoch [349/800], Step [14/53], D_loss: 0.0801, G_loss: 2.2251\n",
      "Epoch [349/800], Step [16/53], D_loss: 0.0569, G_loss: 5.1619\n",
      "Epoch [349/800], Step [18/53], D_loss: 0.0386, G_loss: 4.5173\n",
      "Epoch [349/800], Step [20/53], D_loss: 0.0186, G_loss: 3.7985\n",
      "Epoch [349/800], Step [22/53], D_loss: 0.1507, G_loss: 4.6102\n",
      "Epoch [349/800], Step [24/53], D_loss: 0.0277, G_loss: 6.6594\n",
      "Epoch [349/800], Step [26/53], D_loss: 0.0734, G_loss: 4.7054\n",
      "Epoch [349/800], Step [28/53], D_loss: 0.0112, G_loss: 4.9199\n",
      "Epoch [349/800], Step [30/53], D_loss: 0.0941, G_loss: 4.8843\n",
      "Epoch [349/800], Step [32/53], D_loss: 0.0122, G_loss: 7.4767\n",
      "Epoch [349/800], Step [34/53], D_loss: 0.0265, G_loss: 7.5562\n",
      "Epoch [349/800], Step [36/53], D_loss: 0.0106, G_loss: 5.1700\n",
      "Epoch [349/800], Step [38/53], D_loss: 0.0441, G_loss: 5.1563\n",
      "Epoch [349/800], Step [40/53], D_loss: 0.0013, G_loss: 5.5080\n",
      "Epoch [349/800], Step [42/53], D_loss: 0.0027, G_loss: 8.8439\n",
      "Epoch [349/800], Step [44/53], D_loss: 0.0320, G_loss: 6.4126\n",
      "Epoch [349/800], Step [46/53], D_loss: 0.0278, G_loss: 6.7497\n",
      "Epoch [349/800], Step [48/53], D_loss: 0.0370, G_loss: 6.3510\n",
      "Epoch [349/800], Step [50/53], D_loss: 0.0214, G_loss: 8.7659\n",
      "Epoch [349/800], Step [52/53], D_loss: 0.0230, G_loss: 6.0368\n",
      "Epoch [349/800], Avg D_loss: 0.0604, Avg G_loss: 5.8492\n",
      "Epoch [350/800], Step [2/53], D_loss: 0.0007, G_loss: 5.3982\n",
      "Epoch [350/800], Step [4/53], D_loss: 0.0214, G_loss: 8.7286\n",
      "Epoch [350/800], Step [6/53], D_loss: 0.0569, G_loss: 5.9679\n",
      "Epoch [350/800], Step [8/53], D_loss: 0.0337, G_loss: 7.8058\n",
      "Epoch [350/800], Step [10/53], D_loss: 0.0533, G_loss: 1.6125\n",
      "Epoch [350/800], Step [12/53], D_loss: 0.1285, G_loss: 8.5656\n",
      "Epoch [350/800], Step [14/53], D_loss: 0.0251, G_loss: 9.6141\n",
      "Epoch [350/800], Step [16/53], D_loss: 0.0089, G_loss: 7.1375\n",
      "Epoch [350/800], Step [18/53], D_loss: 0.0158, G_loss: 5.5997\n",
      "Epoch [350/800], Step [20/53], D_loss: 0.0290, G_loss: 6.5522\n",
      "Epoch [350/800], Step [22/53], D_loss: 0.0272, G_loss: 9.8164\n",
      "Epoch [350/800], Step [24/53], D_loss: 0.0101, G_loss: 9.5214\n",
      "Epoch [350/800], Step [26/53], D_loss: 0.0615, G_loss: 4.1253\n",
      "Epoch [350/800], Step [28/53], D_loss: 0.0041, G_loss: 5.1995\n",
      "Epoch [350/800], Step [30/53], D_loss: 0.0073, G_loss: 7.2779\n",
      "Epoch [350/800], Step [32/53], D_loss: 0.0177, G_loss: 6.6895\n",
      "Epoch [350/800], Step [34/53], D_loss: 0.0829, G_loss: 9.5084\n",
      "Epoch [350/800], Step [36/53], D_loss: 0.0021, G_loss: 7.0019\n",
      "Epoch [350/800], Step [38/53], D_loss: 0.0273, G_loss: 6.9352\n",
      "Epoch [350/800], Step [40/53], D_loss: 0.0117, G_loss: 7.1290\n",
      "Epoch [350/800], Step [42/53], D_loss: 0.0110, G_loss: 7.2564\n",
      "Epoch [350/800], Step [44/53], D_loss: 0.0088, G_loss: 5.6699\n",
      "Epoch [350/800], Step [46/53], D_loss: 0.0781, G_loss: 7.4686\n",
      "Epoch [350/800], Step [48/53], D_loss: 0.1466, G_loss: 5.7485\n",
      "Epoch [350/800], Step [50/53], D_loss: 2.9004, G_loss: 5.4183\n",
      "Epoch [350/800], Step [52/53], D_loss: 0.4264, G_loss: 5.5077\n",
      "Epoch [350/800], Avg D_loss: 0.1071, Avg G_loss: 6.7579\n",
      "Models saved for epoch 350\n",
      "Epoch [351/800], Step [2/53], D_loss: 0.7504, G_loss: 2.4543\n",
      "Epoch [351/800], Step [4/53], D_loss: 0.3216, G_loss: 4.0318\n",
      "Epoch [351/800], Step [6/53], D_loss: 0.2952, G_loss: 4.7837\n",
      "Epoch [351/800], Step [8/53], D_loss: 0.1008, G_loss: 3.1908\n",
      "Epoch [351/800], Step [10/53], D_loss: 0.1145, G_loss: 3.4146\n",
      "Epoch [351/800], Step [12/53], D_loss: 0.0883, G_loss: 3.3543\n",
      "Epoch [351/800], Step [14/53], D_loss: 0.1406, G_loss: 4.3093\n",
      "Epoch [351/800], Step [16/53], D_loss: 0.1267, G_loss: 5.1671\n",
      "Epoch [351/800], Step [18/53], D_loss: 0.0215, G_loss: 2.8528\n",
      "Epoch [351/800], Step [20/53], D_loss: 0.1931, G_loss: 4.0341\n",
      "Epoch [351/800], Step [22/53], D_loss: 0.1766, G_loss: 4.6854\n",
      "Epoch [351/800], Step [24/53], D_loss: 0.0759, G_loss: 3.1504\n",
      "Epoch [351/800], Step [26/53], D_loss: 0.1114, G_loss: 4.1682\n",
      "Epoch [351/800], Step [28/53], D_loss: 0.0928, G_loss: 7.0066\n",
      "Epoch [351/800], Step [30/53], D_loss: 0.0627, G_loss: 5.7810\n",
      "Epoch [351/800], Step [32/53], D_loss: 0.0197, G_loss: 4.8008\n",
      "Epoch [351/800], Step [34/53], D_loss: 0.0034, G_loss: 6.7032\n",
      "Epoch [351/800], Step [36/53], D_loss: 0.0119, G_loss: 6.5620\n",
      "Epoch [351/800], Step [38/53], D_loss: 0.0089, G_loss: 6.3795\n",
      "Epoch [351/800], Step [40/53], D_loss: 0.0251, G_loss: 5.6380\n",
      "Epoch [351/800], Step [42/53], D_loss: 0.0087, G_loss: 6.4116\n",
      "Epoch [351/800], Step [44/53], D_loss: 0.0318, G_loss: 7.1562\n",
      "Epoch [351/800], Step [46/53], D_loss: 0.0699, G_loss: 3.8833\n",
      "Epoch [351/800], Step [48/53], D_loss: 0.2511, G_loss: 9.3866\n",
      "Epoch [351/800], Step [50/53], D_loss: 1.0650, G_loss: 3.1954\n",
      "Epoch [351/800], Step [52/53], D_loss: 0.0900, G_loss: 6.9557\n",
      "Epoch [351/800], Avg D_loss: 0.1316, Avg G_loss: 4.8937\n",
      "Epoch [352/800], Step [2/53], D_loss: 0.0042, G_loss: 5.9306\n",
      "Epoch [352/800], Step [4/53], D_loss: 0.0261, G_loss: 3.0858\n",
      "Epoch [352/800], Step [6/53], D_loss: 0.0201, G_loss: 5.4011\n",
      "Epoch [352/800], Step [8/53], D_loss: 0.2042, G_loss: 3.3104\n",
      "Epoch [352/800], Step [10/53], D_loss: 0.3929, G_loss: 5.4996\n",
      "Epoch [352/800], Step [12/53], D_loss: 0.0885, G_loss: 6.8818\n",
      "Epoch [352/800], Step [14/53], D_loss: 0.0156, G_loss: 2.8503\n",
      "Epoch [352/800], Step [16/53], D_loss: 0.0498, G_loss: 4.3531\n",
      "Epoch [352/800], Step [18/53], D_loss: 0.2024, G_loss: 3.6508\n",
      "Epoch [352/800], Step [20/53], D_loss: 0.0562, G_loss: 3.7499\n",
      "Epoch [352/800], Step [22/53], D_loss: 0.0888, G_loss: 2.8533\n",
      "Epoch [352/800], Step [24/53], D_loss: 0.0633, G_loss: 4.3790\n",
      "Epoch [352/800], Step [26/53], D_loss: 0.0776, G_loss: 3.7365\n",
      "Epoch [352/800], Step [28/53], D_loss: 0.0416, G_loss: 3.3478\n",
      "Epoch [352/800], Step [30/53], D_loss: 0.0295, G_loss: 9.1128\n",
      "Epoch [352/800], Step [32/53], D_loss: 0.0523, G_loss: 0.5228\n",
      "Epoch [352/800], Step [34/53], D_loss: 0.0667, G_loss: 8.1195\n",
      "Epoch [352/800], Step [36/53], D_loss: 0.4458, G_loss: 2.0565\n",
      "Epoch [352/800], Step [38/53], D_loss: 0.2399, G_loss: 3.8897\n",
      "Epoch [352/800], Step [40/53], D_loss: 0.4781, G_loss: 5.4953\n",
      "Epoch [352/800], Step [42/53], D_loss: 0.0114, G_loss: 3.2586\n",
      "Epoch [352/800], Step [44/53], D_loss: 0.2483, G_loss: 5.8319\n",
      "Epoch [352/800], Step [46/53], D_loss: 0.3364, G_loss: 3.7061\n",
      "Epoch [352/800], Step [48/53], D_loss: 0.0331, G_loss: 2.2549\n",
      "Epoch [352/800], Step [50/53], D_loss: 0.0392, G_loss: 6.9576\n",
      "Epoch [352/800], Step [52/53], D_loss: 0.0542, G_loss: 5.4923\n",
      "Epoch [352/800], Avg D_loss: 0.1150, Avg G_loss: 4.3946\n",
      "Epoch [353/800], Step [2/53], D_loss: 0.1151, G_loss: 3.2285\n",
      "Epoch [353/800], Step [4/53], D_loss: 0.1169, G_loss: 3.0009\n",
      "Epoch [353/800], Step [6/53], D_loss: 0.0086, G_loss: 5.3448\n",
      "Epoch [353/800], Step [8/53], D_loss: 0.0024, G_loss: 6.5303\n",
      "Epoch [353/800], Step [10/53], D_loss: 0.0979, G_loss: 3.7781\n",
      "Epoch [353/800], Step [12/53], D_loss: 0.0336, G_loss: 5.9408\n",
      "Epoch [353/800], Step [14/53], D_loss: 0.6357, G_loss: 6.5790\n",
      "Epoch [353/800], Step [16/53], D_loss: 0.0438, G_loss: 3.6581\n",
      "Epoch [353/800], Step [18/53], D_loss: 0.0546, G_loss: 4.8388\n",
      "Epoch [353/800], Step [20/53], D_loss: 0.0213, G_loss: 7.1705\n",
      "Epoch [353/800], Step [22/53], D_loss: 0.0750, G_loss: 5.8826\n",
      "Epoch [353/800], Step [24/53], D_loss: 0.0324, G_loss: 6.1566\n",
      "Epoch [353/800], Step [26/53], D_loss: 0.0093, G_loss: 5.8635\n",
      "Epoch [353/800], Step [28/53], D_loss: 0.0150, G_loss: 5.2153\n",
      "Epoch [353/800], Step [30/53], D_loss: 0.1660, G_loss: 7.9790\n",
      "Epoch [353/800], Step [32/53], D_loss: 0.0214, G_loss: 6.1937\n",
      "Epoch [353/800], Step [34/53], D_loss: 0.0371, G_loss: 9.5453\n",
      "Epoch [353/800], Step [36/53], D_loss: 0.0139, G_loss: 5.6866\n",
      "Epoch [353/800], Step [38/53], D_loss: 0.0054, G_loss: 4.9979\n",
      "Epoch [353/800], Step [40/53], D_loss: 0.0929, G_loss: 9.3329\n",
      "Epoch [353/800], Step [42/53], D_loss: 0.0402, G_loss: 2.8902\n",
      "Epoch [353/800], Step [44/53], D_loss: 0.1821, G_loss: 6.7736\n",
      "Epoch [353/800], Step [46/53], D_loss: 0.0739, G_loss: 7.2815\n",
      "Epoch [353/800], Step [48/53], D_loss: 0.0090, G_loss: 8.2659\n",
      "Epoch [353/800], Step [50/53], D_loss: 0.0666, G_loss: 7.7484\n",
      "Epoch [353/800], Step [52/53], D_loss: 0.0215, G_loss: 3.4619\n",
      "Epoch [353/800], Avg D_loss: 0.0823, Avg G_loss: 5.7265\n",
      "Epoch [354/800], Step [2/53], D_loss: 0.0383, G_loss: 8.0292\n",
      "Epoch [354/800], Step [4/53], D_loss: 0.0077, G_loss: 4.6793\n",
      "Epoch [354/800], Step [6/53], D_loss: 0.2279, G_loss: 7.9552\n",
      "Epoch [354/800], Step [8/53], D_loss: 0.1753, G_loss: 6.5437\n",
      "Epoch [354/800], Step [10/53], D_loss: 0.0145, G_loss: 5.5780\n",
      "Epoch [354/800], Step [12/53], D_loss: 0.0249, G_loss: 6.3987\n",
      "Epoch [354/800], Step [14/53], D_loss: 0.1176, G_loss: 7.8143\n",
      "Epoch [354/800], Step [16/53], D_loss: 0.0302, G_loss: 8.0139\n",
      "Epoch [354/800], Step [18/53], D_loss: 0.0026, G_loss: 5.1043\n",
      "Epoch [354/800], Step [20/53], D_loss: 0.0293, G_loss: 5.2378\n",
      "Epoch [354/800], Step [22/53], D_loss: 0.0702, G_loss: 7.8722\n",
      "Epoch [354/800], Step [24/53], D_loss: 0.0026, G_loss: 8.1962\n",
      "Epoch [354/800], Step [26/53], D_loss: 0.0029, G_loss: 5.1171\n",
      "Epoch [354/800], Step [28/53], D_loss: 0.1299, G_loss: 9.9515\n",
      "Epoch [354/800], Step [30/53], D_loss: 0.3332, G_loss: 5.4839\n",
      "Epoch [354/800], Step [32/53], D_loss: 0.1508, G_loss: 4.8281\n",
      "Epoch [354/800], Step [34/53], D_loss: 0.0015, G_loss: 8.2220\n",
      "Epoch [354/800], Step [36/53], D_loss: 0.0036, G_loss: 6.8837\n",
      "Epoch [354/800], Step [38/53], D_loss: 0.0112, G_loss: 4.5744\n",
      "Epoch [354/800], Step [40/53], D_loss: 0.5665, G_loss: 14.4409\n",
      "Epoch [354/800], Step [42/53], D_loss: 0.3348, G_loss: 5.9459\n",
      "Epoch [354/800], Step [44/53], D_loss: 0.2782, G_loss: 6.0886\n",
      "Epoch [354/800], Step [46/53], D_loss: 0.1215, G_loss: 6.5836\n",
      "Epoch [354/800], Step [48/53], D_loss: 0.3096, G_loss: 0.7055\n",
      "Epoch [354/800], Step [50/53], D_loss: 0.0217, G_loss: 11.5936\n",
      "Epoch [354/800], Step [52/53], D_loss: 0.0024, G_loss: 6.5104\n",
      "Epoch [354/800], Avg D_loss: 0.1161, Avg G_loss: 6.6733\n",
      "Epoch [355/800], Step [2/53], D_loss: 0.0299, G_loss: 6.5665\n",
      "Epoch [355/800], Step [4/53], D_loss: 0.0658, G_loss: 2.9904\n",
      "Epoch [355/800], Step [6/53], D_loss: 0.2281, G_loss: 4.2279\n",
      "Epoch [355/800], Step [8/53], D_loss: 0.2782, G_loss: 2.5806\n",
      "Epoch [355/800], Step [10/53], D_loss: 0.1721, G_loss: 3.2248\n",
      "Epoch [355/800], Step [12/53], D_loss: 0.1071, G_loss: 3.1671\n",
      "Epoch [355/800], Step [14/53], D_loss: 0.1074, G_loss: 1.9950\n",
      "Epoch [355/800], Step [16/53], D_loss: 0.1158, G_loss: 5.5860\n",
      "Epoch [355/800], Step [18/53], D_loss: 0.0800, G_loss: 5.3653\n",
      "Epoch [355/800], Step [20/53], D_loss: 0.1170, G_loss: 1.1406\n",
      "Epoch [355/800], Step [22/53], D_loss: 0.2396, G_loss: 6.1763\n",
      "Epoch [355/800], Step [24/53], D_loss: 0.1814, G_loss: 2.6877\n",
      "Epoch [355/800], Step [26/53], D_loss: 0.0434, G_loss: 2.8286\n",
      "Epoch [355/800], Step [28/53], D_loss: 0.1448, G_loss: 4.6423\n",
      "Epoch [355/800], Step [30/53], D_loss: 0.0639, G_loss: 5.5132\n",
      "Epoch [355/800], Step [32/53], D_loss: 0.0419, G_loss: 6.4327\n",
      "Epoch [355/800], Step [34/53], D_loss: 0.0705, G_loss: 2.9920\n",
      "Epoch [355/800], Step [36/53], D_loss: 0.0227, G_loss: 3.4584\n",
      "Epoch [355/800], Step [38/53], D_loss: 0.0083, G_loss: 9.6692\n",
      "Epoch [355/800], Step [40/53], D_loss: 0.0154, G_loss: 3.3069\n",
      "Epoch [355/800], Step [42/53], D_loss: 0.0940, G_loss: 4.6950\n",
      "Epoch [355/800], Step [44/53], D_loss: 0.0556, G_loss: 4.0599\n",
      "Epoch [355/800], Step [46/53], D_loss: 0.0410, G_loss: 6.2080\n",
      "Epoch [355/800], Step [48/53], D_loss: 0.1519, G_loss: 4.7458\n",
      "Epoch [355/800], Step [50/53], D_loss: 0.7474, G_loss: 3.5944\n",
      "Epoch [355/800], Step [52/53], D_loss: 0.0646, G_loss: 4.5003\n",
      "Epoch [355/800], Avg D_loss: 0.2263, Avg G_loss: 4.0596\n",
      "Epoch [356/800], Step [2/53], D_loss: 0.3289, G_loss: 2.8113\n",
      "Epoch [356/800], Step [4/53], D_loss: 0.3156, G_loss: 5.8619\n",
      "Epoch [356/800], Step [6/53], D_loss: 0.1788, G_loss: 3.1107\n",
      "Epoch [356/800], Step [8/53], D_loss: 0.0934, G_loss: 2.8921\n",
      "Epoch [356/800], Step [10/53], D_loss: 0.0442, G_loss: 2.4274\n",
      "Epoch [356/800], Step [12/53], D_loss: 0.0505, G_loss: 4.8380\n",
      "Epoch [356/800], Step [14/53], D_loss: 0.2270, G_loss: 3.7451\n",
      "Epoch [356/800], Step [16/53], D_loss: 0.0401, G_loss: 4.7863\n",
      "Epoch [356/800], Step [18/53], D_loss: 0.6870, G_loss: 5.0536\n",
      "Epoch [356/800], Step [20/53], D_loss: 0.4232, G_loss: 4.2808\n",
      "Epoch [356/800], Step [22/53], D_loss: 0.5167, G_loss: 4.0863\n",
      "Epoch [356/800], Step [24/53], D_loss: 0.0623, G_loss: 4.5457\n",
      "Epoch [356/800], Step [26/53], D_loss: 0.1179, G_loss: 1.6052\n",
      "Epoch [356/800], Step [28/53], D_loss: 0.0802, G_loss: 6.1409\n",
      "Epoch [356/800], Step [30/53], D_loss: 0.7834, G_loss: 1.9072\n",
      "Epoch [356/800], Step [32/53], D_loss: 0.2460, G_loss: 3.4805\n",
      "Epoch [356/800], Step [34/53], D_loss: 0.1917, G_loss: 2.2367\n",
      "Epoch [356/800], Step [36/53], D_loss: 0.4220, G_loss: 4.6291\n",
      "Epoch [356/800], Step [38/53], D_loss: 0.0966, G_loss: 4.1596\n",
      "Epoch [356/800], Step [40/53], D_loss: 0.0283, G_loss: 4.0658\n",
      "Epoch [356/800], Step [42/53], D_loss: 0.1314, G_loss: 4.6889\n",
      "Epoch [356/800], Step [44/53], D_loss: 0.0661, G_loss: 2.3788\n",
      "Epoch [356/800], Step [46/53], D_loss: 0.0134, G_loss: 2.8199\n",
      "Epoch [356/800], Step [48/53], D_loss: 0.0225, G_loss: 4.0605\n",
      "Epoch [356/800], Step [50/53], D_loss: 0.0812, G_loss: 7.5003\n",
      "Epoch [356/800], Step [52/53], D_loss: 0.1578, G_loss: 3.5205\n",
      "Epoch [356/800], Avg D_loss: 0.1886, Avg G_loss: 3.9842\n",
      "Epoch [357/800], Step [2/53], D_loss: 0.0537, G_loss: 7.3728\n",
      "Epoch [357/800], Step [4/53], D_loss: 0.0595, G_loss: 0.6711\n",
      "Epoch [357/800], Step [6/53], D_loss: 0.0072, G_loss: 3.7832\n",
      "Epoch [357/800], Step [8/53], D_loss: 0.0715, G_loss: 7.5520\n",
      "Epoch [357/800], Step [10/53], D_loss: 0.0432, G_loss: 1.5328\n",
      "Epoch [357/800], Step [12/53], D_loss: 0.0335, G_loss: 6.7834\n",
      "Epoch [357/800], Step [14/53], D_loss: 0.1853, G_loss: 8.5334\n",
      "Epoch [357/800], Step [16/53], D_loss: 0.2885, G_loss: 3.1760\n",
      "Epoch [357/800], Step [18/53], D_loss: 0.0557, G_loss: 3.7158\n",
      "Epoch [357/800], Step [20/53], D_loss: 0.3922, G_loss: 6.4933\n",
      "Epoch [357/800], Step [22/53], D_loss: 0.0417, G_loss: 4.1135\n",
      "Epoch [357/800], Step [24/53], D_loss: 0.0079, G_loss: 6.5573\n",
      "Epoch [357/800], Step [26/53], D_loss: 0.4702, G_loss: 1.3134\n",
      "Epoch [357/800], Step [28/53], D_loss: 1.7710, G_loss: 9.5775\n",
      "Epoch [357/800], Step [30/53], D_loss: 0.6443, G_loss: 4.8685\n",
      "Epoch [357/800], Step [32/53], D_loss: 0.1029, G_loss: 2.4634\n",
      "Epoch [357/800], Step [34/53], D_loss: 0.2370, G_loss: 4.9031\n",
      "Epoch [357/800], Step [36/53], D_loss: 0.3150, G_loss: 2.0851\n",
      "Epoch [357/800], Step [38/53], D_loss: 0.2493, G_loss: 2.3934\n",
      "Epoch [357/800], Step [40/53], D_loss: 0.1364, G_loss: 6.2717\n",
      "Epoch [357/800], Step [42/53], D_loss: 0.0307, G_loss: 3.2758\n",
      "Epoch [357/800], Step [44/53], D_loss: 0.0846, G_loss: 4.4990\n",
      "Epoch [357/800], Step [46/53], D_loss: 0.0193, G_loss: 2.6306\n",
      "Epoch [357/800], Step [48/53], D_loss: 0.1086, G_loss: 3.4982\n",
      "Epoch [357/800], Step [50/53], D_loss: 0.0504, G_loss: 4.9504\n",
      "Epoch [357/800], Step [52/53], D_loss: 0.0687, G_loss: 5.0638\n",
      "Epoch [357/800], Avg D_loss: 0.1787, Avg G_loss: 4.6588\n",
      "Epoch [358/800], Step [2/53], D_loss: 0.1828, G_loss: 2.9841\n",
      "Epoch [358/800], Step [4/53], D_loss: 0.2165, G_loss: 4.6244\n",
      "Epoch [358/800], Step [6/53], D_loss: 0.2526, G_loss: 4.6842\n",
      "Epoch [358/800], Step [8/53], D_loss: 0.0467, G_loss: 3.4366\n",
      "Epoch [358/800], Step [10/53], D_loss: 0.0462, G_loss: 5.2587\n",
      "Epoch [358/800], Step [12/53], D_loss: 0.1288, G_loss: 6.1689\n",
      "Epoch [358/800], Step [14/53], D_loss: 0.0871, G_loss: 7.2797\n",
      "Epoch [358/800], Step [16/53], D_loss: 0.0427, G_loss: 3.9787\n",
      "Epoch [358/800], Step [18/53], D_loss: 0.1102, G_loss: 6.6345\n",
      "Epoch [358/800], Step [20/53], D_loss: 0.0249, G_loss: 6.5091\n",
      "Epoch [358/800], Step [22/53], D_loss: 0.0418, G_loss: 6.6585\n",
      "Epoch [358/800], Step [24/53], D_loss: 0.0628, G_loss: 4.9433\n",
      "Epoch [358/800], Step [26/53], D_loss: 0.0071, G_loss: 6.4837\n",
      "Epoch [358/800], Step [28/53], D_loss: 0.0235, G_loss: 9.9297\n",
      "Epoch [358/800], Step [30/53], D_loss: 0.0283, G_loss: 1.1509\n",
      "Epoch [358/800], Step [32/53], D_loss: 0.0318, G_loss: 5.9030\n",
      "Epoch [358/800], Step [34/53], D_loss: 0.0069, G_loss: 7.3208\n",
      "Epoch [358/800], Step [36/53], D_loss: 0.0206, G_loss: 7.0042\n",
      "Epoch [358/800], Step [38/53], D_loss: 0.2306, G_loss: 3.0352\n",
      "Epoch [358/800], Step [40/53], D_loss: 0.0641, G_loss: 5.7476\n",
      "Epoch [358/800], Step [42/53], D_loss: 0.1710, G_loss: 7.8057\n",
      "Epoch [358/800], Step [44/53], D_loss: 0.0456, G_loss: 5.4967\n",
      "Epoch [358/800], Step [46/53], D_loss: 0.0465, G_loss: 5.5611\n",
      "Epoch [358/800], Step [48/53], D_loss: 0.0342, G_loss: 4.1541\n",
      "Epoch [358/800], Step [50/53], D_loss: 0.0979, G_loss: 3.5213\n",
      "Epoch [358/800], Step [52/53], D_loss: 0.0143, G_loss: 6.8203\n",
      "Epoch [358/800], Avg D_loss: 0.0820, Avg G_loss: 5.7779\n",
      "Epoch [359/800], Step [2/53], D_loss: 0.1182, G_loss: 4.9695\n",
      "Epoch [359/800], Step [4/53], D_loss: 0.0003, G_loss: 6.3714\n",
      "Epoch [359/800], Step [6/53], D_loss: 0.0373, G_loss: 5.4741\n",
      "Epoch [359/800], Step [8/53], D_loss: 0.0220, G_loss: 7.4528\n",
      "Epoch [359/800], Step [10/53], D_loss: 0.0249, G_loss: 7.8157\n",
      "Epoch [359/800], Step [12/53], D_loss: 0.0426, G_loss: 7.6488\n",
      "Epoch [359/800], Step [14/53], D_loss: 0.0837, G_loss: 6.1739\n",
      "Epoch [359/800], Step [16/53], D_loss: 0.0546, G_loss: 8.9339\n",
      "Epoch [359/800], Step [18/53], D_loss: 1.3619, G_loss: 0.1564\n",
      "Epoch [359/800], Step [20/53], D_loss: 1.0108, G_loss: 5.6499\n",
      "Epoch [359/800], Step [22/53], D_loss: 0.1015, G_loss: 3.0232\n",
      "Epoch [359/800], Step [24/53], D_loss: 0.1822, G_loss: 2.7663\n",
      "Epoch [359/800], Step [26/53], D_loss: 0.3026, G_loss: 3.6048\n",
      "Epoch [359/800], Step [28/53], D_loss: 0.0823, G_loss: 2.1276\n",
      "Epoch [359/800], Step [30/53], D_loss: 0.2412, G_loss: 3.4283\n",
      "Epoch [359/800], Step [32/53], D_loss: 0.0746, G_loss: 3.4158\n",
      "Epoch [359/800], Step [34/53], D_loss: 0.2102, G_loss: 2.2394\n",
      "Epoch [359/800], Step [36/53], D_loss: 0.4512, G_loss: 6.2976\n",
      "Epoch [359/800], Step [38/53], D_loss: 0.3384, G_loss: 5.2967\n",
      "Epoch [359/800], Step [40/53], D_loss: 0.0157, G_loss: 2.6213\n",
      "Epoch [359/800], Step [42/53], D_loss: 0.0260, G_loss: 4.2538\n",
      "Epoch [359/800], Step [44/53], D_loss: 0.0144, G_loss: 3.1866\n",
      "Epoch [359/800], Step [46/53], D_loss: 0.1005, G_loss: 7.2595\n",
      "Epoch [359/800], Step [48/53], D_loss: 0.0578, G_loss: 1.9642\n",
      "Epoch [359/800], Step [50/53], D_loss: 0.2947, G_loss: 6.7134\n",
      "Epoch [359/800], Step [52/53], D_loss: 0.1157, G_loss: 6.9241\n",
      "Epoch [359/800], Avg D_loss: 0.2348, Avg G_loss: 4.9680\n",
      "Epoch [360/800], Step [2/53], D_loss: 0.1529, G_loss: 3.6388\n",
      "Epoch [360/800], Step [4/53], D_loss: 0.1477, G_loss: 5.8748\n",
      "Epoch [360/800], Step [6/53], D_loss: 0.0936, G_loss: 4.8848\n",
      "Epoch [360/800], Step [8/53], D_loss: 0.0793, G_loss: 5.6482\n",
      "Epoch [360/800], Step [10/53], D_loss: 0.1309, G_loss: 4.6767\n",
      "Epoch [360/800], Step [12/53], D_loss: 0.0171, G_loss: 3.9043\n",
      "Epoch [360/800], Step [14/53], D_loss: 0.0456, G_loss: 6.4310\n",
      "Epoch [360/800], Step [16/53], D_loss: 0.0070, G_loss: 5.3805\n",
      "Epoch [360/800], Step [18/53], D_loss: 0.1241, G_loss: 3.0523\n",
      "Epoch [360/800], Step [20/53], D_loss: 0.0254, G_loss: 6.9388\n",
      "Epoch [360/800], Step [22/53], D_loss: 0.0609, G_loss: 3.6331\n",
      "Epoch [360/800], Step [24/53], D_loss: 0.1935, G_loss: 5.5421\n",
      "Epoch [360/800], Step [26/53], D_loss: 0.2527, G_loss: 6.3315\n",
      "Epoch [360/800], Step [28/53], D_loss: 0.2462, G_loss: 8.8539\n",
      "Epoch [360/800], Step [30/53], D_loss: 0.0191, G_loss: 5.9702\n",
      "Epoch [360/800], Step [32/53], D_loss: 0.0814, G_loss: 4.6026\n",
      "Epoch [360/800], Step [34/53], D_loss: 0.2165, G_loss: 7.1683\n",
      "Epoch [360/800], Step [36/53], D_loss: 0.0832, G_loss: 6.2737\n",
      "Epoch [360/800], Step [38/53], D_loss: 0.0383, G_loss: 6.6591\n",
      "Epoch [360/800], Step [40/53], D_loss: 0.0681, G_loss: 6.4910\n",
      "Epoch [360/800], Step [42/53], D_loss: 0.0701, G_loss: 4.3560\n",
      "Epoch [360/800], Step [44/53], D_loss: 0.1381, G_loss: 7.4686\n",
      "Epoch [360/800], Step [46/53], D_loss: 0.0235, G_loss: 6.9420\n",
      "Epoch [360/800], Step [48/53], D_loss: 0.0247, G_loss: 7.7997\n",
      "Epoch [360/800], Step [50/53], D_loss: 0.0273, G_loss: 8.8459\n",
      "Epoch [360/800], Step [52/53], D_loss: 0.0036, G_loss: 8.8418\n",
      "Epoch [360/800], Avg D_loss: 0.0790, Avg G_loss: 5.9348\n",
      "Generated images saved as generated_images/epoch_360.png\n",
      "Epoch [361/800], Step [2/53], D_loss: 0.0032, G_loss: 8.1111\n",
      "Epoch [361/800], Step [4/53], D_loss: 0.0176, G_loss: 6.8165\n",
      "Epoch [361/800], Step [6/53], D_loss: 0.1652, G_loss: 5.9939\n",
      "Epoch [361/800], Step [8/53], D_loss: 0.0626, G_loss: 7.3868\n",
      "Epoch [361/800], Step [10/53], D_loss: 0.0141, G_loss: 8.0118\n",
      "Epoch [361/800], Step [12/53], D_loss: 0.3224, G_loss: 7.1251\n",
      "Epoch [361/800], Step [14/53], D_loss: 0.0597, G_loss: 6.0977\n",
      "Epoch [361/800], Step [16/53], D_loss: 0.0117, G_loss: 6.1495\n",
      "Epoch [361/800], Step [18/53], D_loss: 0.0088, G_loss: 8.5887\n",
      "Epoch [361/800], Step [20/53], D_loss: 0.1970, G_loss: 8.0782\n",
      "Epoch [361/800], Step [22/53], D_loss: 0.0215, G_loss: 7.9891\n",
      "Epoch [361/800], Step [24/53], D_loss: 0.0164, G_loss: 8.3676\n",
      "Epoch [361/800], Step [26/53], D_loss: 0.0011, G_loss: 8.8771\n",
      "Epoch [361/800], Step [28/53], D_loss: 0.0896, G_loss: 7.6764\n",
      "Epoch [361/800], Step [30/53], D_loss: 0.0554, G_loss: 6.6230\n",
      "Epoch [361/800], Step [32/53], D_loss: 0.0026, G_loss: 7.7235\n",
      "Epoch [361/800], Step [34/53], D_loss: 0.0544, G_loss: 5.5477\n",
      "Epoch [361/800], Step [36/53], D_loss: 0.0734, G_loss: 5.1186\n",
      "Epoch [361/800], Step [38/53], D_loss: 0.6652, G_loss: 7.2596\n",
      "Epoch [361/800], Step [40/53], D_loss: 0.0093, G_loss: 4.3928\n",
      "Epoch [361/800], Step [42/53], D_loss: 0.0240, G_loss: 6.6271\n",
      "Epoch [361/800], Step [44/53], D_loss: 0.0500, G_loss: 9.1920\n",
      "Epoch [361/800], Step [46/53], D_loss: 0.0128, G_loss: 9.9668\n",
      "Epoch [361/800], Step [48/53], D_loss: 0.0315, G_loss: 10.2939\n",
      "Epoch [361/800], Step [50/53], D_loss: 0.0094, G_loss: 6.0046\n",
      "Epoch [361/800], Step [52/53], D_loss: 0.0042, G_loss: 9.0390\n",
      "Epoch [361/800], Avg D_loss: 0.0690, Avg G_loss: 7.1134\n",
      "Epoch [362/800], Step [2/53], D_loss: 0.0240, G_loss: 5.4665\n",
      "Epoch [362/800], Step [4/53], D_loss: 0.0116, G_loss: 5.4359\n",
      "Epoch [362/800], Step [6/53], D_loss: 0.0524, G_loss: 6.9533\n",
      "Epoch [362/800], Step [8/53], D_loss: 0.0530, G_loss: 4.0020\n",
      "Epoch [362/800], Step [10/53], D_loss: 0.0046, G_loss: 6.6790\n",
      "Epoch [362/800], Step [12/53], D_loss: 0.0451, G_loss: 6.8195\n",
      "Epoch [362/800], Step [14/53], D_loss: 0.0384, G_loss: 5.5171\n",
      "Epoch [362/800], Step [16/53], D_loss: 0.1409, G_loss: 6.2643\n",
      "Epoch [362/800], Step [18/53], D_loss: 0.0152, G_loss: 6.7150\n",
      "Epoch [362/800], Step [20/53], D_loss: 0.0834, G_loss: 6.8422\n",
      "Epoch [362/800], Step [22/53], D_loss: 0.0399, G_loss: 6.1998\n",
      "Epoch [362/800], Step [24/53], D_loss: 0.1476, G_loss: 10.2022\n",
      "Epoch [362/800], Step [26/53], D_loss: 0.0504, G_loss: 10.4771\n",
      "Epoch [362/800], Step [28/53], D_loss: 0.0108, G_loss: 6.8202\n",
      "Epoch [362/800], Step [30/53], D_loss: 0.0041, G_loss: 11.1514\n",
      "Epoch [362/800], Step [32/53], D_loss: 0.0196, G_loss: 9.7018\n",
      "Epoch [362/800], Step [34/53], D_loss: 0.0158, G_loss: 8.2747\n",
      "Epoch [362/800], Step [36/53], D_loss: 0.0034, G_loss: 8.9030\n",
      "Epoch [362/800], Step [38/53], D_loss: 0.0006, G_loss: 8.7535\n",
      "Epoch [362/800], Step [40/53], D_loss: 0.1412, G_loss: 8.7109\n",
      "Epoch [362/800], Step [42/53], D_loss: 0.0342, G_loss: 11.6038\n",
      "Epoch [362/800], Step [44/53], D_loss: 0.0083, G_loss: 7.3632\n",
      "Epoch [362/800], Step [46/53], D_loss: 0.1536, G_loss: 8.2336\n",
      "Epoch [362/800], Step [48/53], D_loss: 0.0419, G_loss: 10.7714\n",
      "Epoch [362/800], Step [50/53], D_loss: 0.1105, G_loss: 6.0447\n",
      "Epoch [362/800], Step [52/53], D_loss: 0.0015, G_loss: 10.8108\n",
      "Epoch [362/800], Avg D_loss: 0.0536, Avg G_loss: 7.5759\n",
      "Epoch [363/800], Step [2/53], D_loss: 0.1248, G_loss: 6.5025\n",
      "Epoch [363/800], Step [4/53], D_loss: 0.0166, G_loss: 7.6461\n",
      "Epoch [363/800], Step [6/53], D_loss: 0.0062, G_loss: 12.2378\n",
      "Epoch [363/800], Step [8/53], D_loss: 0.2867, G_loss: 10.2983\n",
      "Epoch [363/800], Step [10/53], D_loss: 0.1688, G_loss: 12.8549\n",
      "Epoch [363/800], Step [12/53], D_loss: 0.0971, G_loss: 3.3357\n",
      "Epoch [363/800], Step [14/53], D_loss: 0.0047, G_loss: 4.2966\n",
      "Epoch [363/800], Step [16/53], D_loss: 0.0050, G_loss: 9.8500\n",
      "Epoch [363/800], Step [18/53], D_loss: 0.0214, G_loss: 7.7903\n",
      "Epoch [363/800], Step [20/53], D_loss: 0.0241, G_loss: 8.2124\n",
      "Epoch [363/800], Step [22/53], D_loss: 0.0022, G_loss: 8.7992\n",
      "Epoch [363/800], Step [24/53], D_loss: 0.0034, G_loss: 9.8290\n",
      "Epoch [363/800], Step [26/53], D_loss: 0.3315, G_loss: 12.0771\n",
      "Epoch [363/800], Step [28/53], D_loss: 0.8227, G_loss: 5.5987\n",
      "Epoch [363/800], Step [30/53], D_loss: 0.5083, G_loss: 8.1526\n",
      "Epoch [363/800], Step [32/53], D_loss: 0.0091, G_loss: 8.7661\n",
      "Epoch [363/800], Step [34/53], D_loss: 0.0248, G_loss: 6.4667\n",
      "Epoch [363/800], Step [36/53], D_loss: 0.0770, G_loss: 6.5866\n",
      "Epoch [363/800], Step [38/53], D_loss: 0.0679, G_loss: 7.3716\n",
      "Epoch [363/800], Step [40/53], D_loss: 0.0701, G_loss: 4.0221\n",
      "Epoch [363/800], Step [42/53], D_loss: 0.0738, G_loss: 6.5157\n",
      "Epoch [363/800], Step [44/53], D_loss: 0.0384, G_loss: 6.9556\n",
      "Epoch [363/800], Step [46/53], D_loss: 0.0407, G_loss: 7.6511\n",
      "Epoch [363/800], Step [48/53], D_loss: 0.0751, G_loss: 3.4090\n",
      "Epoch [363/800], Step [50/53], D_loss: 0.0187, G_loss: 7.4502\n",
      "Epoch [363/800], Step [52/53], D_loss: 0.0342, G_loss: 5.8779\n",
      "Epoch [363/800], Avg D_loss: 0.0801, Avg G_loss: 7.6650\n",
      "Epoch [364/800], Step [2/53], D_loss: 0.0342, G_loss: 7.6754\n",
      "Epoch [364/800], Step [4/53], D_loss: 0.0103, G_loss: 8.8248\n",
      "Epoch [364/800], Step [6/53], D_loss: 0.0090, G_loss: 7.6430\n",
      "Epoch [364/800], Step [8/53], D_loss: 0.0330, G_loss: 5.5972\n",
      "Epoch [364/800], Step [10/53], D_loss: 0.0536, G_loss: 8.0504\n",
      "Epoch [364/800], Step [12/53], D_loss: 0.0176, G_loss: 6.7088\n",
      "Epoch [364/800], Step [14/53], D_loss: 0.0495, G_loss: 8.7170\n",
      "Epoch [364/800], Step [16/53], D_loss: 0.0312, G_loss: 10.4386\n",
      "Epoch [364/800], Step [18/53], D_loss: 0.0591, G_loss: 10.4735\n",
      "Epoch [364/800], Step [20/53], D_loss: 0.0408, G_loss: 5.2714\n",
      "Epoch [364/800], Step [22/53], D_loss: 0.0020, G_loss: 4.5877\n",
      "Epoch [364/800], Step [24/53], D_loss: 0.0155, G_loss: 9.1917\n",
      "Epoch [364/800], Step [26/53], D_loss: 0.0375, G_loss: 10.9437\n",
      "Epoch [364/800], Step [28/53], D_loss: 0.0196, G_loss: 9.1455\n",
      "Epoch [364/800], Step [30/53], D_loss: 0.0160, G_loss: 6.8949\n",
      "Epoch [364/800], Step [32/53], D_loss: 0.0561, G_loss: 6.7779\n",
      "Epoch [364/800], Step [34/53], D_loss: 0.0070, G_loss: 8.3330\n",
      "Epoch [364/800], Step [36/53], D_loss: 0.0010, G_loss: 5.6522\n",
      "Epoch [364/800], Step [38/53], D_loss: 0.0070, G_loss: 9.4668\n",
      "Epoch [364/800], Step [40/53], D_loss: 0.0143, G_loss: 8.8179\n",
      "Epoch [364/800], Step [42/53], D_loss: 0.0089, G_loss: 5.5949\n",
      "Epoch [364/800], Step [44/53], D_loss: 0.0003, G_loss: 9.9313\n",
      "Epoch [364/800], Step [46/53], D_loss: 0.0177, G_loss: 8.5128\n",
      "Epoch [364/800], Step [48/53], D_loss: 0.0073, G_loss: 7.6454\n",
      "Epoch [364/800], Step [50/53], D_loss: 0.0039, G_loss: 7.2297\n",
      "Epoch [364/800], Step [52/53], D_loss: 0.0044, G_loss: 7.6227\n",
      "Epoch [364/800], Avg D_loss: 0.0223, Avg G_loss: 7.7832\n",
      "Epoch [365/800], Step [2/53], D_loss: 0.0156, G_loss: 8.9114\n",
      "Epoch [365/800], Step [4/53], D_loss: 0.1439, G_loss: 10.5847\n",
      "Epoch [365/800], Step [6/53], D_loss: 0.3364, G_loss: 4.9403\n",
      "Epoch [365/800], Step [8/53], D_loss: 0.3565, G_loss: 11.1848\n",
      "Epoch [365/800], Step [10/53], D_loss: 0.0860, G_loss: 8.8315\n",
      "Epoch [365/800], Step [12/53], D_loss: 0.0067, G_loss: 3.5423\n",
      "Epoch [365/800], Step [14/53], D_loss: 0.0209, G_loss: 5.0395\n",
      "Epoch [365/800], Step [16/53], D_loss: 0.0620, G_loss: 6.0841\n",
      "Epoch [365/800], Step [18/53], D_loss: 0.0180, G_loss: 7.3067\n",
      "Epoch [365/800], Step [20/53], D_loss: 0.0093, G_loss: 6.5763\n",
      "Epoch [365/800], Step [22/53], D_loss: 0.0079, G_loss: 7.5194\n",
      "Epoch [365/800], Step [24/53], D_loss: 0.0445, G_loss: 9.3300\n",
      "Epoch [365/800], Step [26/53], D_loss: 0.1795, G_loss: 6.8061\n",
      "Epoch [365/800], Step [28/53], D_loss: 0.0229, G_loss: 10.7698\n",
      "Epoch [365/800], Step [30/53], D_loss: 0.3818, G_loss: 13.6526\n",
      "Epoch [365/800], Step [32/53], D_loss: 0.1672, G_loss: 9.2577\n",
      "Epoch [365/800], Step [34/53], D_loss: 0.0067, G_loss: 2.8782\n",
      "Epoch [365/800], Step [36/53], D_loss: 0.0562, G_loss: 5.2130\n",
      "Epoch [365/800], Step [38/53], D_loss: 0.0035, G_loss: 6.9438\n",
      "Epoch [365/800], Step [40/53], D_loss: 0.0325, G_loss: 11.7238\n",
      "Epoch [365/800], Step [42/53], D_loss: 0.0127, G_loss: 8.0490\n",
      "Epoch [365/800], Step [44/53], D_loss: 0.0003, G_loss: 8.9745\n",
      "Epoch [365/800], Step [46/53], D_loss: 0.0004, G_loss: 9.4253\n",
      "Epoch [365/800], Step [48/53], D_loss: 0.0037, G_loss: 7.1461\n",
      "Epoch [365/800], Step [50/53], D_loss: 0.0025, G_loss: 5.6542\n",
      "Epoch [365/800], Step [52/53], D_loss: 0.1676, G_loss: 12.2074\n",
      "Epoch [365/800], Avg D_loss: 0.0778, Avg G_loss: 7.8755\n",
      "Epoch [366/800], Step [2/53], D_loss: 0.0919, G_loss: 9.0893\n",
      "Epoch [366/800], Step [4/53], D_loss: 2.2983, G_loss: 9.7719\n",
      "Epoch [366/800], Step [6/53], D_loss: 0.2208, G_loss: 5.3250\n",
      "Epoch [366/800], Step [8/53], D_loss: 0.0835, G_loss: 3.8974\n",
      "Epoch [366/800], Step [10/53], D_loss: 0.1133, G_loss: 4.5389\n",
      "Epoch [366/800], Step [12/53], D_loss: 0.0482, G_loss: 3.2277\n",
      "Epoch [366/800], Step [14/53], D_loss: 0.1929, G_loss: 3.8476\n",
      "Epoch [366/800], Step [16/53], D_loss: 0.0837, G_loss: 5.4495\n",
      "Epoch [366/800], Step [18/53], D_loss: 0.0820, G_loss: 4.5771\n",
      "Epoch [366/800], Step [20/53], D_loss: 0.0451, G_loss: 4.8839\n",
      "Epoch [366/800], Step [22/53], D_loss: 0.0430, G_loss: 2.8503\n",
      "Epoch [366/800], Step [24/53], D_loss: 0.2592, G_loss: 7.1169\n",
      "Epoch [366/800], Step [26/53], D_loss: 0.0383, G_loss: 5.5337\n",
      "Epoch [366/800], Step [28/53], D_loss: 0.0642, G_loss: 3.6939\n",
      "Epoch [366/800], Step [30/53], D_loss: 0.2768, G_loss: 5.7294\n",
      "Epoch [366/800], Step [32/53], D_loss: 0.1655, G_loss: 5.1258\n",
      "Epoch [366/800], Step [34/53], D_loss: 0.0129, G_loss: 4.8400\n",
      "Epoch [366/800], Step [36/53], D_loss: 0.0284, G_loss: 3.5592\n",
      "Epoch [366/800], Step [38/53], D_loss: 0.0921, G_loss: 7.0672\n",
      "Epoch [366/800], Step [40/53], D_loss: 0.0592, G_loss: 7.9157\n",
      "Epoch [366/800], Step [42/53], D_loss: 0.0248, G_loss: 5.5292\n",
      "Epoch [366/800], Step [44/53], D_loss: 0.2373, G_loss: 1.7492\n",
      "Epoch [366/800], Step [46/53], D_loss: 0.0455, G_loss: 8.5045\n",
      "Epoch [366/800], Step [48/53], D_loss: 0.2222, G_loss: 2.6313\n",
      "Epoch [366/800], Step [50/53], D_loss: 0.0373, G_loss: 4.7882\n",
      "Epoch [366/800], Step [52/53], D_loss: 0.0168, G_loss: 5.6847\n",
      "Epoch [366/800], Avg D_loss: 0.1982, Avg G_loss: 4.9419\n",
      "Epoch [367/800], Step [2/53], D_loss: 0.1355, G_loss: 7.1182\n",
      "Epoch [367/800], Step [4/53], D_loss: 0.0171, G_loss: 7.4448\n",
      "Epoch [367/800], Step [6/53], D_loss: 0.0605, G_loss: 5.9600\n",
      "Epoch [367/800], Step [8/53], D_loss: 0.0299, G_loss: 5.1007\n",
      "Epoch [367/800], Step [10/53], D_loss: 0.0741, G_loss: 3.8006\n",
      "Epoch [367/800], Step [12/53], D_loss: 0.0796, G_loss: 5.0114\n",
      "Epoch [367/800], Step [14/53], D_loss: 0.0705, G_loss: 6.5727\n",
      "Epoch [367/800], Step [16/53], D_loss: 0.0983, G_loss: 4.8988\n",
      "Epoch [367/800], Step [18/53], D_loss: 0.0504, G_loss: 5.0254\n",
      "Epoch [367/800], Step [20/53], D_loss: 0.0934, G_loss: 3.8376\n",
      "Epoch [367/800], Step [22/53], D_loss: 0.0067, G_loss: 8.3133\n",
      "Epoch [367/800], Step [24/53], D_loss: 0.0876, G_loss: 6.7682\n",
      "Epoch [367/800], Step [26/53], D_loss: 0.0202, G_loss: 4.1488\n",
      "Epoch [367/800], Step [28/53], D_loss: 0.0070, G_loss: 4.5573\n",
      "Epoch [367/800], Step [30/53], D_loss: 0.0069, G_loss: 6.6006\n",
      "Epoch [367/800], Step [32/53], D_loss: 0.1379, G_loss: 8.6769\n",
      "Epoch [367/800], Step [34/53], D_loss: 0.0373, G_loss: 6.8047\n",
      "Epoch [367/800], Step [36/53], D_loss: 0.0898, G_loss: 5.1648\n",
      "Epoch [367/800], Step [38/53], D_loss: 0.0548, G_loss: 3.5415\n",
      "Epoch [367/800], Step [40/53], D_loss: 0.0467, G_loss: 9.9460\n",
      "Epoch [367/800], Step [42/53], D_loss: 0.0211, G_loss: 7.0631\n",
      "Epoch [367/800], Step [44/53], D_loss: 0.0802, G_loss: 4.7158\n",
      "Epoch [367/800], Step [46/53], D_loss: 0.2315, G_loss: 11.4071\n",
      "Epoch [367/800], Step [48/53], D_loss: 0.0009, G_loss: 7.9491\n",
      "Epoch [367/800], Step [50/53], D_loss: 0.0373, G_loss: 6.0717\n",
      "Epoch [367/800], Step [52/53], D_loss: 0.0115, G_loss: 5.5264\n",
      "Epoch [367/800], Avg D_loss: 0.0627, Avg G_loss: 6.1578\n",
      "Epoch [368/800], Step [2/53], D_loss: 0.0248, G_loss: 6.4659\n",
      "Epoch [368/800], Step [4/53], D_loss: 0.0406, G_loss: 6.5570\n",
      "Epoch [368/800], Step [6/53], D_loss: 0.0118, G_loss: 5.2154\n",
      "Epoch [368/800], Step [8/53], D_loss: 0.0039, G_loss: 10.1041\n",
      "Epoch [368/800], Step [10/53], D_loss: 0.1832, G_loss: 7.6391\n",
      "Epoch [368/800], Step [12/53], D_loss: 0.0628, G_loss: 3.3856\n",
      "Epoch [368/800], Step [14/53], D_loss: 0.0076, G_loss: 6.6211\n",
      "Epoch [368/800], Step [16/53], D_loss: 0.0439, G_loss: 6.0879\n",
      "Epoch [368/800], Step [18/53], D_loss: 0.0094, G_loss: 7.6751\n",
      "Epoch [368/800], Step [20/53], D_loss: 0.1378, G_loss: 11.6449\n",
      "Epoch [368/800], Step [22/53], D_loss: 0.0257, G_loss: 11.5116\n",
      "Epoch [368/800], Step [24/53], D_loss: 0.0809, G_loss: 4.0016\n",
      "Epoch [368/800], Step [26/53], D_loss: 0.3466, G_loss: 8.4757\n",
      "Epoch [368/800], Step [28/53], D_loss: 0.0142, G_loss: 8.5157\n",
      "Epoch [368/800], Step [30/53], D_loss: 0.0980, G_loss: 8.5816\n",
      "Epoch [368/800], Step [32/53], D_loss: 0.0039, G_loss: 4.9104\n",
      "Epoch [368/800], Step [34/53], D_loss: 0.0016, G_loss: 5.9170\n",
      "Epoch [368/800], Step [36/53], D_loss: 0.0534, G_loss: 6.6244\n",
      "Epoch [368/800], Step [38/53], D_loss: 0.0459, G_loss: 9.7174\n",
      "Epoch [368/800], Step [40/53], D_loss: 0.1997, G_loss: 8.6387\n",
      "Epoch [368/800], Step [42/53], D_loss: 0.0094, G_loss: 4.7820\n",
      "Epoch [368/800], Step [44/53], D_loss: 0.0266, G_loss: 4.8946\n",
      "Epoch [368/800], Step [46/53], D_loss: 0.4384, G_loss: 8.7070\n",
      "Epoch [368/800], Step [48/53], D_loss: 0.1652, G_loss: 9.4152\n",
      "Epoch [368/800], Step [50/53], D_loss: 0.0011, G_loss: 6.0153\n",
      "Epoch [368/800], Step [52/53], D_loss: 0.1552, G_loss: 11.5727\n",
      "Epoch [368/800], Avg D_loss: 0.0947, Avg G_loss: 7.3956\n",
      "Epoch [369/800], Step [2/53], D_loss: 0.1594, G_loss: 6.0780\n",
      "Epoch [369/800], Step [4/53], D_loss: 0.0489, G_loss: 7.0807\n",
      "Epoch [369/800], Step [6/53], D_loss: 0.1325, G_loss: 5.1835\n",
      "Epoch [369/800], Step [8/53], D_loss: 0.1051, G_loss: 5.5288\n",
      "Epoch [369/800], Step [10/53], D_loss: 0.0401, G_loss: 5.8296\n",
      "Epoch [369/800], Step [12/53], D_loss: 0.0914, G_loss: 4.0960\n",
      "Epoch [369/800], Step [14/53], D_loss: 0.0186, G_loss: 7.1462\n",
      "Epoch [369/800], Step [16/53], D_loss: 0.0186, G_loss: 4.2953\n",
      "Epoch [369/800], Step [18/53], D_loss: 0.0201, G_loss: 2.6310\n",
      "Epoch [369/800], Step [20/53], D_loss: 0.0055, G_loss: 7.5113\n",
      "Epoch [369/800], Step [22/53], D_loss: 0.0476, G_loss: 6.0870\n",
      "Epoch [369/800], Step [24/53], D_loss: 0.0111, G_loss: 6.7836\n",
      "Epoch [369/800], Step [26/53], D_loss: 0.0178, G_loss: 3.3802\n",
      "Epoch [369/800], Step [28/53], D_loss: 0.0074, G_loss: 9.1554\n",
      "Epoch [369/800], Step [30/53], D_loss: 0.0037, G_loss: 6.5939\n",
      "Epoch [369/800], Step [32/53], D_loss: 0.1160, G_loss: 8.4106\n",
      "Epoch [369/800], Step [34/53], D_loss: 0.0185, G_loss: 5.4187\n",
      "Epoch [369/800], Step [36/53], D_loss: 0.0182, G_loss: 5.2218\n",
      "Epoch [369/800], Step [38/53], D_loss: 0.0456, G_loss: 8.7827\n",
      "Epoch [369/800], Step [40/53], D_loss: 0.0107, G_loss: 9.3316\n",
      "Epoch [369/800], Step [42/53], D_loss: 0.0716, G_loss: 2.4048\n",
      "Epoch [369/800], Step [44/53], D_loss: 0.0157, G_loss: 15.0176\n",
      "Epoch [369/800], Step [46/53], D_loss: 0.0019, G_loss: 7.4781\n",
      "Epoch [369/800], Step [48/53], D_loss: 0.0213, G_loss: 7.2434\n",
      "Epoch [369/800], Step [50/53], D_loss: 0.0242, G_loss: 2.9552\n",
      "Epoch [369/800], Step [52/53], D_loss: 0.4157, G_loss: 12.1202\n",
      "Epoch [369/800], Avg D_loss: 0.0932, Avg G_loss: 6.5978\n",
      "Epoch [370/800], Step [2/53], D_loss: 0.2140, G_loss: 5.8061\n",
      "Epoch [370/800], Step [4/53], D_loss: 0.0137, G_loss: 3.9171\n",
      "Epoch [370/800], Step [6/53], D_loss: 0.0352, G_loss: 5.3451\n",
      "Epoch [370/800], Step [8/53], D_loss: 0.2557, G_loss: 5.1876\n",
      "Epoch [370/800], Step [10/53], D_loss: 0.3411, G_loss: 7.1308\n",
      "Epoch [370/800], Step [12/53], D_loss: 0.0296, G_loss: 8.0509\n",
      "Epoch [370/800], Step [14/53], D_loss: 0.1674, G_loss: 5.1126\n",
      "Epoch [370/800], Step [16/53], D_loss: 0.0157, G_loss: 3.7106\n",
      "Epoch [370/800], Step [18/53], D_loss: 0.0326, G_loss: 8.6191\n",
      "Epoch [370/800], Step [20/53], D_loss: 0.0103, G_loss: 2.5800\n",
      "Epoch [370/800], Step [22/53], D_loss: 0.0127, G_loss: 8.9142\n",
      "Epoch [370/800], Step [24/53], D_loss: 0.4384, G_loss: 2.4675\n",
      "Epoch [370/800], Step [26/53], D_loss: 0.3129, G_loss: 8.1482\n",
      "Epoch [370/800], Step [28/53], D_loss: 0.0191, G_loss: 5.4332\n",
      "Epoch [370/800], Step [30/53], D_loss: 0.0174, G_loss: 6.2616\n",
      "Epoch [370/800], Step [32/53], D_loss: 0.0281, G_loss: 5.8371\n",
      "Epoch [370/800], Step [34/53], D_loss: 0.0377, G_loss: 6.8342\n",
      "Epoch [370/800], Step [36/53], D_loss: 0.0097, G_loss: 8.6912\n",
      "Epoch [370/800], Step [38/53], D_loss: 0.0253, G_loss: 7.0534\n",
      "Epoch [370/800], Step [40/53], D_loss: 0.0258, G_loss: 4.8146\n",
      "Epoch [370/800], Step [42/53], D_loss: 0.0059, G_loss: 6.8891\n",
      "Epoch [370/800], Step [44/53], D_loss: 0.0117, G_loss: 6.3973\n",
      "Epoch [370/800], Step [46/53], D_loss: 0.0072, G_loss: 6.5146\n",
      "Epoch [370/800], Step [48/53], D_loss: 0.0131, G_loss: 7.9087\n",
      "Epoch [370/800], Step [50/53], D_loss: 0.0616, G_loss: 4.3405\n",
      "Epoch [370/800], Step [52/53], D_loss: 0.0794, G_loss: 9.5742\n",
      "Epoch [370/800], Avg D_loss: 0.1056, Avg G_loss: 5.8764\n",
      "Epoch [371/800], Step [2/53], D_loss: 0.0767, G_loss: 8.5190\n",
      "Epoch [371/800], Step [4/53], D_loss: 0.0011, G_loss: 6.5600\n",
      "Epoch [371/800], Step [6/53], D_loss: 0.0269, G_loss: 6.7111\n",
      "Epoch [371/800], Step [8/53], D_loss: 0.0123, G_loss: 6.4365\n",
      "Epoch [371/800], Step [10/53], D_loss: 0.0179, G_loss: 6.7906\n",
      "Epoch [371/800], Step [12/53], D_loss: 0.0188, G_loss: 2.6491\n",
      "Epoch [371/800], Step [14/53], D_loss: 0.0377, G_loss: 6.2436\n",
      "Epoch [371/800], Step [16/53], D_loss: 0.0275, G_loss: 5.4411\n",
      "Epoch [371/800], Step [18/53], D_loss: 0.1533, G_loss: 6.1207\n",
      "Epoch [371/800], Step [20/53], D_loss: 0.0391, G_loss: 6.3122\n",
      "Epoch [371/800], Step [22/53], D_loss: 0.1579, G_loss: 8.0053\n",
      "Epoch [371/800], Step [24/53], D_loss: 0.1941, G_loss: 5.0783\n",
      "Epoch [371/800], Step [26/53], D_loss: 0.0233, G_loss: 6.1227\n",
      "Epoch [371/800], Step [28/53], D_loss: 0.0100, G_loss: 8.4116\n",
      "Epoch [371/800], Step [30/53], D_loss: 0.1334, G_loss: 5.1334\n",
      "Epoch [371/800], Step [32/53], D_loss: 0.0743, G_loss: 6.7154\n",
      "Epoch [371/800], Step [34/53], D_loss: 0.0175, G_loss: 10.7455\n",
      "Epoch [371/800], Step [36/53], D_loss: 0.0221, G_loss: 9.9787\n",
      "Epoch [371/800], Step [38/53], D_loss: 0.1812, G_loss: 9.1001\n",
      "Epoch [371/800], Step [40/53], D_loss: 0.0247, G_loss: 10.4617\n",
      "Epoch [371/800], Step [42/53], D_loss: 0.2355, G_loss: 6.2235\n",
      "Epoch [371/800], Step [44/53], D_loss: 0.0326, G_loss: 6.4583\n",
      "Epoch [371/800], Step [46/53], D_loss: 0.0451, G_loss: 7.6487\n",
      "Epoch [371/800], Step [48/53], D_loss: 0.0046, G_loss: 4.6942\n",
      "Epoch [371/800], Step [50/53], D_loss: 0.0002, G_loss: 9.3988\n",
      "Epoch [371/800], Step [52/53], D_loss: 0.0162, G_loss: 6.6657\n",
      "Epoch [371/800], Avg D_loss: 0.0535, Avg G_loss: 6.9405\n",
      "Epoch [372/800], Step [2/53], D_loss: 0.0268, G_loss: 7.9102\n",
      "Epoch [372/800], Step [4/53], D_loss: 0.0013, G_loss: 9.4519\n",
      "Epoch [372/800], Step [6/53], D_loss: 0.0076, G_loss: 8.2019\n",
      "Epoch [372/800], Step [8/53], D_loss: 0.0053, G_loss: 8.2824\n",
      "Epoch [372/800], Step [10/53], D_loss: 0.2073, G_loss: 10.7733\n",
      "Epoch [372/800], Step [12/53], D_loss: 0.3961, G_loss: 5.0260\n",
      "Epoch [372/800], Step [14/53], D_loss: 0.0178, G_loss: 6.2566\n",
      "Epoch [372/800], Step [16/53], D_loss: 0.2193, G_loss: 5.8461\n",
      "Epoch [372/800], Step [18/53], D_loss: 0.0446, G_loss: 5.4454\n",
      "Epoch [372/800], Step [20/53], D_loss: 0.0587, G_loss: 6.3642\n",
      "Epoch [372/800], Step [22/53], D_loss: 0.0417, G_loss: 6.1367\n",
      "Epoch [372/800], Step [24/53], D_loss: 0.0341, G_loss: 7.1286\n",
      "Epoch [372/800], Step [26/53], D_loss: 0.0736, G_loss: 3.4205\n",
      "Epoch [372/800], Step [28/53], D_loss: 0.0036, G_loss: 9.9900\n",
      "Epoch [372/800], Step [30/53], D_loss: 0.0021, G_loss: 8.5046\n",
      "Epoch [372/800], Step [32/53], D_loss: 0.8765, G_loss: 6.0029\n",
      "Epoch [372/800], Step [34/53], D_loss: 0.4115, G_loss: 7.0989\n",
      "Epoch [372/800], Step [36/53], D_loss: 0.0129, G_loss: 4.2036\n",
      "Epoch [372/800], Step [38/53], D_loss: 0.0197, G_loss: 5.9114\n",
      "Epoch [372/800], Step [40/53], D_loss: 0.0489, G_loss: 6.0044\n",
      "Epoch [372/800], Step [42/53], D_loss: 0.0834, G_loss: 5.1087\n",
      "Epoch [372/800], Step [44/53], D_loss: 0.0216, G_loss: 5.7370\n",
      "Epoch [372/800], Step [46/53], D_loss: 0.0152, G_loss: 5.0893\n",
      "Epoch [372/800], Step [48/53], D_loss: 0.0757, G_loss: 4.3943\n",
      "Epoch [372/800], Step [50/53], D_loss: 0.0429, G_loss: 5.9192\n",
      "Epoch [372/800], Step [52/53], D_loss: 0.0202, G_loss: 3.3847\n",
      "Epoch [372/800], Avg D_loss: 0.1274, Avg G_loss: 6.4652\n",
      "Epoch [373/800], Step [2/53], D_loss: 0.0586, G_loss: 8.6016\n",
      "Epoch [373/800], Step [4/53], D_loss: 0.0116, G_loss: 3.9849\n",
      "Epoch [373/800], Step [6/53], D_loss: 0.1425, G_loss: 8.6525\n",
      "Epoch [373/800], Step [8/53], D_loss: 0.0063, G_loss: 1.9224\n",
      "Epoch [373/800], Step [10/53], D_loss: 0.2119, G_loss: 6.1684\n",
      "Epoch [373/800], Step [12/53], D_loss: 0.0155, G_loss: 5.4128\n",
      "Epoch [373/800], Step [14/53], D_loss: 0.0914, G_loss: 8.1059\n",
      "Epoch [373/800], Step [16/53], D_loss: 0.0285, G_loss: 6.9253\n",
      "Epoch [373/800], Step [18/53], D_loss: 0.0084, G_loss: 5.5372\n",
      "Epoch [373/800], Step [20/53], D_loss: 0.0558, G_loss: 4.4837\n",
      "Epoch [373/800], Step [22/53], D_loss: 0.0062, G_loss: 3.2432\n",
      "Epoch [373/800], Step [24/53], D_loss: 0.0201, G_loss: 7.9208\n",
      "Epoch [373/800], Step [26/53], D_loss: 0.1613, G_loss: 5.9141\n",
      "Epoch [373/800], Step [28/53], D_loss: 0.7296, G_loss: 10.6968\n",
      "Epoch [373/800], Step [30/53], D_loss: 0.0964, G_loss: 3.5519\n",
      "Epoch [373/800], Step [32/53], D_loss: 0.1972, G_loss: 5.4647\n",
      "Epoch [373/800], Step [34/53], D_loss: 0.0549, G_loss: 5.3958\n",
      "Epoch [373/800], Step [36/53], D_loss: 0.0476, G_loss: 4.5618\n",
      "Epoch [373/800], Step [38/53], D_loss: 0.0663, G_loss: 6.5953\n",
      "Epoch [373/800], Step [40/53], D_loss: 0.0551, G_loss: 4.9743\n",
      "Epoch [373/800], Step [42/53], D_loss: 0.0885, G_loss: 8.8699\n",
      "Epoch [373/800], Step [44/53], D_loss: 0.0257, G_loss: 6.5684\n",
      "Epoch [373/800], Step [46/53], D_loss: 0.0506, G_loss: 2.9881\n",
      "Epoch [373/800], Step [48/53], D_loss: 0.0540, G_loss: 3.3264\n",
      "Epoch [373/800], Step [50/53], D_loss: 0.0299, G_loss: 5.8524\n",
      "Epoch [373/800], Step [52/53], D_loss: 0.0989, G_loss: 6.8980\n",
      "Epoch [373/800], Avg D_loss: 0.1048, Avg G_loss: 5.6881\n",
      "Epoch [374/800], Step [2/53], D_loss: 0.0022, G_loss: 7.2853\n",
      "Epoch [374/800], Step [4/53], D_loss: 0.0283, G_loss: 7.5252\n",
      "Epoch [374/800], Step [6/53], D_loss: 0.0228, G_loss: 8.7563\n",
      "Epoch [374/800], Step [8/53], D_loss: 0.0321, G_loss: 8.9081\n",
      "Epoch [374/800], Step [10/53], D_loss: 0.0783, G_loss: 5.7114\n",
      "Epoch [374/800], Step [12/53], D_loss: 0.1316, G_loss: 8.5390\n",
      "Epoch [374/800], Step [14/53], D_loss: 0.0406, G_loss: 4.2907\n",
      "Epoch [374/800], Step [16/53], D_loss: 0.1303, G_loss: 6.1431\n",
      "Epoch [374/800], Step [18/53], D_loss: 0.0886, G_loss: 8.5506\n",
      "Epoch [374/800], Step [20/53], D_loss: 0.0929, G_loss: 6.6007\n",
      "Epoch [374/800], Step [22/53], D_loss: 0.0081, G_loss: 4.0173\n",
      "Epoch [374/800], Step [24/53], D_loss: 0.0122, G_loss: 8.3203\n",
      "Epoch [374/800], Step [26/53], D_loss: 0.0006, G_loss: 10.7261\n",
      "Epoch [374/800], Step [28/53], D_loss: 0.0068, G_loss: 7.1583\n",
      "Epoch [374/800], Step [30/53], D_loss: 0.0492, G_loss: 4.8824\n",
      "Epoch [374/800], Step [32/53], D_loss: 0.0030, G_loss: 9.1315\n",
      "Epoch [374/800], Step [34/53], D_loss: 0.0009, G_loss: 7.1667\n",
      "Epoch [374/800], Step [36/53], D_loss: 0.0022, G_loss: 5.2641\n",
      "Epoch [374/800], Step [38/53], D_loss: 0.0093, G_loss: 7.2849\n",
      "Epoch [374/800], Step [40/53], D_loss: 0.0269, G_loss: 3.3396\n",
      "Epoch [374/800], Step [42/53], D_loss: 0.0206, G_loss: 9.1409\n",
      "Epoch [374/800], Step [44/53], D_loss: 0.0300, G_loss: 14.1123\n",
      "Epoch [374/800], Step [46/53], D_loss: 0.0149, G_loss: 6.9937\n",
      "Epoch [374/800], Step [48/53], D_loss: 0.2854, G_loss: 13.2357\n",
      "Epoch [374/800], Step [50/53], D_loss: 0.0150, G_loss: 13.0319\n",
      "Epoch [374/800], Step [52/53], D_loss: 0.3678, G_loss: 6.6838\n",
      "Epoch [374/800], Avg D_loss: 0.0575, Avg G_loss: 7.5422\n",
      "Epoch [375/800], Step [2/53], D_loss: 0.0455, G_loss: 8.2821\n",
      "Epoch [375/800], Step [4/53], D_loss: 0.0174, G_loss: 7.4162\n",
      "Epoch [375/800], Step [6/53], D_loss: 0.0113, G_loss: 7.9445\n",
      "Epoch [375/800], Step [8/53], D_loss: 0.0633, G_loss: 9.0504\n",
      "Epoch [375/800], Step [10/53], D_loss: 0.0469, G_loss: 3.2572\n",
      "Epoch [375/800], Step [12/53], D_loss: 0.0023, G_loss: 8.8965\n",
      "Epoch [375/800], Step [14/53], D_loss: 0.0856, G_loss: 9.7176\n",
      "Epoch [375/800], Step [16/53], D_loss: 0.0071, G_loss: 8.9410\n",
      "Epoch [375/800], Step [18/53], D_loss: 0.1156, G_loss: 2.9508\n",
      "Epoch [375/800], Step [20/53], D_loss: 0.0800, G_loss: 4.9927\n",
      "Epoch [375/800], Step [22/53], D_loss: 0.0048, G_loss: 10.7748\n",
      "Epoch [375/800], Step [24/53], D_loss: 0.0372, G_loss: 7.7644\n",
      "Epoch [375/800], Step [26/53], D_loss: 0.0102, G_loss: 8.9440\n",
      "Epoch [375/800], Step [28/53], D_loss: 0.0006, G_loss: 10.1130\n",
      "Epoch [375/800], Step [30/53], D_loss: 0.0305, G_loss: 6.0883\n",
      "Epoch [375/800], Step [32/53], D_loss: 0.0595, G_loss: 6.7690\n",
      "Epoch [375/800], Step [34/53], D_loss: 0.1061, G_loss: 7.0279\n",
      "Epoch [375/800], Step [36/53], D_loss: 0.0865, G_loss: 11.7576\n",
      "Epoch [375/800], Step [38/53], D_loss: 0.3352, G_loss: 8.7049\n",
      "Epoch [375/800], Step [40/53], D_loss: 0.0271, G_loss: 8.3437\n",
      "Epoch [375/800], Step [42/53], D_loss: 2.4531, G_loss: 8.4877\n",
      "Epoch [375/800], Step [44/53], D_loss: 0.3380, G_loss: 1.6751\n",
      "Epoch [375/800], Step [46/53], D_loss: 0.2726, G_loss: 1.6969\n",
      "Epoch [375/800], Step [48/53], D_loss: 0.2364, G_loss: 2.9151\n",
      "Epoch [375/800], Step [50/53], D_loss: 0.2175, G_loss: 2.9719\n",
      "Epoch [375/800], Step [52/53], D_loss: 0.5517, G_loss: 3.9434\n",
      "Epoch [375/800], Avg D_loss: 0.2251, Avg G_loss: 6.6484\n",
      "Epoch [376/800], Step [2/53], D_loss: 0.1052, G_loss: 2.6863\n",
      "Epoch [376/800], Step [4/53], D_loss: 0.7130, G_loss: 2.1788\n",
      "Epoch [376/800], Step [6/53], D_loss: 0.1070, G_loss: 1.9572\n",
      "Epoch [376/800], Step [8/53], D_loss: 0.1631, G_loss: 2.5553\n",
      "Epoch [376/800], Step [10/53], D_loss: 0.1257, G_loss: 0.9073\n",
      "Epoch [376/800], Step [12/53], D_loss: 0.1311, G_loss: 5.4354\n",
      "Epoch [376/800], Step [14/53], D_loss: 0.0582, G_loss: 0.8984\n",
      "Epoch [376/800], Step [16/53], D_loss: 0.2049, G_loss: 5.4805\n",
      "Epoch [376/800], Step [18/53], D_loss: 0.3946, G_loss: 1.8654\n",
      "Epoch [376/800], Step [20/53], D_loss: 0.3981, G_loss: 3.6258\n",
      "Epoch [376/800], Step [22/53], D_loss: 0.1958, G_loss: 3.7536\n",
      "Epoch [376/800], Step [24/53], D_loss: 0.1238, G_loss: 3.9771\n",
      "Epoch [376/800], Step [26/53], D_loss: 0.1048, G_loss: 5.7273\n",
      "Epoch [376/800], Step [28/53], D_loss: 0.0416, G_loss: 3.8438\n",
      "Epoch [376/800], Step [30/53], D_loss: 0.0999, G_loss: 4.1871\n",
      "Epoch [376/800], Step [32/53], D_loss: 0.0338, G_loss: 4.8655\n",
      "Epoch [376/800], Step [34/53], D_loss: 0.1323, G_loss: 7.5414\n",
      "Epoch [376/800], Step [36/53], D_loss: 0.0870, G_loss: 3.3240\n",
      "Epoch [376/800], Step [38/53], D_loss: 0.0977, G_loss: 6.9558\n",
      "Epoch [376/800], Step [40/53], D_loss: 0.4931, G_loss: 3.9369\n",
      "Epoch [376/800], Step [42/53], D_loss: 0.3148, G_loss: 3.8033\n",
      "Epoch [376/800], Step [44/53], D_loss: 0.0344, G_loss: 4.3004\n",
      "Epoch [376/800], Step [46/53], D_loss: 0.0678, G_loss: 6.6473\n",
      "Epoch [376/800], Step [48/53], D_loss: 0.1012, G_loss: 4.0135\n",
      "Epoch [376/800], Step [50/53], D_loss: 0.2923, G_loss: 8.5122\n",
      "Epoch [376/800], Step [52/53], D_loss: 0.9764, G_loss: 5.1361\n",
      "Epoch [376/800], Avg D_loss: 0.2392, Avg G_loss: 3.7840\n",
      "Epoch [377/800], Step [2/53], D_loss: 0.0250, G_loss: 2.6244\n",
      "Epoch [377/800], Step [4/53], D_loss: 0.1623, G_loss: 3.7603\n",
      "Epoch [377/800], Step [6/53], D_loss: 0.0499, G_loss: 3.9171\n",
      "Epoch [377/800], Step [8/53], D_loss: 0.1201, G_loss: 2.4822\n",
      "Epoch [377/800], Step [10/53], D_loss: 0.0879, G_loss: 4.2725\n",
      "Epoch [377/800], Step [12/53], D_loss: 0.2865, G_loss: 1.8637\n",
      "Epoch [377/800], Step [14/53], D_loss: 0.1022, G_loss: 6.2833\n",
      "Epoch [377/800], Step [16/53], D_loss: 0.1901, G_loss: 3.2086\n",
      "Epoch [377/800], Step [18/53], D_loss: 0.3035, G_loss: 3.2636\n",
      "Epoch [377/800], Step [20/53], D_loss: 0.2862, G_loss: 5.0790\n",
      "Epoch [377/800], Step [22/53], D_loss: 0.0305, G_loss: 5.2086\n",
      "Epoch [377/800], Step [24/53], D_loss: 0.1152, G_loss: 4.4759\n",
      "Epoch [377/800], Step [26/53], D_loss: 0.3299, G_loss: 3.5860\n",
      "Epoch [377/800], Step [28/53], D_loss: 0.0243, G_loss: 4.3631\n",
      "Epoch [377/800], Step [30/53], D_loss: 0.1694, G_loss: 5.2195\n",
      "Epoch [377/800], Step [32/53], D_loss: 0.1440, G_loss: 4.3447\n",
      "Epoch [377/800], Step [34/53], D_loss: 0.0379, G_loss: 3.7069\n",
      "Epoch [377/800], Step [36/53], D_loss: 0.0412, G_loss: 5.9242\n",
      "Epoch [377/800], Step [38/53], D_loss: 0.0479, G_loss: 4.3700\n",
      "Epoch [377/800], Step [40/53], D_loss: 0.3446, G_loss: 6.5972\n",
      "Epoch [377/800], Step [42/53], D_loss: 0.5709, G_loss: 2.6319\n",
      "Epoch [377/800], Step [44/53], D_loss: 0.4804, G_loss: 7.5105\n",
      "Epoch [377/800], Step [46/53], D_loss: 0.0416, G_loss: 7.9214\n",
      "Epoch [377/800], Step [48/53], D_loss: 0.1037, G_loss: 3.1204\n",
      "Epoch [377/800], Step [50/53], D_loss: 0.1152, G_loss: 5.2815\n",
      "Epoch [377/800], Step [52/53], D_loss: 0.0908, G_loss: 4.0747\n",
      "Epoch [377/800], Avg D_loss: 0.1644, Avg G_loss: 4.3157\n",
      "Epoch [378/800], Step [2/53], D_loss: 0.0135, G_loss: 2.0104\n",
      "Epoch [378/800], Step [4/53], D_loss: 0.0223, G_loss: 2.9965\n",
      "Epoch [378/800], Step [6/53], D_loss: 0.2909, G_loss: 3.8183\n",
      "Epoch [378/800], Step [8/53], D_loss: 0.1678, G_loss: 7.1505\n",
      "Epoch [378/800], Step [10/53], D_loss: 0.0283, G_loss: 1.0060\n",
      "Epoch [378/800], Step [12/53], D_loss: 0.0596, G_loss: 3.4533\n",
      "Epoch [378/800], Step [14/53], D_loss: 0.1530, G_loss: 6.7522\n",
      "Epoch [378/800], Step [16/53], D_loss: 1.1158, G_loss: 1.9514\n",
      "Epoch [378/800], Step [18/53], D_loss: 0.1028, G_loss: 4.2276\n",
      "Epoch [378/800], Step [20/53], D_loss: 0.2044, G_loss: 2.2342\n",
      "Epoch [378/800], Step [22/53], D_loss: 0.0762, G_loss: 2.1480\n",
      "Epoch [378/800], Step [24/53], D_loss: 0.0754, G_loss: 6.7130\n",
      "Epoch [378/800], Step [26/53], D_loss: 0.0627, G_loss: 2.0534\n",
      "Epoch [378/800], Step [28/53], D_loss: 0.1896, G_loss: 6.4056\n",
      "Epoch [378/800], Step [30/53], D_loss: 0.0577, G_loss: 5.4591\n",
      "Epoch [378/800], Step [32/53], D_loss: 0.0556, G_loss: 5.4358\n",
      "Epoch [378/800], Step [34/53], D_loss: 0.1189, G_loss: 5.3704\n",
      "Epoch [378/800], Step [36/53], D_loss: 0.0488, G_loss: 6.0642\n",
      "Epoch [378/800], Step [38/53], D_loss: 0.0806, G_loss: 2.7802\n",
      "Epoch [378/800], Step [40/53], D_loss: 0.0385, G_loss: 3.5611\n",
      "Epoch [378/800], Step [42/53], D_loss: 0.0378, G_loss: 7.5760\n",
      "Epoch [378/800], Step [44/53], D_loss: 0.7432, G_loss: 0.9986\n",
      "Epoch [378/800], Step [46/53], D_loss: 0.0300, G_loss: 6.9119\n",
      "Epoch [378/800], Step [48/53], D_loss: 0.0650, G_loss: 6.2499\n",
      "Epoch [378/800], Step [50/53], D_loss: 0.0515, G_loss: 1.4662\n",
      "Epoch [378/800], Step [52/53], D_loss: 0.0115, G_loss: 3.5400\n",
      "Epoch [378/800], Avg D_loss: 0.1482, Avg G_loss: 4.4257\n",
      "Epoch [379/800], Step [2/53], D_loss: 0.0365, G_loss: 4.4708\n",
      "Epoch [379/800], Step [4/53], D_loss: 0.0332, G_loss: 5.0970\n",
      "Epoch [379/800], Step [6/53], D_loss: 0.0442, G_loss: 4.0050\n",
      "Epoch [379/800], Step [8/53], D_loss: 0.0525, G_loss: 5.0344\n",
      "Epoch [379/800], Step [10/53], D_loss: 0.1329, G_loss: 4.9381\n",
      "Epoch [379/800], Step [12/53], D_loss: 0.2012, G_loss: 6.6784\n",
      "Epoch [379/800], Step [14/53], D_loss: 0.1031, G_loss: 7.1560\n",
      "Epoch [379/800], Step [16/53], D_loss: 0.0376, G_loss: 6.4718\n",
      "Epoch [379/800], Step [18/53], D_loss: 0.1917, G_loss: 7.7733\n",
      "Epoch [379/800], Step [20/53], D_loss: 0.0219, G_loss: 2.4344\n",
      "Epoch [379/800], Step [22/53], D_loss: 0.1189, G_loss: 4.2689\n",
      "Epoch [379/800], Step [24/53], D_loss: 0.0270, G_loss: 6.3468\n",
      "Epoch [379/800], Step [26/53], D_loss: 0.0394, G_loss: 6.6692\n",
      "Epoch [379/800], Step [28/53], D_loss: 0.0172, G_loss: 4.9489\n",
      "Epoch [379/800], Step [30/53], D_loss: 0.0462, G_loss: 3.6028\n",
      "Epoch [379/800], Step [32/53], D_loss: 0.1153, G_loss: 5.1810\n",
      "Epoch [379/800], Step [34/53], D_loss: 0.0070, G_loss: 7.0751\n",
      "Epoch [379/800], Step [36/53], D_loss: 0.0149, G_loss: 5.2406\n",
      "Epoch [379/800], Step [38/53], D_loss: 0.0149, G_loss: 5.5593\n",
      "Epoch [379/800], Step [40/53], D_loss: 0.0667, G_loss: 7.5486\n",
      "Epoch [379/800], Step [42/53], D_loss: 0.0250, G_loss: 7.4199\n",
      "Epoch [379/800], Step [44/53], D_loss: 0.0323, G_loss: 5.0969\n",
      "Epoch [379/800], Step [46/53], D_loss: 0.0826, G_loss: 8.4187\n",
      "Epoch [379/800], Step [48/53], D_loss: 0.0190, G_loss: 6.1470\n",
      "Epoch [379/800], Step [50/53], D_loss: 0.0145, G_loss: 6.7215\n",
      "Epoch [379/800], Step [52/53], D_loss: 0.0165, G_loss: 10.0845\n",
      "Epoch [379/800], Avg D_loss: 0.0570, Avg G_loss: 5.8777\n",
      "Epoch [380/800], Step [2/53], D_loss: 0.0191, G_loss: 7.3541\n",
      "Epoch [380/800], Step [4/53], D_loss: 0.0083, G_loss: 10.4908\n",
      "Epoch [380/800], Step [6/53], D_loss: 0.0048, G_loss: 10.1141\n",
      "Epoch [380/800], Step [8/53], D_loss: 0.0115, G_loss: 8.1016\n",
      "Epoch [380/800], Step [10/53], D_loss: 0.0308, G_loss: 5.6457\n",
      "Epoch [380/800], Step [12/53], D_loss: 0.1410, G_loss: 8.2485\n",
      "Epoch [380/800], Step [14/53], D_loss: 0.0045, G_loss: 8.2348\n",
      "Epoch [380/800], Step [16/53], D_loss: 0.0202, G_loss: 6.3598\n",
      "Epoch [380/800], Step [18/53], D_loss: 0.0510, G_loss: 8.3465\n",
      "Epoch [380/800], Step [20/53], D_loss: 0.0034, G_loss: 9.3622\n",
      "Epoch [380/800], Step [22/53], D_loss: 0.0191, G_loss: 8.5617\n",
      "Epoch [380/800], Step [24/53], D_loss: 0.0161, G_loss: 8.5374\n",
      "Epoch [380/800], Step [26/53], D_loss: 0.0269, G_loss: 9.8113\n",
      "Epoch [380/800], Step [28/53], D_loss: 0.0108, G_loss: 10.4690\n",
      "Epoch [380/800], Step [30/53], D_loss: 0.0032, G_loss: 10.8017\n",
      "Epoch [380/800], Step [32/53], D_loss: 0.0467, G_loss: 12.0229\n",
      "Epoch [380/800], Step [34/53], D_loss: 0.0338, G_loss: 4.8619\n",
      "Epoch [380/800], Step [36/53], D_loss: 0.0241, G_loss: 7.8169\n",
      "Epoch [380/800], Step [38/53], D_loss: 0.0054, G_loss: 7.4735\n",
      "Epoch [380/800], Step [40/53], D_loss: 0.0139, G_loss: 5.9836\n",
      "Epoch [380/800], Step [42/53], D_loss: 0.0297, G_loss: 9.7048\n",
      "Epoch [380/800], Step [44/53], D_loss: 0.0856, G_loss: 10.6750\n",
      "Epoch [380/800], Step [46/53], D_loss: 0.0007, G_loss: 9.1772\n",
      "Epoch [380/800], Step [48/53], D_loss: 0.0138, G_loss: 9.1508\n",
      "Epoch [380/800], Step [50/53], D_loss: 0.0907, G_loss: 5.4042\n",
      "Epoch [380/800], Step [52/53], D_loss: 0.0282, G_loss: 7.4156\n",
      "Epoch [380/800], Avg D_loss: 0.0368, Avg G_loss: 8.1833\n",
      "Generated images saved as generated_images/epoch_380.png\n",
      "Epoch [381/800], Step [2/53], D_loss: 0.0035, G_loss: 12.6208\n",
      "Epoch [381/800], Step [4/53], D_loss: 0.0188, G_loss: 7.1827\n",
      "Epoch [381/800], Step [6/53], D_loss: 0.0185, G_loss: 10.3990\n",
      "Epoch [381/800], Step [8/53], D_loss: 0.0576, G_loss: 7.9280\n",
      "Epoch [381/800], Step [10/53], D_loss: 0.0010, G_loss: 10.6458\n",
      "Epoch [381/800], Step [12/53], D_loss: 0.0265, G_loss: 6.5453\n",
      "Epoch [381/800], Step [14/53], D_loss: 0.0043, G_loss: 9.8883\n",
      "Epoch [381/800], Step [16/53], D_loss: 0.0478, G_loss: 7.8207\n",
      "Epoch [381/800], Step [18/53], D_loss: 0.0030, G_loss: 5.5171\n",
      "Epoch [381/800], Step [20/53], D_loss: 0.4409, G_loss: 18.2301\n",
      "Epoch [381/800], Step [22/53], D_loss: 0.0203, G_loss: 5.0207\n",
      "Epoch [381/800], Step [24/53], D_loss: 0.8126, G_loss: 12.0011\n",
      "Epoch [381/800], Step [26/53], D_loss: 0.1947, G_loss: 7.4391\n",
      "Epoch [381/800], Step [28/53], D_loss: 0.0899, G_loss: 3.9082\n",
      "Epoch [381/800], Step [30/53], D_loss: 0.0117, G_loss: 3.1073\n",
      "Epoch [381/800], Step [32/53], D_loss: 0.0380, G_loss: 4.6947\n",
      "Epoch [381/800], Step [34/53], D_loss: 0.0555, G_loss: 4.6603\n",
      "Epoch [381/800], Step [36/53], D_loss: 0.1077, G_loss: 6.6302\n",
      "Epoch [381/800], Step [38/53], D_loss: 0.0832, G_loss: 4.5105\n",
      "Epoch [381/800], Step [40/53], D_loss: 0.0338, G_loss: 4.6066\n",
      "Epoch [381/800], Step [42/53], D_loss: 0.0374, G_loss: 4.4172\n",
      "Epoch [381/800], Step [44/53], D_loss: 0.0266, G_loss: 3.1795\n",
      "Epoch [381/800], Step [46/53], D_loss: 0.0083, G_loss: 5.1187\n",
      "Epoch [381/800], Step [48/53], D_loss: 0.0379, G_loss: 3.4306\n",
      "Epoch [381/800], Step [50/53], D_loss: 0.0024, G_loss: 5.6355\n",
      "Epoch [381/800], Step [52/53], D_loss: 0.0570, G_loss: 5.6688\n",
      "Epoch [381/800], Avg D_loss: 0.1011, Avg G_loss: 6.8861\n",
      "Epoch [382/800], Step [2/53], D_loss: 0.0876, G_loss: 5.1668\n",
      "Epoch [382/800], Step [4/53], D_loss: 0.0036, G_loss: 9.3041\n",
      "Epoch [382/800], Step [6/53], D_loss: 0.0079, G_loss: 3.6796\n",
      "Epoch [382/800], Step [8/53], D_loss: 0.1355, G_loss: 8.9925\n",
      "Epoch [382/800], Step [10/53], D_loss: 0.0219, G_loss: 9.0032\n",
      "Epoch [382/800], Step [12/53], D_loss: 0.0481, G_loss: 9.2530\n",
      "Epoch [382/800], Step [14/53], D_loss: 0.0097, G_loss: 9.6893\n",
      "Epoch [382/800], Step [16/53], D_loss: 0.0510, G_loss: 8.2262\n",
      "Epoch [382/800], Step [18/53], D_loss: 0.0032, G_loss: 8.3045\n",
      "Epoch [382/800], Step [20/53], D_loss: 0.0221, G_loss: 6.8897\n",
      "Epoch [382/800], Step [22/53], D_loss: 0.0203, G_loss: 9.7951\n",
      "Epoch [382/800], Step [24/53], D_loss: 0.0261, G_loss: 7.1551\n",
      "Epoch [382/800], Step [26/53], D_loss: 1.3692, G_loss: 1.3469\n",
      "Epoch [382/800], Step [28/53], D_loss: 1.4139, G_loss: 6.1250\n",
      "Epoch [382/800], Step [30/53], D_loss: 0.0655, G_loss: 4.0636\n",
      "Epoch [382/800], Step [32/53], D_loss: 0.1765, G_loss: 2.7152\n",
      "Epoch [382/800], Step [34/53], D_loss: 0.0919, G_loss: 3.4648\n",
      "Epoch [382/800], Step [36/53], D_loss: 0.1145, G_loss: 4.1841\n",
      "Epoch [382/800], Step [38/53], D_loss: 0.0709, G_loss: 2.6276\n",
      "Epoch [382/800], Step [40/53], D_loss: 0.0770, G_loss: 3.3343\n",
      "Epoch [382/800], Step [42/53], D_loss: 0.0678, G_loss: 5.1127\n",
      "Epoch [382/800], Step [44/53], D_loss: 0.1123, G_loss: 3.6777\n",
      "Epoch [382/800], Step [46/53], D_loss: 0.0220, G_loss: 4.1232\n",
      "Epoch [382/800], Step [48/53], D_loss: 0.0205, G_loss: 4.1937\n",
      "Epoch [382/800], Step [50/53], D_loss: 0.0317, G_loss: 5.2192\n",
      "Epoch [382/800], Step [52/53], D_loss: 0.0982, G_loss: 4.2550\n",
      "Epoch [382/800], Avg D_loss: 0.1681, Avg G_loss: 5.9129\n",
      "Epoch [383/800], Step [2/53], D_loss: 0.0150, G_loss: 3.0043\n",
      "Epoch [383/800], Step [4/53], D_loss: 0.0951, G_loss: 5.8043\n",
      "Epoch [383/800], Step [6/53], D_loss: 0.0152, G_loss: 5.4113\n",
      "Epoch [383/800], Step [8/53], D_loss: 0.0681, G_loss: 6.8118\n",
      "Epoch [383/800], Step [10/53], D_loss: 0.0674, G_loss: 2.1846\n",
      "Epoch [383/800], Step [12/53], D_loss: 0.0128, G_loss: 4.6454\n",
      "Epoch [383/800], Step [14/53], D_loss: 0.0095, G_loss: 4.9473\n",
      "Epoch [383/800], Step [16/53], D_loss: 0.0762, G_loss: 8.0974\n",
      "Epoch [383/800], Step [18/53], D_loss: 0.0192, G_loss: 6.7739\n",
      "Epoch [383/800], Step [20/53], D_loss: 0.0963, G_loss: 9.1897\n",
      "Epoch [383/800], Step [22/53], D_loss: 0.2366, G_loss: 3.3502\n",
      "Epoch [383/800], Step [24/53], D_loss: 0.0190, G_loss: 2.0993\n",
      "Epoch [383/800], Step [26/53], D_loss: 0.0099, G_loss: 6.8777\n",
      "Epoch [383/800], Step [28/53], D_loss: 0.0554, G_loss: 5.9447\n",
      "Epoch [383/800], Step [30/53], D_loss: 0.0115, G_loss: 9.0320\n",
      "Epoch [383/800], Step [32/53], D_loss: 0.1138, G_loss: 6.1129\n",
      "Epoch [383/800], Step [34/53], D_loss: 0.0047, G_loss: 6.4898\n",
      "Epoch [383/800], Step [36/53], D_loss: 0.1405, G_loss: 7.5925\n",
      "Epoch [383/800], Step [38/53], D_loss: 0.0125, G_loss: 4.7977\n",
      "Epoch [383/800], Step [40/53], D_loss: 0.0640, G_loss: 7.0811\n",
      "Epoch [383/800], Step [42/53], D_loss: 0.0166, G_loss: 5.3532\n",
      "Epoch [383/800], Step [44/53], D_loss: 0.0053, G_loss: 7.5616\n",
      "Epoch [383/800], Step [46/53], D_loss: 0.0394, G_loss: 5.0238\n",
      "Epoch [383/800], Step [48/53], D_loss: 0.7076, G_loss: 7.8141\n",
      "Epoch [383/800], Step [50/53], D_loss: 0.0392, G_loss: 5.6862\n",
      "Epoch [383/800], Step [52/53], D_loss: 0.0558, G_loss: 9.5327\n",
      "Epoch [383/800], Avg D_loss: 0.1309, Avg G_loss: 5.6932\n",
      "Epoch [384/800], Step [2/53], D_loss: 0.3045, G_loss: 4.3447\n",
      "Epoch [384/800], Step [4/53], D_loss: 0.0667, G_loss: 4.9132\n",
      "Epoch [384/800], Step [6/53], D_loss: 0.1805, G_loss: 3.0718\n",
      "Epoch [384/800], Step [8/53], D_loss: 0.2978, G_loss: 4.7153\n",
      "Epoch [384/800], Step [10/53], D_loss: 0.1072, G_loss: 5.0333\n",
      "Epoch [384/800], Step [12/53], D_loss: 0.0664, G_loss: 3.4657\n",
      "Epoch [384/800], Step [14/53], D_loss: 0.0213, G_loss: 1.9458\n",
      "Epoch [384/800], Step [16/53], D_loss: 0.1067, G_loss: 3.7645\n",
      "Epoch [384/800], Step [18/53], D_loss: 0.1139, G_loss: 3.0213\n",
      "Epoch [384/800], Step [20/53], D_loss: 0.0385, G_loss: 5.4444\n",
      "Epoch [384/800], Step [22/53], D_loss: 0.0113, G_loss: 3.6609\n",
      "Epoch [384/800], Step [24/53], D_loss: 0.0182, G_loss: 4.2062\n",
      "Epoch [384/800], Step [26/53], D_loss: 0.2056, G_loss: 5.5083\n",
      "Epoch [384/800], Step [28/53], D_loss: 0.1417, G_loss: 2.5939\n",
      "Epoch [384/800], Step [30/53], D_loss: 0.1455, G_loss: 6.1652\n",
      "Epoch [384/800], Step [32/53], D_loss: 0.1542, G_loss: 6.7820\n",
      "Epoch [384/800], Step [34/53], D_loss: 0.0053, G_loss: 4.3971\n",
      "Epoch [384/800], Step [36/53], D_loss: 0.2555, G_loss: 8.8771\n",
      "Epoch [384/800], Step [38/53], D_loss: 0.1632, G_loss: 5.3973\n",
      "Epoch [384/800], Step [40/53], D_loss: 0.2329, G_loss: 7.3157\n",
      "Epoch [384/800], Step [42/53], D_loss: 0.1068, G_loss: 6.5727\n",
      "Epoch [384/800], Step [44/53], D_loss: 0.0419, G_loss: 3.7222\n",
      "Epoch [384/800], Step [46/53], D_loss: 0.0910, G_loss: 4.1220\n",
      "Epoch [384/800], Step [48/53], D_loss: 0.1328, G_loss: 6.0748\n",
      "Epoch [384/800], Step [50/53], D_loss: 0.0113, G_loss: 6.0120\n",
      "Epoch [384/800], Step [52/53], D_loss: 0.0295, G_loss: 2.1250\n",
      "Epoch [384/800], Avg D_loss: 0.1560, Avg G_loss: 4.7050\n",
      "Epoch [385/800], Step [2/53], D_loss: 0.0910, G_loss: 4.0079\n",
      "Epoch [385/800], Step [4/53], D_loss: 0.2107, G_loss: 2.0511\n",
      "Epoch [385/800], Step [6/53], D_loss: 0.1254, G_loss: 4.0964\n",
      "Epoch [385/800], Step [8/53], D_loss: 0.1356, G_loss: 1.6078\n",
      "Epoch [385/800], Step [10/53], D_loss: 0.0705, G_loss: 3.6422\n",
      "Epoch [385/800], Step [12/53], D_loss: 0.0481, G_loss: 1.6760\n",
      "Epoch [385/800], Step [14/53], D_loss: 0.1631, G_loss: 5.1544\n",
      "Epoch [385/800], Step [16/53], D_loss: 0.0735, G_loss: 4.9858\n",
      "Epoch [385/800], Step [18/53], D_loss: 0.1359, G_loss: 2.4909\n",
      "Epoch [385/800], Step [20/53], D_loss: 0.0791, G_loss: 2.0342\n",
      "Epoch [385/800], Step [22/53], D_loss: 0.0737, G_loss: 3.4708\n",
      "Epoch [385/800], Step [24/53], D_loss: 0.2417, G_loss: 4.8043\n",
      "Epoch [385/800], Step [26/53], D_loss: 0.0487, G_loss: 5.0735\n",
      "Epoch [385/800], Step [28/53], D_loss: 0.0187, G_loss: 3.6377\n",
      "Epoch [385/800], Step [30/53], D_loss: 0.0507, G_loss: 5.6360\n",
      "Epoch [385/800], Step [32/53], D_loss: 0.1041, G_loss: 4.8226\n",
      "Epoch [385/800], Step [34/53], D_loss: 0.0463, G_loss: 5.2768\n",
      "Epoch [385/800], Step [36/53], D_loss: 0.0157, G_loss: 1.0185\n",
      "Epoch [385/800], Step [38/53], D_loss: 0.0964, G_loss: 4.9665\n",
      "Epoch [385/800], Step [40/53], D_loss: 0.0100, G_loss: 6.1763\n",
      "Epoch [385/800], Step [42/53], D_loss: 0.0696, G_loss: 6.3049\n",
      "Epoch [385/800], Step [44/53], D_loss: 0.0511, G_loss: 6.9364\n",
      "Epoch [385/800], Step [46/53], D_loss: 0.0789, G_loss: 3.3882\n",
      "Epoch [385/800], Step [48/53], D_loss: 0.1156, G_loss: 8.4807\n",
      "Epoch [385/800], Step [50/53], D_loss: 0.0228, G_loss: 7.1730\n",
      "Epoch [385/800], Step [52/53], D_loss: 0.0119, G_loss: 6.0701\n",
      "Epoch [385/800], Avg D_loss: 0.1234, Avg G_loss: 4.4637\n",
      "Epoch [386/800], Step [2/53], D_loss: 0.0062, G_loss: 5.8558\n",
      "Epoch [386/800], Step [4/53], D_loss: 0.0246, G_loss: 7.3405\n",
      "Epoch [386/800], Step [6/53], D_loss: 0.0549, G_loss: 7.4068\n",
      "Epoch [386/800], Step [8/53], D_loss: 0.0166, G_loss: 10.2516\n",
      "Epoch [386/800], Step [10/53], D_loss: 0.2110, G_loss: 6.5831\n",
      "Epoch [386/800], Step [12/53], D_loss: 0.0359, G_loss: 7.3486\n",
      "Epoch [386/800], Step [14/53], D_loss: 0.0150, G_loss: 5.1504\n",
      "Epoch [386/800], Step [16/53], D_loss: 0.0846, G_loss: 5.1027\n",
      "Epoch [386/800], Step [18/53], D_loss: 0.0098, G_loss: 5.4558\n",
      "Epoch [386/800], Step [20/53], D_loss: 2.0076, G_loss: 5.8123\n",
      "Epoch [386/800], Step [22/53], D_loss: 0.0394, G_loss: 2.5952\n",
      "Epoch [386/800], Step [24/53], D_loss: 0.0587, G_loss: 6.1224\n",
      "Epoch [386/800], Step [26/53], D_loss: 0.2460, G_loss: 4.5892\n",
      "Epoch [386/800], Step [28/53], D_loss: 0.0544, G_loss: 3.2740\n",
      "Epoch [386/800], Step [30/53], D_loss: 0.0648, G_loss: 5.8154\n",
      "Epoch [386/800], Step [32/53], D_loss: 0.1646, G_loss: 3.2516\n",
      "Epoch [386/800], Step [34/53], D_loss: 0.0425, G_loss: 4.1162\n",
      "Epoch [386/800], Step [36/53], D_loss: 0.2511, G_loss: 6.8222\n",
      "Epoch [386/800], Step [38/53], D_loss: 0.1894, G_loss: 5.4152\n",
      "Epoch [386/800], Step [40/53], D_loss: 0.2290, G_loss: 2.8877\n",
      "Epoch [386/800], Step [42/53], D_loss: 0.1171, G_loss: 6.8998\n",
      "Epoch [386/800], Step [44/53], D_loss: 0.3131, G_loss: 3.9117\n",
      "Epoch [386/800], Step [46/53], D_loss: 0.1740, G_loss: 2.2742\n",
      "Epoch [386/800], Step [48/53], D_loss: 0.1872, G_loss: 5.6012\n",
      "Epoch [386/800], Step [50/53], D_loss: 0.0185, G_loss: 6.7043\n",
      "Epoch [386/800], Step [52/53], D_loss: 0.0394, G_loss: 5.1681\n",
      "Epoch [386/800], Avg D_loss: 0.1748, Avg G_loss: 5.2873\n",
      "Epoch [387/800], Step [2/53], D_loss: 0.0036, G_loss: 5.2698\n",
      "Epoch [387/800], Step [4/53], D_loss: 0.2268, G_loss: 1.6438\n",
      "Epoch [387/800], Step [6/53], D_loss: 0.2130, G_loss: 5.8854\n",
      "Epoch [387/800], Step [8/53], D_loss: 0.0622, G_loss: 4.6227\n",
      "Epoch [387/800], Step [10/53], D_loss: 0.0093, G_loss: 5.7141\n",
      "Epoch [387/800], Step [12/53], D_loss: 0.3495, G_loss: 9.5799\n",
      "Epoch [387/800], Step [14/53], D_loss: 0.0564, G_loss: 6.1756\n",
      "Epoch [387/800], Step [16/53], D_loss: 0.0084, G_loss: 4.5875\n",
      "Epoch [387/800], Step [18/53], D_loss: 0.1579, G_loss: 4.2804\n",
      "Epoch [387/800], Step [20/53], D_loss: 0.0225, G_loss: 5.7064\n",
      "Epoch [387/800], Step [22/53], D_loss: 0.1173, G_loss: 4.9186\n",
      "Epoch [387/800], Step [24/53], D_loss: 0.0238, G_loss: 5.7161\n",
      "Epoch [387/800], Step [26/53], D_loss: 0.0078, G_loss: 8.5944\n",
      "Epoch [387/800], Step [28/53], D_loss: 0.1407, G_loss: 7.1418\n",
      "Epoch [387/800], Step [30/53], D_loss: 0.2426, G_loss: 7.8872\n",
      "Epoch [387/800], Step [32/53], D_loss: 0.0252, G_loss: 7.7879\n",
      "Epoch [387/800], Step [34/53], D_loss: 0.0143, G_loss: 5.7406\n",
      "Epoch [387/800], Step [36/53], D_loss: 0.2048, G_loss: 6.1147\n",
      "Epoch [387/800], Step [38/53], D_loss: 0.3827, G_loss: 8.2113\n",
      "Epoch [387/800], Step [40/53], D_loss: 0.0133, G_loss: 4.8420\n",
      "Epoch [387/800], Step [42/53], D_loss: 0.3274, G_loss: 7.5883\n",
      "Epoch [387/800], Step [44/53], D_loss: 0.4686, G_loss: 3.7253\n",
      "Epoch [387/800], Step [46/53], D_loss: 0.3796, G_loss: 3.9348\n",
      "Epoch [387/800], Step [48/53], D_loss: 0.0887, G_loss: 5.0979\n",
      "Epoch [387/800], Step [50/53], D_loss: 0.0169, G_loss: 4.7237\n",
      "Epoch [387/800], Step [52/53], D_loss: 0.0938, G_loss: 3.2066\n",
      "Epoch [387/800], Avg D_loss: 0.1143, Avg G_loss: 5.5280\n",
      "Epoch [388/800], Step [2/53], D_loss: 0.2114, G_loss: 5.7156\n",
      "Epoch [388/800], Step [4/53], D_loss: 0.0842, G_loss: 5.8794\n",
      "Epoch [388/800], Step [6/53], D_loss: 0.0353, G_loss: 8.8294\n",
      "Epoch [388/800], Step [8/53], D_loss: 0.0114, G_loss: 6.2657\n",
      "Epoch [388/800], Step [10/53], D_loss: 0.0162, G_loss: 8.3266\n",
      "Epoch [388/800], Step [12/53], D_loss: 0.0407, G_loss: 7.4407\n",
      "Epoch [388/800], Step [14/53], D_loss: 0.0187, G_loss: 5.5123\n",
      "Epoch [388/800], Step [16/53], D_loss: 0.0162, G_loss: 5.1142\n",
      "Epoch [388/800], Step [18/53], D_loss: 0.0616, G_loss: 6.2736\n",
      "Epoch [388/800], Step [20/53], D_loss: 0.1478, G_loss: 7.4014\n",
      "Epoch [388/800], Step [22/53], D_loss: 0.0606, G_loss: 7.5851\n",
      "Epoch [388/800], Step [24/53], D_loss: 0.0502, G_loss: 4.0304\n",
      "Epoch [388/800], Step [26/53], D_loss: 0.0561, G_loss: 5.8266\n",
      "Epoch [388/800], Step [28/53], D_loss: 0.0247, G_loss: 4.7379\n",
      "Epoch [388/800], Step [30/53], D_loss: 0.0443, G_loss: 5.9697\n",
      "Epoch [388/800], Step [32/53], D_loss: 0.2218, G_loss: 10.7139\n",
      "Epoch [388/800], Step [34/53], D_loss: 0.0499, G_loss: 7.2135\n",
      "Epoch [388/800], Step [36/53], D_loss: 0.0384, G_loss: 6.3600\n",
      "Epoch [388/800], Step [38/53], D_loss: 0.0352, G_loss: 8.1586\n",
      "Epoch [388/800], Step [40/53], D_loss: 0.0383, G_loss: 8.2946\n",
      "Epoch [388/800], Step [42/53], D_loss: 0.0050, G_loss: 5.9445\n",
      "Epoch [388/800], Step [44/53], D_loss: 0.0477, G_loss: 8.2760\n",
      "Epoch [388/800], Step [46/53], D_loss: 0.0017, G_loss: 3.3765\n",
      "Epoch [388/800], Step [48/53], D_loss: 0.0285, G_loss: 6.3774\n",
      "Epoch [388/800], Step [50/53], D_loss: 0.0099, G_loss: 5.3661\n",
      "Epoch [388/800], Step [52/53], D_loss: 0.0195, G_loss: 7.5678\n",
      "Epoch [388/800], Avg D_loss: 0.0497, Avg G_loss: 6.6824\n",
      "Epoch [389/800], Step [2/53], D_loss: 0.0015, G_loss: 8.1956\n",
      "Epoch [389/800], Step [4/53], D_loss: 0.2511, G_loss: 2.2598\n",
      "Epoch [389/800], Step [6/53], D_loss: 0.0695, G_loss: 7.5624\n",
      "Epoch [389/800], Step [8/53], D_loss: 0.1062, G_loss: 4.5729\n",
      "Epoch [389/800], Step [10/53], D_loss: 0.1547, G_loss: 4.7274\n",
      "Epoch [389/800], Step [12/53], D_loss: 0.0726, G_loss: 10.5019\n",
      "Epoch [389/800], Step [14/53], D_loss: 0.0007, G_loss: 8.1782\n",
      "Epoch [389/800], Step [16/53], D_loss: 0.0194, G_loss: 5.8368\n",
      "Epoch [389/800], Step [18/53], D_loss: 0.0018, G_loss: 7.0667\n",
      "Epoch [389/800], Step [20/53], D_loss: 0.0335, G_loss: 7.2248\n",
      "Epoch [389/800], Step [22/53], D_loss: 0.0041, G_loss: 4.2911\n",
      "Epoch [389/800], Step [24/53], D_loss: 0.2814, G_loss: 5.3325\n",
      "Epoch [389/800], Step [26/53], D_loss: 0.0782, G_loss: 5.5002\n",
      "Epoch [389/800], Step [28/53], D_loss: 0.0397, G_loss: 8.6126\n",
      "Epoch [389/800], Step [30/53], D_loss: 0.1018, G_loss: 2.2418\n",
      "Epoch [389/800], Step [32/53], D_loss: 0.0027, G_loss: 8.4265\n",
      "Epoch [389/800], Step [34/53], D_loss: 0.0203, G_loss: 9.4441\n",
      "Epoch [389/800], Step [36/53], D_loss: 0.0078, G_loss: 6.4935\n",
      "Epoch [389/800], Step [38/53], D_loss: 0.0305, G_loss: 7.3996\n",
      "Epoch [389/800], Step [40/53], D_loss: 0.1930, G_loss: 12.4854\n",
      "Epoch [389/800], Step [42/53], D_loss: 0.0086, G_loss: 2.9262\n",
      "Epoch [389/800], Step [44/53], D_loss: 0.1650, G_loss: 7.8026\n",
      "Epoch [389/800], Step [46/53], D_loss: 0.5168, G_loss: 2.3156\n",
      "Epoch [389/800], Step [48/53], D_loss: 0.1256, G_loss: 4.4106\n",
      "Epoch [389/800], Step [50/53], D_loss: 0.0517, G_loss: 5.7757\n",
      "Epoch [389/800], Step [52/53], D_loss: 0.0101, G_loss: 7.0671\n",
      "Epoch [389/800], Avg D_loss: 0.1142, Avg G_loss: 6.5081\n",
      "Epoch [390/800], Step [2/53], D_loss: 0.5323, G_loss: 8.6418\n",
      "Epoch [390/800], Step [4/53], D_loss: 0.7178, G_loss: 5.3979\n",
      "Epoch [390/800], Step [6/53], D_loss: 0.1781, G_loss: 3.7637\n",
      "Epoch [390/800], Step [8/53], D_loss: 0.3362, G_loss: 5.9719\n",
      "Epoch [390/800], Step [10/53], D_loss: 0.0417, G_loss: 2.8124\n",
      "Epoch [390/800], Step [12/53], D_loss: 0.0067, G_loss: 6.4236\n",
      "Epoch [390/800], Step [14/53], D_loss: 0.0608, G_loss: 5.1901\n",
      "Epoch [390/800], Step [16/53], D_loss: 0.1994, G_loss: 3.7510\n",
      "Epoch [390/800], Step [18/53], D_loss: 0.0792, G_loss: 6.4747\n",
      "Epoch [390/800], Step [20/53], D_loss: 0.0106, G_loss: 2.6548\n",
      "Epoch [390/800], Step [22/53], D_loss: 0.5003, G_loss: 5.2413\n",
      "Epoch [390/800], Step [24/53], D_loss: 0.1527, G_loss: 4.2682\n",
      "Epoch [390/800], Step [26/53], D_loss: 0.0213, G_loss: 6.2324\n",
      "Epoch [390/800], Step [28/53], D_loss: 0.0656, G_loss: 6.6480\n",
      "Epoch [390/800], Step [30/53], D_loss: 0.0549, G_loss: 3.2396\n",
      "Epoch [390/800], Step [32/53], D_loss: 0.0733, G_loss: 6.5560\n",
      "Epoch [390/800], Step [34/53], D_loss: 0.1713, G_loss: 4.5938\n",
      "Epoch [390/800], Step [36/53], D_loss: 0.0502, G_loss: 2.8948\n",
      "Epoch [390/800], Step [38/53], D_loss: 0.0399, G_loss: 4.8636\n",
      "Epoch [390/800], Step [40/53], D_loss: 0.0195, G_loss: 4.0007\n",
      "Epoch [390/800], Step [42/53], D_loss: 0.0702, G_loss: 7.0439\n",
      "Epoch [390/800], Step [44/53], D_loss: 0.0673, G_loss: 5.6784\n",
      "Epoch [390/800], Step [46/53], D_loss: 0.0142, G_loss: 4.8339\n",
      "Epoch [390/800], Step [48/53], D_loss: 0.0054, G_loss: 5.7663\n",
      "Epoch [390/800], Step [50/53], D_loss: 0.0150, G_loss: 5.7190\n",
      "Epoch [390/800], Step [52/53], D_loss: 0.0186, G_loss: 7.0988\n",
      "Epoch [390/800], Avg D_loss: 0.1160, Avg G_loss: 5.1793\n",
      "Epoch [391/800], Step [2/53], D_loss: 0.0027, G_loss: 5.4338\n",
      "Epoch [391/800], Step [4/53], D_loss: 0.0114, G_loss: 5.4310\n",
      "Epoch [391/800], Step [6/53], D_loss: 0.0257, G_loss: 6.2440\n",
      "Epoch [391/800], Step [8/53], D_loss: 0.0428, G_loss: 6.9270\n",
      "Epoch [391/800], Step [10/53], D_loss: 0.0652, G_loss: 4.6771\n",
      "Epoch [391/800], Step [12/53], D_loss: 0.0351, G_loss: 4.4332\n",
      "Epoch [391/800], Step [14/53], D_loss: 0.3129, G_loss: 7.3688\n",
      "Epoch [391/800], Step [16/53], D_loss: 0.0007, G_loss: 4.6518\n",
      "Epoch [391/800], Step [18/53], D_loss: 0.1118, G_loss: 5.7346\n",
      "Epoch [391/800], Step [20/53], D_loss: 0.0384, G_loss: 5.6100\n",
      "Epoch [391/800], Step [22/53], D_loss: 0.0873, G_loss: 9.7222\n",
      "Epoch [391/800], Step [24/53], D_loss: 0.0046, G_loss: 3.5110\n",
      "Epoch [391/800], Step [26/53], D_loss: 0.0549, G_loss: 8.0642\n",
      "Epoch [391/800], Step [28/53], D_loss: 0.0083, G_loss: 6.9508\n",
      "Epoch [391/800], Step [30/53], D_loss: 0.0275, G_loss: 6.3043\n",
      "Epoch [391/800], Step [32/53], D_loss: 0.0234, G_loss: 7.4094\n",
      "Epoch [391/800], Step [34/53], D_loss: 0.0151, G_loss: 5.9897\n",
      "Epoch [391/800], Step [36/53], D_loss: 0.0086, G_loss: 9.1682\n",
      "Epoch [391/800], Step [38/53], D_loss: 0.0284, G_loss: 8.9361\n",
      "Epoch [391/800], Step [40/53], D_loss: 0.0011, G_loss: 12.1052\n",
      "Epoch [391/800], Step [42/53], D_loss: 0.0189, G_loss: 8.0926\n",
      "Epoch [391/800], Step [44/53], D_loss: 0.0447, G_loss: 5.8937\n",
      "Epoch [391/800], Step [46/53], D_loss: 0.0669, G_loss: 9.2778\n",
      "Epoch [391/800], Step [48/53], D_loss: 0.0107, G_loss: 7.5805\n",
      "Epoch [391/800], Step [50/53], D_loss: 0.0958, G_loss: 8.8917\n",
      "Epoch [391/800], Step [52/53], D_loss: 0.0441, G_loss: 6.1408\n",
      "Epoch [391/800], Avg D_loss: 0.0444, Avg G_loss: 7.1684\n",
      "Epoch [392/800], Step [2/53], D_loss: 0.0982, G_loss: 11.4469\n",
      "Epoch [392/800], Step [4/53], D_loss: 0.1227, G_loss: 6.8970\n",
      "Epoch [392/800], Step [6/53], D_loss: 0.0065, G_loss: 6.4315\n",
      "Epoch [392/800], Step [8/53], D_loss: 0.0318, G_loss: 12.1501\n",
      "Epoch [392/800], Step [10/53], D_loss: 0.1002, G_loss: 8.1277\n",
      "Epoch [392/800], Step [12/53], D_loss: 0.3305, G_loss: 0.5421\n",
      "Epoch [392/800], Step [14/53], D_loss: 0.0132, G_loss: 7.7607\n",
      "Epoch [392/800], Step [16/53], D_loss: 0.0688, G_loss: 6.8395\n",
      "Epoch [392/800], Step [18/53], D_loss: 0.0537, G_loss: 6.5137\n",
      "Epoch [392/800], Step [20/53], D_loss: 0.0882, G_loss: 6.7672\n",
      "Epoch [392/800], Step [22/53], D_loss: 0.1830, G_loss: 5.3125\n",
      "Epoch [392/800], Step [24/53], D_loss: 0.0254, G_loss: 9.2791\n",
      "Epoch [392/800], Step [26/53], D_loss: 0.0269, G_loss: 7.0891\n",
      "Epoch [392/800], Step [28/53], D_loss: 0.0030, G_loss: 4.4988\n",
      "Epoch [392/800], Step [30/53], D_loss: 0.0427, G_loss: 7.9970\n",
      "Epoch [392/800], Step [32/53], D_loss: 0.0089, G_loss: 2.4779\n",
      "Epoch [392/800], Step [34/53], D_loss: 0.1271, G_loss: 7.2542\n",
      "Epoch [392/800], Step [36/53], D_loss: 0.0708, G_loss: 7.9736\n",
      "Epoch [392/800], Step [38/53], D_loss: 0.1112, G_loss: 7.3579\n",
      "Epoch [392/800], Step [40/53], D_loss: 0.0483, G_loss: 5.2534\n",
      "Epoch [392/800], Step [42/53], D_loss: 0.3647, G_loss: 8.2957\n",
      "Epoch [392/800], Step [44/53], D_loss: 1.5040, G_loss: 0.8300\n",
      "Epoch [392/800], Step [46/53], D_loss: 0.1216, G_loss: 6.4782\n",
      "Epoch [392/800], Step [48/53], D_loss: 0.1052, G_loss: 3.5634\n",
      "Epoch [392/800], Step [50/53], D_loss: 0.0933, G_loss: 4.0349\n",
      "Epoch [392/800], Step [52/53], D_loss: 0.0409, G_loss: 5.2734\n",
      "Epoch [392/800], Avg D_loss: 0.1708, Avg G_loss: 6.1695\n",
      "Epoch [393/800], Step [2/53], D_loss: 0.0426, G_loss: 4.8055\n",
      "Epoch [393/800], Step [4/53], D_loss: 0.0662, G_loss: 3.7412\n",
      "Epoch [393/800], Step [6/53], D_loss: 0.0397, G_loss: 2.6278\n",
      "Epoch [393/800], Step [8/53], D_loss: 0.1950, G_loss: 6.9270\n",
      "Epoch [393/800], Step [10/53], D_loss: 0.0476, G_loss: 5.3260\n",
      "Epoch [393/800], Step [12/53], D_loss: 0.1563, G_loss: 4.1980\n",
      "Epoch [393/800], Step [14/53], D_loss: 0.0424, G_loss: 4.5649\n",
      "Epoch [393/800], Step [16/53], D_loss: 0.1197, G_loss: 5.2862\n",
      "Epoch [393/800], Step [18/53], D_loss: 0.0183, G_loss: 6.3224\n",
      "Epoch [393/800], Step [20/53], D_loss: 0.0225, G_loss: 5.8037\n",
      "Epoch [393/800], Step [22/53], D_loss: 0.0639, G_loss: 4.7282\n",
      "Epoch [393/800], Step [24/53], D_loss: 0.1112, G_loss: 5.4645\n",
      "Epoch [393/800], Step [26/53], D_loss: 0.0033, G_loss: 6.3914\n",
      "Epoch [393/800], Step [28/53], D_loss: 0.0346, G_loss: 2.9754\n",
      "Epoch [393/800], Step [30/53], D_loss: 0.0779, G_loss: 7.4747\n",
      "Epoch [393/800], Step [32/53], D_loss: 0.3552, G_loss: 4.1089\n",
      "Epoch [393/800], Step [34/53], D_loss: 0.0313, G_loss: 2.4194\n",
      "Epoch [393/800], Step [36/53], D_loss: 0.0640, G_loss: 4.3175\n",
      "Epoch [393/800], Step [38/53], D_loss: 0.0243, G_loss: 3.4237\n",
      "Epoch [393/800], Step [40/53], D_loss: 0.0624, G_loss: 7.5996\n",
      "Epoch [393/800], Step [42/53], D_loss: 0.0776, G_loss: 6.7795\n",
      "Epoch [393/800], Step [44/53], D_loss: 0.0880, G_loss: 5.2780\n",
      "Epoch [393/800], Step [46/53], D_loss: 0.0074, G_loss: 5.4933\n",
      "Epoch [393/800], Step [48/53], D_loss: 0.0152, G_loss: 3.9973\n",
      "Epoch [393/800], Step [50/53], D_loss: 0.0034, G_loss: 6.1802\n",
      "Epoch [393/800], Step [52/53], D_loss: 0.1915, G_loss: 3.6149\n",
      "Epoch [393/800], Avg D_loss: 0.0797, Avg G_loss: 5.1127\n",
      "Epoch [394/800], Step [2/53], D_loss: 0.0390, G_loss: 6.4508\n",
      "Epoch [394/800], Step [4/53], D_loss: 0.0058, G_loss: 7.9879\n",
      "Epoch [394/800], Step [6/53], D_loss: 0.0141, G_loss: 6.0511\n",
      "Epoch [394/800], Step [8/53], D_loss: 0.0058, G_loss: 6.8845\n",
      "Epoch [394/800], Step [10/53], D_loss: 0.1071, G_loss: 10.8860\n",
      "Epoch [394/800], Step [12/53], D_loss: 0.0542, G_loss: 6.0603\n",
      "Epoch [394/800], Step [14/53], D_loss: 0.0498, G_loss: 5.9872\n",
      "Epoch [394/800], Step [16/53], D_loss: 0.0088, G_loss: 3.5699\n",
      "Epoch [394/800], Step [18/53], D_loss: 0.0104, G_loss: 9.6409\n",
      "Epoch [394/800], Step [20/53], D_loss: 0.0311, G_loss: 6.4536\n",
      "Epoch [394/800], Step [22/53], D_loss: 0.3177, G_loss: 5.7794\n",
      "Epoch [394/800], Step [24/53], D_loss: 0.1435, G_loss: 8.7598\n",
      "Epoch [394/800], Step [26/53], D_loss: 0.1075, G_loss: 7.2210\n",
      "Epoch [394/800], Step [28/53], D_loss: 0.0383, G_loss: 6.3681\n",
      "Epoch [394/800], Step [30/53], D_loss: 0.0339, G_loss: 7.1078\n",
      "Epoch [394/800], Step [32/53], D_loss: 0.0043, G_loss: 7.3734\n",
      "Epoch [394/800], Step [34/53], D_loss: 0.0074, G_loss: 7.1131\n",
      "Epoch [394/800], Step [36/53], D_loss: 0.0034, G_loss: 3.7160\n",
      "Epoch [394/800], Step [38/53], D_loss: 0.0886, G_loss: 7.5595\n",
      "Epoch [394/800], Step [40/53], D_loss: 0.2201, G_loss: 4.5553\n",
      "Epoch [394/800], Step [42/53], D_loss: 0.0013, G_loss: 4.3813\n",
      "Epoch [394/800], Step [44/53], D_loss: 0.0673, G_loss: 11.9083\n",
      "Epoch [394/800], Step [46/53], D_loss: 0.3042, G_loss: 4.7899\n",
      "Epoch [394/800], Step [48/53], D_loss: 0.0057, G_loss: 3.4813\n",
      "Epoch [394/800], Step [50/53], D_loss: 2.1703, G_loss: 8.7312\n",
      "Epoch [394/800], Step [52/53], D_loss: 0.0511, G_loss: 1.7462\n",
      "Epoch [394/800], Avg D_loss: 0.1564, Avg G_loss: 6.3669\n",
      "Epoch [395/800], Step [2/53], D_loss: 0.3445, G_loss: 4.9326\n",
      "Epoch [395/800], Step [4/53], D_loss: 0.1010, G_loss: 2.6268\n",
      "Epoch [395/800], Step [6/53], D_loss: 0.0995, G_loss: 3.0829\n",
      "Epoch [395/800], Step [8/53], D_loss: 0.1978, G_loss: 3.8840\n",
      "Epoch [395/800], Step [10/53], D_loss: 0.0759, G_loss: 2.7033\n",
      "Epoch [395/800], Step [12/53], D_loss: 0.2615, G_loss: 4.6780\n",
      "Epoch [395/800], Step [14/53], D_loss: 0.1371, G_loss: 1.2099\n",
      "Epoch [395/800], Step [16/53], D_loss: 0.0858, G_loss: 2.7999\n",
      "Epoch [395/800], Step [18/53], D_loss: 0.0642, G_loss: 6.2939\n",
      "Epoch [395/800], Step [20/53], D_loss: 0.0800, G_loss: 6.0668\n",
      "Epoch [395/800], Step [22/53], D_loss: 0.1900, G_loss: 3.7101\n",
      "Epoch [395/800], Step [24/53], D_loss: 0.2359, G_loss: 5.3564\n",
      "Epoch [395/800], Step [26/53], D_loss: 0.1222, G_loss: 6.6660\n",
      "Epoch [395/800], Step [28/53], D_loss: 0.0128, G_loss: 4.2238\n",
      "Epoch [395/800], Step [30/53], D_loss: 0.0451, G_loss: 4.2285\n",
      "Epoch [395/800], Step [32/53], D_loss: 0.0487, G_loss: 4.5868\n",
      "Epoch [395/800], Step [34/53], D_loss: 0.0212, G_loss: 3.5899\n",
      "Epoch [395/800], Step [36/53], D_loss: 0.1340, G_loss: 5.1802\n",
      "Epoch [395/800], Step [38/53], D_loss: 0.0213, G_loss: 6.4299\n",
      "Epoch [395/800], Step [40/53], D_loss: 0.0308, G_loss: 5.5268\n",
      "Epoch [395/800], Step [42/53], D_loss: 0.0262, G_loss: 5.6482\n",
      "Epoch [395/800], Step [44/53], D_loss: 0.0581, G_loss: 6.4275\n",
      "Epoch [395/800], Step [46/53], D_loss: 0.0045, G_loss: 5.2375\n",
      "Epoch [395/800], Step [48/53], D_loss: 0.4249, G_loss: 0.4832\n",
      "Epoch [395/800], Step [50/53], D_loss: 0.3344, G_loss: 4.7180\n",
      "Epoch [395/800], Step [52/53], D_loss: 0.0133, G_loss: 7.3038\n",
      "Epoch [395/800], Avg D_loss: 0.1277, Avg G_loss: 4.5582\n",
      "Epoch [396/800], Step [2/53], D_loss: 0.0596, G_loss: 5.6831\n",
      "Epoch [396/800], Step [4/53], D_loss: 0.0236, G_loss: 2.5405\n",
      "Epoch [396/800], Step [6/53], D_loss: 0.1235, G_loss: 3.9545\n",
      "Epoch [396/800], Step [8/53], D_loss: 0.0681, G_loss: 2.5220\n",
      "Epoch [396/800], Step [10/53], D_loss: 0.1310, G_loss: 2.9923\n",
      "Epoch [396/800], Step [12/53], D_loss: 0.0458, G_loss: 2.6260\n",
      "Epoch [396/800], Step [14/53], D_loss: 0.1218, G_loss: 4.4867\n",
      "Epoch [396/800], Step [16/53], D_loss: 0.0145, G_loss: 5.1115\n",
      "Epoch [396/800], Step [18/53], D_loss: 0.0780, G_loss: 6.1243\n",
      "Epoch [396/800], Step [20/53], D_loss: 0.0415, G_loss: 5.1635\n",
      "Epoch [396/800], Step [22/53], D_loss: 0.2376, G_loss: 3.6144\n",
      "Epoch [396/800], Step [24/53], D_loss: 0.0298, G_loss: 1.9380\n",
      "Epoch [396/800], Step [26/53], D_loss: 0.0355, G_loss: 6.5237\n",
      "Epoch [396/800], Step [28/53], D_loss: 0.0551, G_loss: 7.4763\n",
      "Epoch [396/800], Step [30/53], D_loss: 0.0629, G_loss: 5.7874\n",
      "Epoch [396/800], Step [32/53], D_loss: 0.1369, G_loss: 6.7086\n",
      "Epoch [396/800], Step [34/53], D_loss: 0.3401, G_loss: 2.5484\n",
      "Epoch [396/800], Step [36/53], D_loss: 0.0800, G_loss: 5.9831\n",
      "Epoch [396/800], Step [38/53], D_loss: 0.0907, G_loss: 5.2762\n",
      "Epoch [396/800], Step [40/53], D_loss: 0.0173, G_loss: 4.5897\n",
      "Epoch [396/800], Step [42/53], D_loss: 0.0251, G_loss: 4.0983\n",
      "Epoch [396/800], Step [44/53], D_loss: 0.1877, G_loss: 6.8545\n",
      "Epoch [396/800], Step [46/53], D_loss: 0.0279, G_loss: 3.8023\n",
      "Epoch [396/800], Step [48/53], D_loss: 0.0301, G_loss: 5.7282\n",
      "Epoch [396/800], Step [50/53], D_loss: 0.0420, G_loss: 6.0368\n",
      "Epoch [396/800], Step [52/53], D_loss: 0.1375, G_loss: 5.3565\n",
      "Epoch [396/800], Avg D_loss: 0.1107, Avg G_loss: 5.0099\n",
      "Epoch [397/800], Step [2/53], D_loss: 0.0213, G_loss: 7.5050\n",
      "Epoch [397/800], Step [4/53], D_loss: 0.0054, G_loss: 4.5102\n",
      "Epoch [397/800], Step [6/53], D_loss: 0.3108, G_loss: 3.5734\n",
      "Epoch [397/800], Step [8/53], D_loss: 0.1651, G_loss: 6.4388\n",
      "Epoch [397/800], Step [10/53], D_loss: 0.0317, G_loss: 8.2302\n",
      "Epoch [397/800], Step [12/53], D_loss: 0.0186, G_loss: 8.0110\n",
      "Epoch [397/800], Step [14/53], D_loss: 0.1254, G_loss: 3.9550\n",
      "Epoch [397/800], Step [16/53], D_loss: 0.0359, G_loss: 3.4756\n",
      "Epoch [397/800], Step [18/53], D_loss: 0.3248, G_loss: 8.6489\n",
      "Epoch [397/800], Step [20/53], D_loss: 0.0095, G_loss: 7.0698\n",
      "Epoch [397/800], Step [22/53], D_loss: 0.0823, G_loss: 4.4078\n",
      "Epoch [397/800], Step [24/53], D_loss: 0.0180, G_loss: 5.9698\n",
      "Epoch [397/800], Step [26/53], D_loss: 0.1028, G_loss: 9.7659\n",
      "Epoch [397/800], Step [28/53], D_loss: 0.2894, G_loss: 2.1761\n",
      "Epoch [397/800], Step [30/53], D_loss: 0.3626, G_loss: 6.8851\n",
      "Epoch [397/800], Step [32/53], D_loss: 0.1536, G_loss: 6.0479\n",
      "Epoch [397/800], Step [34/53], D_loss: 0.0409, G_loss: 7.0207\n",
      "Epoch [397/800], Step [36/53], D_loss: 0.0183, G_loss: 3.8233\n",
      "Epoch [397/800], Step [38/53], D_loss: 0.1454, G_loss: 7.4901\n",
      "Epoch [397/800], Step [40/53], D_loss: 0.0963, G_loss: 6.2013\n",
      "Epoch [397/800], Step [42/53], D_loss: 0.0602, G_loss: 8.6523\n",
      "Epoch [397/800], Step [44/53], D_loss: 0.0454, G_loss: 9.2820\n",
      "Epoch [397/800], Step [46/53], D_loss: 0.0052, G_loss: 1.9590\n",
      "Epoch [397/800], Step [48/53], D_loss: 0.3868, G_loss: 10.5556\n",
      "Epoch [397/800], Step [50/53], D_loss: 0.1357, G_loss: 2.0309\n",
      "Epoch [397/800], Step [52/53], D_loss: 0.3225, G_loss: 5.3409\n",
      "Epoch [397/800], Avg D_loss: 0.1464, Avg G_loss: 6.0176\n",
      "Epoch [398/800], Step [2/53], D_loss: 0.0251, G_loss: 3.4178\n",
      "Epoch [398/800], Step [4/53], D_loss: 0.0573, G_loss: 6.3205\n",
      "Epoch [398/800], Step [6/53], D_loss: 0.1653, G_loss: 4.3529\n",
      "Epoch [398/800], Step [8/53], D_loss: 0.1693, G_loss: 4.5402\n",
      "Epoch [398/800], Step [10/53], D_loss: 0.2791, G_loss: 5.2574\n",
      "Epoch [398/800], Step [12/53], D_loss: 0.0510, G_loss: 6.2053\n",
      "Epoch [398/800], Step [14/53], D_loss: 0.0264, G_loss: 7.4194\n",
      "Epoch [398/800], Step [16/53], D_loss: 0.0140, G_loss: 4.5673\n",
      "Epoch [398/800], Step [18/53], D_loss: 0.2299, G_loss: 6.1525\n",
      "Epoch [398/800], Step [20/53], D_loss: 0.2618, G_loss: 3.1430\n",
      "Epoch [398/800], Step [22/53], D_loss: 0.1968, G_loss: 5.6078\n",
      "Epoch [398/800], Step [24/53], D_loss: 0.0678, G_loss: 6.0816\n",
      "Epoch [398/800], Step [26/53], D_loss: 0.1854, G_loss: 5.1186\n",
      "Epoch [398/800], Step [28/53], D_loss: 0.2471, G_loss: 2.4931\n",
      "Epoch [398/800], Step [30/53], D_loss: 0.0453, G_loss: 3.3991\n",
      "Epoch [398/800], Step [32/53], D_loss: 0.0682, G_loss: 5.6436\n",
      "Epoch [398/800], Step [34/53], D_loss: 0.0035, G_loss: 7.0335\n",
      "Epoch [398/800], Step [36/53], D_loss: 0.0507, G_loss: 3.7769\n",
      "Epoch [398/800], Step [38/53], D_loss: 0.0242, G_loss: 7.7830\n",
      "Epoch [398/800], Step [40/53], D_loss: 0.0137, G_loss: 6.4500\n",
      "Epoch [398/800], Step [42/53], D_loss: 0.2153, G_loss: 6.3250\n",
      "Epoch [398/800], Step [44/53], D_loss: 0.7219, G_loss: 3.3819\n",
      "Epoch [398/800], Step [46/53], D_loss: 0.1242, G_loss: 8.4165\n",
      "Epoch [398/800], Step [48/53], D_loss: 0.0576, G_loss: 6.9387\n",
      "Epoch [398/800], Step [50/53], D_loss: 0.0217, G_loss: 2.3773\n",
      "Epoch [398/800], Step [52/53], D_loss: 0.4332, G_loss: 7.1666\n",
      "Epoch [398/800], Avg D_loss: 0.1289, Avg G_loss: 5.2296\n",
      "Epoch [399/800], Step [2/53], D_loss: 0.0491, G_loss: 4.4061\n",
      "Epoch [399/800], Step [4/53], D_loss: 0.0564, G_loss: 5.9313\n",
      "Epoch [399/800], Step [6/53], D_loss: 0.0970, G_loss: 4.5182\n",
      "Epoch [399/800], Step [8/53], D_loss: 0.0513, G_loss: 5.4007\n",
      "Epoch [399/800], Step [10/53], D_loss: 0.1055, G_loss: 6.7217\n",
      "Epoch [399/800], Step [12/53], D_loss: 0.0211, G_loss: 5.1587\n",
      "Epoch [399/800], Step [14/53], D_loss: 0.0832, G_loss: 8.1555\n",
      "Epoch [399/800], Step [16/53], D_loss: 0.0529, G_loss: 6.4253\n",
      "Epoch [399/800], Step [18/53], D_loss: 0.0415, G_loss: 4.8605\n",
      "Epoch [399/800], Step [20/53], D_loss: 0.0352, G_loss: 3.4714\n",
      "Epoch [399/800], Step [22/53], D_loss: 0.0130, G_loss: 4.8024\n",
      "Epoch [399/800], Step [24/53], D_loss: 0.0079, G_loss: 7.3123\n",
      "Epoch [399/800], Step [26/53], D_loss: 0.0463, G_loss: 1.3151\n",
      "Epoch [399/800], Step [28/53], D_loss: 0.0380, G_loss: 0.4026\n",
      "Epoch [399/800], Step [30/53], D_loss: 0.2220, G_loss: 7.5119\n",
      "Epoch [399/800], Step [32/53], D_loss: 0.0021, G_loss: 4.3932\n",
      "Epoch [399/800], Step [34/53], D_loss: 0.0743, G_loss: 6.2285\n",
      "Epoch [399/800], Step [36/53], D_loss: 0.0786, G_loss: 4.9849\n",
      "Epoch [399/800], Step [38/53], D_loss: 0.0415, G_loss: 4.9490\n",
      "Epoch [399/800], Step [40/53], D_loss: 0.0120, G_loss: 10.8990\n",
      "Epoch [399/800], Step [42/53], D_loss: 0.0615, G_loss: 7.8824\n",
      "Epoch [399/800], Step [44/53], D_loss: 0.0105, G_loss: 5.7167\n",
      "Epoch [399/800], Step [46/53], D_loss: 0.6334, G_loss: 9.7371\n",
      "Epoch [399/800], Step [48/53], D_loss: 0.0062, G_loss: 2.4503\n",
      "Epoch [399/800], Step [50/53], D_loss: 0.2106, G_loss: 6.1335\n",
      "Epoch [399/800], Step [52/53], D_loss: 0.0049, G_loss: 6.8910\n",
      "Epoch [399/800], Avg D_loss: 0.0863, Avg G_loss: 5.9031\n",
      "Epoch [400/800], Step [2/53], D_loss: 0.0101, G_loss: 8.2356\n",
      "Epoch [400/800], Step [4/53], D_loss: 0.0462, G_loss: 4.4843\n",
      "Epoch [400/800], Step [6/53], D_loss: 0.0130, G_loss: 7.3481\n",
      "Epoch [400/800], Step [8/53], D_loss: 0.0041, G_loss: 6.8281\n",
      "Epoch [400/800], Step [10/53], D_loss: 0.0905, G_loss: 7.3607\n",
      "Epoch [400/800], Step [12/53], D_loss: 0.0857, G_loss: 3.2911\n",
      "Epoch [400/800], Step [14/53], D_loss: 0.0268, G_loss: 8.5694\n",
      "Epoch [400/800], Step [16/53], D_loss: 0.1299, G_loss: 7.4926\n",
      "Epoch [400/800], Step [18/53], D_loss: 0.2584, G_loss: 7.0878\n",
      "Epoch [400/800], Step [20/53], D_loss: 0.1482, G_loss: 7.5254\n",
      "Epoch [400/800], Step [22/53], D_loss: 0.0038, G_loss: 3.5446\n",
      "Epoch [400/800], Step [24/53], D_loss: 0.2267, G_loss: 8.2836\n",
      "Epoch [400/800], Step [26/53], D_loss: 0.0120, G_loss: 8.6587\n",
      "Epoch [400/800], Step [28/53], D_loss: 0.0055, G_loss: 4.8134\n",
      "Epoch [400/800], Step [30/53], D_loss: 0.1495, G_loss: 4.8690\n",
      "Epoch [400/800], Step [32/53], D_loss: 0.1293, G_loss: 10.0232\n",
      "Epoch [400/800], Step [34/53], D_loss: 0.0621, G_loss: 7.1938\n",
      "Epoch [400/800], Step [36/53], D_loss: 0.0090, G_loss: 6.1899\n",
      "Epoch [400/800], Step [38/53], D_loss: 0.0500, G_loss: 6.8823\n",
      "Epoch [400/800], Step [40/53], D_loss: 0.2233, G_loss: 3.7108\n",
      "Epoch [400/800], Step [42/53], D_loss: 0.3039, G_loss: 10.8479\n",
      "Epoch [400/800], Step [44/53], D_loss: 0.1010, G_loss: 6.8286\n",
      "Epoch [400/800], Step [46/53], D_loss: 0.0370, G_loss: 6.3551\n",
      "Epoch [400/800], Step [48/53], D_loss: 0.0025, G_loss: 4.9710\n",
      "Epoch [400/800], Step [50/53], D_loss: 0.0166, G_loss: 7.5068\n",
      "Epoch [400/800], Step [52/53], D_loss: 0.1240, G_loss: 6.9491\n",
      "Epoch [400/800], Avg D_loss: 0.0766, Avg G_loss: 6.6461\n",
      "Generated images saved as generated_images/epoch_400.png\n",
      "Models saved for epoch 400\n",
      "Epoch [401/800], Step [2/53], D_loss: 0.0020, G_loss: 8.1352\n",
      "Epoch [401/800], Step [4/53], D_loss: 0.0370, G_loss: 7.2920\n",
      "Epoch [401/800], Step [6/53], D_loss: 0.0296, G_loss: 8.8869\n",
      "Epoch [401/800], Step [8/53], D_loss: 0.0023, G_loss: 6.0185\n",
      "Epoch [401/800], Step [10/53], D_loss: 0.0050, G_loss: 8.1699\n",
      "Epoch [401/800], Step [12/53], D_loss: 0.0029, G_loss: 8.6412\n",
      "Epoch [401/800], Step [14/53], D_loss: 0.0573, G_loss: 4.0522\n",
      "Epoch [401/800], Step [16/53], D_loss: 0.0068, G_loss: 2.2081\n",
      "Epoch [401/800], Step [18/53], D_loss: 0.0027, G_loss: 3.7874\n",
      "Epoch [401/800], Step [20/53], D_loss: 0.0256, G_loss: 4.1438\n",
      "Epoch [401/800], Step [22/53], D_loss: 0.0139, G_loss: 7.8387\n",
      "Epoch [401/800], Step [24/53], D_loss: 0.0018, G_loss: 7.4612\n",
      "Epoch [401/800], Step [26/53], D_loss: 0.0012, G_loss: 9.0127\n",
      "Epoch [401/800], Step [28/53], D_loss: 0.0081, G_loss: 6.3325\n",
      "Epoch [401/800], Step [30/53], D_loss: 0.0116, G_loss: 7.1390\n",
      "Epoch [401/800], Step [32/53], D_loss: 0.0131, G_loss: 10.2644\n",
      "Epoch [401/800], Step [34/53], D_loss: 0.0309, G_loss: 7.2150\n",
      "Epoch [401/800], Step [36/53], D_loss: 0.0227, G_loss: 6.6124\n",
      "Epoch [401/800], Step [38/53], D_loss: 0.2105, G_loss: 9.2192\n",
      "Epoch [401/800], Step [40/53], D_loss: 0.0074, G_loss: 6.2848\n",
      "Epoch [401/800], Step [42/53], D_loss: 0.0048, G_loss: 11.2131\n",
      "Epoch [401/800], Step [44/53], D_loss: 0.0083, G_loss: 8.2911\n",
      "Epoch [401/800], Step [46/53], D_loss: 0.2373, G_loss: 9.2243\n",
      "Epoch [401/800], Step [48/53], D_loss: 0.4267, G_loss: 3.4793\n",
      "Epoch [401/800], Step [50/53], D_loss: 0.0007, G_loss: 3.3343\n",
      "Epoch [401/800], Step [52/53], D_loss: 0.0115, G_loss: 4.8135\n",
      "Epoch [401/800], Avg D_loss: 0.0606, Avg G_loss: 7.6142\n",
      "Epoch [402/800], Step [2/53], D_loss: 0.2042, G_loss: 5.2151\n",
      "Epoch [402/800], Step [4/53], D_loss: 0.2787, G_loss: 1.8996\n",
      "Epoch [402/800], Step [6/53], D_loss: 0.1058, G_loss: 6.1283\n",
      "Epoch [402/800], Step [8/53], D_loss: 0.1615, G_loss: 3.1764\n",
      "Epoch [402/800], Step [10/53], D_loss: 0.0425, G_loss: 2.4245\n",
      "Epoch [402/800], Step [12/53], D_loss: 0.0328, G_loss: 3.6344\n",
      "Epoch [402/800], Step [14/53], D_loss: 0.0341, G_loss: 1.8754\n",
      "Epoch [402/800], Step [16/53], D_loss: 0.0534, G_loss: 2.7287\n",
      "Epoch [402/800], Step [18/53], D_loss: 0.0396, G_loss: 4.9937\n",
      "Epoch [402/800], Step [20/53], D_loss: 0.0614, G_loss: 5.4841\n",
      "Epoch [402/800], Step [22/53], D_loss: 0.0405, G_loss: 2.7893\n",
      "Epoch [402/800], Step [24/53], D_loss: 0.0120, G_loss: 2.4661\n",
      "Epoch [402/800], Step [26/53], D_loss: 0.0955, G_loss: 5.6118\n",
      "Epoch [402/800], Step [28/53], D_loss: 0.0453, G_loss: 4.5682\n",
      "Epoch [402/800], Step [30/53], D_loss: 0.0740, G_loss: 5.4333\n",
      "Epoch [402/800], Step [32/53], D_loss: 0.0943, G_loss: 5.9664\n",
      "Epoch [402/800], Step [34/53], D_loss: 0.0470, G_loss: 5.1231\n",
      "Epoch [402/800], Step [36/53], D_loss: 0.0859, G_loss: 5.6204\n",
      "Epoch [402/800], Step [38/53], D_loss: 0.0802, G_loss: 5.3235\n",
      "Epoch [402/800], Step [40/53], D_loss: 0.1591, G_loss: 5.8802\n",
      "Epoch [402/800], Step [42/53], D_loss: 0.1047, G_loss: 5.5479\n",
      "Epoch [402/800], Step [44/53], D_loss: 0.0283, G_loss: 6.4516\n",
      "Epoch [402/800], Step [46/53], D_loss: 0.0484, G_loss: 5.1176\n",
      "Epoch [402/800], Step [48/53], D_loss: 0.0463, G_loss: 3.6649\n",
      "Epoch [402/800], Step [50/53], D_loss: 0.0056, G_loss: 6.5987\n",
      "Epoch [402/800], Step [52/53], D_loss: 0.0823, G_loss: 6.9369\n",
      "Epoch [402/800], Avg D_loss: 0.1387, Avg G_loss: 4.6058\n",
      "Epoch [403/800], Step [2/53], D_loss: 0.0269, G_loss: 5.2915\n",
      "Epoch [403/800], Step [4/53], D_loss: 0.0327, G_loss: 5.0201\n",
      "Epoch [403/800], Step [6/53], D_loss: 0.0363, G_loss: 5.1459\n",
      "Epoch [403/800], Step [8/53], D_loss: 0.1250, G_loss: 7.8279\n",
      "Epoch [403/800], Step [10/53], D_loss: 0.0050, G_loss: 5.3131\n",
      "Epoch [403/800], Step [12/53], D_loss: 0.0054, G_loss: 7.4079\n",
      "Epoch [403/800], Step [14/53], D_loss: 0.0021, G_loss: 4.8950\n",
      "Epoch [403/800], Step [16/53], D_loss: 0.0273, G_loss: 8.4544\n",
      "Epoch [403/800], Step [18/53], D_loss: 0.0241, G_loss: 7.7897\n",
      "Epoch [403/800], Step [20/53], D_loss: 0.0032, G_loss: 6.8992\n",
      "Epoch [403/800], Step [22/53], D_loss: 0.0069, G_loss: 7.4921\n",
      "Epoch [403/800], Step [24/53], D_loss: 0.0325, G_loss: 7.6026\n",
      "Epoch [403/800], Step [26/53], D_loss: 0.0371, G_loss: 5.1394\n",
      "Epoch [403/800], Step [28/53], D_loss: 0.0153, G_loss: 8.0252\n",
      "Epoch [403/800], Step [30/53], D_loss: 0.0854, G_loss: 7.5183\n",
      "Epoch [403/800], Step [32/53], D_loss: 0.0030, G_loss: 7.3134\n",
      "Epoch [403/800], Step [34/53], D_loss: 0.0045, G_loss: 8.9236\n",
      "Epoch [403/800], Step [36/53], D_loss: 0.0219, G_loss: 5.7999\n",
      "Epoch [403/800], Step [38/53], D_loss: 0.0214, G_loss: 6.1154\n",
      "Epoch [403/800], Step [40/53], D_loss: 0.0026, G_loss: 8.4689\n",
      "Epoch [403/800], Step [42/53], D_loss: 0.0330, G_loss: 11.4506\n",
      "Epoch [403/800], Step [44/53], D_loss: 0.0760, G_loss: 5.3759\n",
      "Epoch [403/800], Step [46/53], D_loss: 0.0302, G_loss: 7.1586\n",
      "Epoch [403/800], Step [48/53], D_loss: 0.0218, G_loss: 5.2005\n",
      "Epoch [403/800], Step [50/53], D_loss: 0.0289, G_loss: 6.3824\n",
      "Epoch [403/800], Step [52/53], D_loss: 0.0053, G_loss: 5.5203\n",
      "Epoch [403/800], Avg D_loss: 0.0454, Avg G_loss: 6.8198\n",
      "Epoch [404/800], Step [2/53], D_loss: 0.0765, G_loss: 6.2791\n",
      "Epoch [404/800], Step [4/53], D_loss: 0.0080, G_loss: 11.4280\n",
      "Epoch [404/800], Step [6/53], D_loss: 0.0363, G_loss: 6.1224\n",
      "Epoch [404/800], Step [8/53], D_loss: 0.0171, G_loss: 8.8125\n",
      "Epoch [404/800], Step [10/53], D_loss: 0.0010, G_loss: 5.1165\n",
      "Epoch [404/800], Step [12/53], D_loss: 0.0026, G_loss: 6.4844\n",
      "Epoch [404/800], Step [14/53], D_loss: 0.0079, G_loss: 7.5229\n",
      "Epoch [404/800], Step [16/53], D_loss: 0.0074, G_loss: 7.2260\n",
      "Epoch [404/800], Step [18/53], D_loss: 0.0025, G_loss: 9.2428\n",
      "Epoch [404/800], Step [20/53], D_loss: 0.0015, G_loss: 7.1846\n",
      "Epoch [404/800], Step [22/53], D_loss: 0.0043, G_loss: 6.4660\n",
      "Epoch [404/800], Step [24/53], D_loss: 0.0031, G_loss: 5.3715\n",
      "Epoch [404/800], Step [26/53], D_loss: 0.0419, G_loss: 12.4447\n",
      "Epoch [404/800], Step [28/53], D_loss: 0.0055, G_loss: 4.8044\n",
      "Epoch [404/800], Step [30/53], D_loss: 0.1913, G_loss: 7.6078\n",
      "Epoch [404/800], Step [32/53], D_loss: 0.0833, G_loss: 10.8956\n",
      "Epoch [404/800], Step [34/53], D_loss: 0.0018, G_loss: 6.1391\n",
      "Epoch [404/800], Step [36/53], D_loss: 0.0031, G_loss: 8.8356\n",
      "Epoch [404/800], Step [38/53], D_loss: 0.0258, G_loss: 5.8367\n",
      "Epoch [404/800], Step [40/53], D_loss: 0.0010, G_loss: 6.9505\n",
      "Epoch [404/800], Step [42/53], D_loss: 0.0462, G_loss: 6.4151\n",
      "Epoch [404/800], Step [44/53], D_loss: 0.0039, G_loss: 6.0340\n",
      "Epoch [404/800], Step [46/53], D_loss: 0.0437, G_loss: 6.5353\n",
      "Epoch [404/800], Step [48/53], D_loss: 0.0199, G_loss: 7.5868\n",
      "Epoch [404/800], Step [50/53], D_loss: 0.0125, G_loss: 7.2770\n",
      "Epoch [404/800], Step [52/53], D_loss: 0.0076, G_loss: 6.8363\n",
      "Epoch [404/800], Avg D_loss: 0.0333, Avg G_loss: 7.4296\n",
      "Epoch [405/800], Step [2/53], D_loss: 0.2216, G_loss: 8.3197\n",
      "Epoch [405/800], Step [4/53], D_loss: 0.0007, G_loss: 8.1538\n",
      "Epoch [405/800], Step [6/53], D_loss: 0.1863, G_loss: 6.0542\n",
      "Epoch [405/800], Step [8/53], D_loss: 0.0471, G_loss: 9.1607\n",
      "Epoch [405/800], Step [10/53], D_loss: 0.0443, G_loss: 9.3393\n",
      "Epoch [405/800], Step [12/53], D_loss: 0.0246, G_loss: 6.2295\n",
      "Epoch [405/800], Step [14/53], D_loss: 0.0422, G_loss: 6.3635\n",
      "Epoch [405/800], Step [16/53], D_loss: 0.1287, G_loss: 8.6491\n",
      "Epoch [405/800], Step [18/53], D_loss: 0.0011, G_loss: 10.0548\n",
      "Epoch [405/800], Step [20/53], D_loss: 0.4400, G_loss: 9.6809\n",
      "Epoch [405/800], Step [22/53], D_loss: 0.0083, G_loss: 9.5723\n",
      "Epoch [405/800], Step [24/53], D_loss: 0.0260, G_loss: 5.2777\n",
      "Epoch [405/800], Step [26/53], D_loss: 0.0029, G_loss: 2.9402\n",
      "Epoch [405/800], Step [28/53], D_loss: 0.1389, G_loss: 5.0197\n",
      "Epoch [405/800], Step [30/53], D_loss: 0.0143, G_loss: 9.7703\n",
      "Epoch [405/800], Step [32/53], D_loss: 0.0047, G_loss: 7.4614\n",
      "Epoch [405/800], Step [34/53], D_loss: 0.0039, G_loss: 8.3550\n",
      "Epoch [405/800], Step [36/53], D_loss: 0.0048, G_loss: 9.0643\n",
      "Epoch [405/800], Step [38/53], D_loss: 0.0634, G_loss: 5.0865\n",
      "Epoch [405/800], Step [40/53], D_loss: 0.0024, G_loss: 6.5371\n",
      "Epoch [405/800], Step [42/53], D_loss: 0.0107, G_loss: 5.1028\n",
      "Epoch [405/800], Step [44/53], D_loss: 0.0410, G_loss: 6.6545\n",
      "Epoch [405/800], Step [46/53], D_loss: 0.0020, G_loss: 5.6453\n",
      "Epoch [405/800], Step [48/53], D_loss: 0.0150, G_loss: 5.3168\n",
      "Epoch [405/800], Step [50/53], D_loss: 0.1273, G_loss: 2.6351\n",
      "Epoch [405/800], Step [52/53], D_loss: 0.0271, G_loss: 8.9386\n",
      "Epoch [405/800], Avg D_loss: 0.0676, Avg G_loss: 7.3661\n",
      "Epoch [406/800], Step [2/53], D_loss: 0.0143, G_loss: 3.9116\n",
      "Epoch [406/800], Step [4/53], D_loss: 0.0341, G_loss: 13.1835\n",
      "Epoch [406/800], Step [6/53], D_loss: 0.7276, G_loss: 0.8927\n",
      "Epoch [406/800], Step [8/53], D_loss: 0.0512, G_loss: 6.6667\n",
      "Epoch [406/800], Step [10/53], D_loss: 0.0165, G_loss: 5.7430\n",
      "Epoch [406/800], Step [12/53], D_loss: 0.1024, G_loss: 7.1935\n",
      "Epoch [406/800], Step [14/53], D_loss: 0.0508, G_loss: 6.5847\n",
      "Epoch [406/800], Step [16/53], D_loss: 0.0355, G_loss: 0.7709\n",
      "Epoch [406/800], Step [18/53], D_loss: 0.0117, G_loss: 8.8119\n",
      "Epoch [406/800], Step [20/53], D_loss: 0.0162, G_loss: 2.1488\n",
      "Epoch [406/800], Step [22/53], D_loss: 0.0177, G_loss: 7.1001\n",
      "Epoch [406/800], Step [24/53], D_loss: 0.0400, G_loss: 4.2486\n",
      "Epoch [406/800], Step [26/53], D_loss: 0.0176, G_loss: 4.6869\n",
      "Epoch [406/800], Step [28/53], D_loss: 0.0371, G_loss: 5.8724\n",
      "Epoch [406/800], Step [30/53], D_loss: 0.0048, G_loss: 3.4758\n",
      "Epoch [406/800], Step [32/53], D_loss: 0.1101, G_loss: 5.8085\n",
      "Epoch [406/800], Step [34/53], D_loss: 0.0069, G_loss: 5.3682\n",
      "Epoch [406/800], Step [36/53], D_loss: 0.0103, G_loss: 3.5410\n",
      "Epoch [406/800], Step [38/53], D_loss: 0.0066, G_loss: 2.6424\n",
      "Epoch [406/800], Step [40/53], D_loss: 0.0745, G_loss: 5.0145\n",
      "Epoch [406/800], Step [42/53], D_loss: 0.0106, G_loss: 8.5089\n",
      "Epoch [406/800], Step [44/53], D_loss: 0.1262, G_loss: 4.4434\n",
      "Epoch [406/800], Step [46/53], D_loss: 0.0534, G_loss: 1.1740\n",
      "Epoch [406/800], Step [48/53], D_loss: 0.0203, G_loss: 7.1899\n",
      "Epoch [406/800], Step [50/53], D_loss: 0.0061, G_loss: 6.1277\n",
      "Epoch [406/800], Step [52/53], D_loss: 0.0261, G_loss: 5.9742\n",
      "Epoch [406/800], Avg D_loss: 0.1079, Avg G_loss: 5.6091\n",
      "Epoch [407/800], Step [2/53], D_loss: 0.0372, G_loss: 6.8843\n",
      "Epoch [407/800], Step [4/53], D_loss: 0.0009, G_loss: 6.2476\n",
      "Epoch [407/800], Step [6/53], D_loss: 0.1142, G_loss: 7.7476\n",
      "Epoch [407/800], Step [8/53], D_loss: 0.0988, G_loss: 5.3214\n",
      "Epoch [407/800], Step [10/53], D_loss: 0.0498, G_loss: 6.0457\n",
      "Epoch [407/800], Step [12/53], D_loss: 0.0235, G_loss: 6.8775\n",
      "Epoch [407/800], Step [14/53], D_loss: 0.0370, G_loss: 5.2874\n",
      "Epoch [407/800], Step [16/53], D_loss: 0.0337, G_loss: 5.0109\n",
      "Epoch [407/800], Step [18/53], D_loss: 0.0250, G_loss: 5.3718\n",
      "Epoch [407/800], Step [20/53], D_loss: 0.7364, G_loss: 0.4636\n",
      "Epoch [407/800], Step [22/53], D_loss: 0.0125, G_loss: 9.6594\n",
      "Epoch [407/800], Step [24/53], D_loss: 0.0192, G_loss: 4.6154\n",
      "Epoch [407/800], Step [26/53], D_loss: 0.3477, G_loss: 6.9837\n",
      "Epoch [407/800], Step [28/53], D_loss: 0.0995, G_loss: 4.6864\n",
      "Epoch [407/800], Step [30/53], D_loss: 0.0445, G_loss: 4.2846\n",
      "Epoch [407/800], Step [32/53], D_loss: 0.0884, G_loss: 4.1584\n",
      "Epoch [407/800], Step [34/53], D_loss: 0.0138, G_loss: 5.5920\n",
      "Epoch [407/800], Step [36/53], D_loss: 0.1275, G_loss: 5.1461\n",
      "Epoch [407/800], Step [38/53], D_loss: 0.0329, G_loss: 6.3020\n",
      "Epoch [407/800], Step [40/53], D_loss: 0.0205, G_loss: 6.5768\n",
      "Epoch [407/800], Step [42/53], D_loss: 0.0843, G_loss: 6.4448\n",
      "Epoch [407/800], Step [44/53], D_loss: 0.0642, G_loss: 2.3316\n",
      "Epoch [407/800], Step [46/53], D_loss: 0.0961, G_loss: 8.0937\n",
      "Epoch [407/800], Step [48/53], D_loss: 0.1494, G_loss: 2.2514\n",
      "Epoch [407/800], Step [50/53], D_loss: 0.0248, G_loss: 2.6321\n",
      "Epoch [407/800], Step [52/53], D_loss: 0.0396, G_loss: 4.9629\n",
      "Epoch [407/800], Avg D_loss: 0.1294, Avg G_loss: 5.5593\n",
      "Epoch [408/800], Step [2/53], D_loss: 0.6648, G_loss: 7.0740\n",
      "Epoch [408/800], Step [4/53], D_loss: 0.0401, G_loss: 5.1451\n",
      "Epoch [408/800], Step [6/53], D_loss: 0.1152, G_loss: 2.6462\n",
      "Epoch [408/800], Step [8/53], D_loss: 0.1002, G_loss: 5.5199\n",
      "Epoch [408/800], Step [10/53], D_loss: 0.0290, G_loss: 4.3902\n",
      "Epoch [408/800], Step [12/53], D_loss: 0.0083, G_loss: 3.9991\n",
      "Epoch [408/800], Step [14/53], D_loss: 0.0696, G_loss: 2.3599\n",
      "Epoch [408/800], Step [16/53], D_loss: 0.0094, G_loss: 7.1647\n",
      "Epoch [408/800], Step [18/53], D_loss: 0.0606, G_loss: 4.3666\n",
      "Epoch [408/800], Step [20/53], D_loss: 0.1339, G_loss: 4.9768\n",
      "Epoch [408/800], Step [22/53], D_loss: 0.0258, G_loss: 6.1777\n",
      "Epoch [408/800], Step [24/53], D_loss: 0.0419, G_loss: 7.1283\n",
      "Epoch [408/800], Step [26/53], D_loss: 0.0664, G_loss: 6.1250\n",
      "Epoch [408/800], Step [28/53], D_loss: 0.0220, G_loss: 3.5891\n",
      "Epoch [408/800], Step [30/53], D_loss: 1.2178, G_loss: 7.2185\n",
      "Epoch [408/800], Step [32/53], D_loss: 0.0925, G_loss: 3.8752\n",
      "Epoch [408/800], Step [34/53], D_loss: 0.0980, G_loss: 3.6848\n",
      "Epoch [408/800], Step [36/53], D_loss: 0.0519, G_loss: 5.6745\n",
      "Epoch [408/800], Step [38/53], D_loss: 0.1684, G_loss: 3.7795\n",
      "Epoch [408/800], Step [40/53], D_loss: 0.2331, G_loss: 5.4992\n",
      "Epoch [408/800], Step [42/53], D_loss: 0.0136, G_loss: 3.3012\n",
      "Epoch [408/800], Step [44/53], D_loss: 0.0255, G_loss: 1.6924\n",
      "Epoch [408/800], Step [46/53], D_loss: 0.0626, G_loss: 6.8593\n",
      "Epoch [408/800], Step [48/53], D_loss: 0.1663, G_loss: 4.9889\n",
      "Epoch [408/800], Step [50/53], D_loss: 0.1862, G_loss: 6.7808\n",
      "Epoch [408/800], Step [52/53], D_loss: 0.3057, G_loss: 7.2487\n",
      "Epoch [408/800], Avg D_loss: 0.1367, Avg G_loss: 5.0582\n",
      "Epoch [409/800], Step [2/53], D_loss: 0.0056, G_loss: 4.0224\n",
      "Epoch [409/800], Step [4/53], D_loss: 0.0571, G_loss: 5.1794\n",
      "Epoch [409/800], Step [6/53], D_loss: 0.1630, G_loss: 3.1978\n",
      "Epoch [409/800], Step [8/53], D_loss: 0.1100, G_loss: 3.6456\n",
      "Epoch [409/800], Step [10/53], D_loss: 0.0046, G_loss: 7.8975\n",
      "Epoch [409/800], Step [12/53], D_loss: 0.0176, G_loss: 5.7209\n",
      "Epoch [409/800], Step [14/53], D_loss: 0.0092, G_loss: 4.5822\n",
      "Epoch [409/800], Step [16/53], D_loss: 0.0502, G_loss: 8.5585\n",
      "Epoch [409/800], Step [18/53], D_loss: 0.0783, G_loss: 2.8571\n",
      "Epoch [409/800], Step [20/53], D_loss: 0.1311, G_loss: 4.9152\n",
      "Epoch [409/800], Step [22/53], D_loss: 0.0064, G_loss: 8.1347\n",
      "Epoch [409/800], Step [24/53], D_loss: 0.1483, G_loss: 3.5932\n",
      "Epoch [409/800], Step [26/53], D_loss: 0.0545, G_loss: 3.9887\n",
      "Epoch [409/800], Step [28/53], D_loss: 0.0215, G_loss: 6.4094\n",
      "Epoch [409/800], Step [30/53], D_loss: 0.1700, G_loss: 7.4612\n",
      "Epoch [409/800], Step [32/53], D_loss: 0.0053, G_loss: 8.2887\n",
      "Epoch [409/800], Step [34/53], D_loss: 0.0269, G_loss: 6.2702\n",
      "Epoch [409/800], Step [36/53], D_loss: 0.0308, G_loss: 4.0728\n",
      "Epoch [409/800], Step [38/53], D_loss: 0.0096, G_loss: 3.1649\n",
      "Epoch [409/800], Step [40/53], D_loss: 0.1804, G_loss: 3.1043\n",
      "Epoch [409/800], Step [42/53], D_loss: 0.0122, G_loss: 4.6115\n",
      "Epoch [409/800], Step [44/53], D_loss: 0.1040, G_loss: 4.4451\n",
      "Epoch [409/800], Step [46/53], D_loss: 0.0454, G_loss: 5.7738\n",
      "Epoch [409/800], Step [48/53], D_loss: 0.0221, G_loss: 5.4073\n",
      "Epoch [409/800], Step [50/53], D_loss: 0.1112, G_loss: 5.4824\n",
      "Epoch [409/800], Step [52/53], D_loss: 0.2880, G_loss: 8.1443\n",
      "Epoch [409/800], Avg D_loss: 0.0761, Avg G_loss: 5.6883\n",
      "Epoch [410/800], Step [2/53], D_loss: 0.1109, G_loss: 7.0030\n",
      "Epoch [410/800], Step [4/53], D_loss: 0.1942, G_loss: 6.2472\n",
      "Epoch [410/800], Step [6/53], D_loss: 0.0676, G_loss: 5.3722\n",
      "Epoch [410/800], Step [8/53], D_loss: 0.0565, G_loss: 7.3839\n",
      "Epoch [410/800], Step [10/53], D_loss: 0.1256, G_loss: 4.4784\n",
      "Epoch [410/800], Step [12/53], D_loss: 0.3306, G_loss: 7.9946\n",
      "Epoch [410/800], Step [14/53], D_loss: 0.0076, G_loss: 3.1429\n",
      "Epoch [410/800], Step [16/53], D_loss: 0.0454, G_loss: 4.4968\n",
      "Epoch [410/800], Step [18/53], D_loss: 0.0315, G_loss: 9.6261\n",
      "Epoch [410/800], Step [20/53], D_loss: 0.3337, G_loss: 6.5498\n",
      "Epoch [410/800], Step [22/53], D_loss: 0.0159, G_loss: 6.7858\n",
      "Epoch [410/800], Step [24/53], D_loss: 0.0180, G_loss: 7.4611\n",
      "Epoch [410/800], Step [26/53], D_loss: 0.0316, G_loss: 8.0452\n",
      "Epoch [410/800], Step [28/53], D_loss: 0.0094, G_loss: 6.1239\n",
      "Epoch [410/800], Step [30/53], D_loss: 0.0874, G_loss: 2.9276\n",
      "Epoch [410/800], Step [32/53], D_loss: 0.1740, G_loss: 7.1946\n",
      "Epoch [410/800], Step [34/53], D_loss: 0.0173, G_loss: 3.4457\n",
      "Epoch [410/800], Step [36/53], D_loss: 0.4164, G_loss: 8.2222\n",
      "Epoch [410/800], Step [38/53], D_loss: 0.0231, G_loss: 8.0047\n",
      "Epoch [410/800], Step [40/53], D_loss: 0.0044, G_loss: 3.4102\n",
      "Epoch [410/800], Step [42/53], D_loss: 0.0049, G_loss: 4.5330\n",
      "Epoch [410/800], Step [44/53], D_loss: 0.0178, G_loss: 2.3703\n",
      "Epoch [410/800], Step [46/53], D_loss: 0.0398, G_loss: 7.4771\n",
      "Epoch [410/800], Step [48/53], D_loss: 0.0351, G_loss: 5.9960\n",
      "Epoch [410/800], Step [50/53], D_loss: 0.0860, G_loss: 4.2378\n",
      "Epoch [410/800], Step [52/53], D_loss: 0.0029, G_loss: 6.7667\n",
      "Epoch [410/800], Avg D_loss: 0.0890, Avg G_loss: 6.0801\n",
      "Epoch [411/800], Step [2/53], D_loss: 0.0408, G_loss: 9.0378\n",
      "Epoch [411/800], Step [4/53], D_loss: 0.0121, G_loss: 4.1379\n",
      "Epoch [411/800], Step [6/53], D_loss: 0.0368, G_loss: 5.4119\n",
      "Epoch [411/800], Step [8/53], D_loss: 0.0989, G_loss: 4.8293\n",
      "Epoch [411/800], Step [10/53], D_loss: 0.0350, G_loss: 7.0825\n",
      "Epoch [411/800], Step [12/53], D_loss: 0.0835, G_loss: 3.7664\n",
      "Epoch [411/800], Step [14/53], D_loss: 0.0033, G_loss: 5.2678\n",
      "Epoch [411/800], Step [16/53], D_loss: 0.0073, G_loss: 6.0283\n",
      "Epoch [411/800], Step [18/53], D_loss: 0.0569, G_loss: 8.5144\n",
      "Epoch [411/800], Step [20/53], D_loss: 0.0759, G_loss: 4.2537\n",
      "Epoch [411/800], Step [22/53], D_loss: 0.0026, G_loss: 4.1590\n",
      "Epoch [411/800], Step [24/53], D_loss: 0.0218, G_loss: 6.7177\n",
      "Epoch [411/800], Step [26/53], D_loss: 0.0061, G_loss: 11.2630\n",
      "Epoch [411/800], Step [28/53], D_loss: 0.0564, G_loss: 5.5513\n",
      "Epoch [411/800], Step [30/53], D_loss: 0.1284, G_loss: 3.5976\n",
      "Epoch [411/800], Step [32/53], D_loss: 0.0134, G_loss: 6.1947\n",
      "Epoch [411/800], Step [34/53], D_loss: 0.0720, G_loss: 9.9437\n",
      "Epoch [411/800], Step [36/53], D_loss: 0.0033, G_loss: 9.3501\n",
      "Epoch [411/800], Step [38/53], D_loss: 0.0045, G_loss: 6.8543\n",
      "Epoch [411/800], Step [40/53], D_loss: 0.0111, G_loss: 7.0886\n",
      "Epoch [411/800], Step [42/53], D_loss: 0.0824, G_loss: 10.4133\n",
      "Epoch [411/800], Step [44/53], D_loss: 0.1105, G_loss: 9.0276\n",
      "Epoch [411/800], Step [46/53], D_loss: 0.0058, G_loss: 6.9298\n",
      "Epoch [411/800], Step [48/53], D_loss: 0.0522, G_loss: 8.3850\n",
      "Epoch [411/800], Step [50/53], D_loss: 0.0140, G_loss: 6.3682\n",
      "Epoch [411/800], Step [52/53], D_loss: 0.0041, G_loss: 4.5225\n",
      "Epoch [411/800], Avg D_loss: 0.0423, Avg G_loss: 7.1484\n",
      "Epoch [412/800], Step [2/53], D_loss: 0.4350, G_loss: 4.6809\n",
      "Epoch [412/800], Step [4/53], D_loss: 0.0565, G_loss: 8.2190\n",
      "Epoch [412/800], Step [6/53], D_loss: 0.0035, G_loss: 8.2355\n",
      "Epoch [412/800], Step [8/53], D_loss: 0.0199, G_loss: 6.7096\n",
      "Epoch [412/800], Step [10/53], D_loss: 0.0034, G_loss: 6.1716\n",
      "Epoch [412/800], Step [12/53], D_loss: 0.0211, G_loss: 5.7500\n",
      "Epoch [412/800], Step [14/53], D_loss: 0.0566, G_loss: 8.4130\n",
      "Epoch [412/800], Step [16/53], D_loss: 0.0101, G_loss: 7.8172\n",
      "Epoch [412/800], Step [18/53], D_loss: 0.1564, G_loss: 8.6655\n",
      "Epoch [412/800], Step [20/53], D_loss: 0.2711, G_loss: 12.0309\n",
      "Epoch [412/800], Step [22/53], D_loss: 0.3775, G_loss: 2.9796\n",
      "Epoch [412/800], Step [24/53], D_loss: 0.0019, G_loss: 11.6074\n",
      "Epoch [412/800], Step [26/53], D_loss: 0.0198, G_loss: 6.0301\n",
      "Epoch [412/800], Step [28/53], D_loss: 0.0306, G_loss: 3.0546\n",
      "Epoch [412/800], Step [30/53], D_loss: 0.0016, G_loss: 9.2449\n",
      "Epoch [412/800], Step [32/53], D_loss: 0.2302, G_loss: 5.3537\n",
      "Epoch [412/800], Step [34/53], D_loss: 0.0771, G_loss: 6.1822\n",
      "Epoch [412/800], Step [36/53], D_loss: 0.0193, G_loss: 5.0180\n",
      "Epoch [412/800], Step [38/53], D_loss: 0.0069, G_loss: 9.1324\n",
      "Epoch [412/800], Step [40/53], D_loss: 0.0161, G_loss: 6.7703\n",
      "Epoch [412/800], Step [42/53], D_loss: 0.0306, G_loss: 6.4343\n",
      "Epoch [412/800], Step [44/53], D_loss: 0.0193, G_loss: 7.2286\n",
      "Epoch [412/800], Step [46/53], D_loss: 0.0620, G_loss: 2.8825\n",
      "Epoch [412/800], Step [48/53], D_loss: 0.0029, G_loss: 9.8691\n",
      "Epoch [412/800], Step [50/53], D_loss: 0.0336, G_loss: 4.6451\n",
      "Epoch [412/800], Step [52/53], D_loss: 0.1144, G_loss: 7.0574\n",
      "Epoch [412/800], Avg D_loss: 0.0810, Avg G_loss: 7.3860\n",
      "Epoch [413/800], Step [2/53], D_loss: 0.0427, G_loss: 6.2346\n",
      "Epoch [413/800], Step [4/53], D_loss: 0.0833, G_loss: 4.7495\n",
      "Epoch [413/800], Step [6/53], D_loss: 0.0627, G_loss: 3.3865\n",
      "Epoch [413/800], Step [8/53], D_loss: 0.0217, G_loss: 9.8584\n",
      "Epoch [413/800], Step [10/53], D_loss: 0.0039, G_loss: 9.2219\n",
      "Epoch [413/800], Step [12/53], D_loss: 0.0541, G_loss: 8.1106\n",
      "Epoch [413/800], Step [14/53], D_loss: 0.0169, G_loss: 8.2211\n",
      "Epoch [413/800], Step [16/53], D_loss: 0.0023, G_loss: 6.8571\n",
      "Epoch [413/800], Step [18/53], D_loss: 0.0018, G_loss: 3.6677\n",
      "Epoch [413/800], Step [20/53], D_loss: 0.0119, G_loss: 6.5271\n",
      "Epoch [413/800], Step [22/53], D_loss: 0.0109, G_loss: 8.6310\n",
      "Epoch [413/800], Step [24/53], D_loss: 0.0063, G_loss: 5.6126\n",
      "Epoch [413/800], Step [26/53], D_loss: 0.0032, G_loss: 10.3654\n",
      "Epoch [413/800], Step [28/53], D_loss: 0.1277, G_loss: 4.0090\n",
      "Epoch [413/800], Step [30/53], D_loss: 0.0080, G_loss: 9.8829\n",
      "Epoch [413/800], Step [32/53], D_loss: 0.0345, G_loss: 7.9891\n",
      "Epoch [413/800], Step [34/53], D_loss: 0.0433, G_loss: 7.2580\n",
      "Epoch [413/800], Step [36/53], D_loss: 0.1751, G_loss: 6.8749\n",
      "Epoch [413/800], Step [38/53], D_loss: 0.1157, G_loss: 6.7119\n",
      "Epoch [413/800], Step [40/53], D_loss: 0.0122, G_loss: 10.5256\n",
      "Epoch [413/800], Step [42/53], D_loss: 0.1444, G_loss: 2.2875\n",
      "Epoch [413/800], Step [44/53], D_loss: 0.1625, G_loss: 9.4425\n",
      "Epoch [413/800], Step [46/53], D_loss: 0.0182, G_loss: 7.3761\n",
      "Epoch [413/800], Step [48/53], D_loss: 0.0738, G_loss: 9.0983\n",
      "Epoch [413/800], Step [50/53], D_loss: 0.0385, G_loss: 6.2416\n",
      "Epoch [413/800], Step [52/53], D_loss: 0.0256, G_loss: 6.7996\n",
      "Epoch [413/800], Avg D_loss: 0.0477, Avg G_loss: 7.8421\n",
      "Epoch [414/800], Step [2/53], D_loss: 0.0004, G_loss: 8.4127\n",
      "Epoch [414/800], Step [4/53], D_loss: 0.0018, G_loss: 6.1633\n",
      "Epoch [414/800], Step [6/53], D_loss: 0.1502, G_loss: 6.6554\n",
      "Epoch [414/800], Step [8/53], D_loss: 0.0311, G_loss: 7.2587\n",
      "Epoch [414/800], Step [10/53], D_loss: 0.0158, G_loss: 5.6576\n",
      "Epoch [414/800], Step [12/53], D_loss: 0.0118, G_loss: 8.6462\n",
      "Epoch [414/800], Step [14/53], D_loss: 0.0520, G_loss: 1.9577\n",
      "Epoch [414/800], Step [16/53], D_loss: 0.0822, G_loss: 6.2372\n",
      "Epoch [414/800], Step [18/53], D_loss: 0.0407, G_loss: 11.1753\n",
      "Epoch [414/800], Step [20/53], D_loss: 0.0817, G_loss: 7.9830\n",
      "Epoch [414/800], Step [22/53], D_loss: 0.0068, G_loss: 13.9941\n",
      "Epoch [414/800], Step [24/53], D_loss: 0.0596, G_loss: 7.6994\n",
      "Epoch [414/800], Step [26/53], D_loss: 0.0311, G_loss: 2.8935\n",
      "Epoch [414/800], Step [28/53], D_loss: 0.0354, G_loss: 8.4273\n",
      "Epoch [414/800], Step [30/53], D_loss: 0.0014, G_loss: 6.6892\n",
      "Epoch [414/800], Step [32/53], D_loss: 0.0167, G_loss: 8.2030\n",
      "Epoch [414/800], Step [34/53], D_loss: 0.0011, G_loss: 10.1161\n",
      "Epoch [414/800], Step [36/53], D_loss: 0.0018, G_loss: 10.5608\n",
      "Epoch [414/800], Step [38/53], D_loss: 0.0010, G_loss: 15.2141\n",
      "Epoch [414/800], Step [40/53], D_loss: 0.0278, G_loss: 7.2390\n",
      "Epoch [414/800], Step [42/53], D_loss: 0.2322, G_loss: 12.3936\n",
      "Epoch [414/800], Step [44/53], D_loss: 0.1811, G_loss: 10.7914\n",
      "Epoch [414/800], Step [46/53], D_loss: 1.0289, G_loss: 0.1298\n",
      "Epoch [414/800], Step [48/53], D_loss: 1.1496, G_loss: 9.2952\n",
      "Epoch [414/800], Step [50/53], D_loss: 0.1227, G_loss: 5.0515\n",
      "Epoch [414/800], Step [52/53], D_loss: 0.0791, G_loss: 2.9405\n",
      "Epoch [414/800], Avg D_loss: 0.1922, Avg G_loss: 7.8145\n",
      "Epoch [415/800], Step [2/53], D_loss: 0.1504, G_loss: 3.4981\n",
      "Epoch [415/800], Step [4/53], D_loss: 0.0664, G_loss: 3.8299\n",
      "Epoch [415/800], Step [6/53], D_loss: 0.1828, G_loss: 2.9020\n",
      "Epoch [415/800], Step [8/53], D_loss: 0.0680, G_loss: 4.3152\n",
      "Epoch [415/800], Step [10/53], D_loss: 0.2260, G_loss: 5.0729\n",
      "Epoch [415/800], Step [12/53], D_loss: 0.0618, G_loss: 3.0521\n",
      "Epoch [415/800], Step [14/53], D_loss: 0.0974, G_loss: 8.2959\n",
      "Epoch [415/800], Step [16/53], D_loss: 0.0202, G_loss: 0.5220\n",
      "Epoch [415/800], Step [18/53], D_loss: 0.1103, G_loss: 6.1656\n",
      "Epoch [415/800], Step [20/53], D_loss: 0.3757, G_loss: 4.8644\n",
      "Epoch [415/800], Step [22/53], D_loss: 0.1349, G_loss: 5.1921\n",
      "Epoch [415/800], Step [24/53], D_loss: 0.2828, G_loss: 3.3974\n",
      "Epoch [415/800], Step [26/53], D_loss: 0.0328, G_loss: 5.9345\n",
      "Epoch [415/800], Step [28/53], D_loss: 0.0948, G_loss: 4.5228\n",
      "Epoch [415/800], Step [30/53], D_loss: 0.1398, G_loss: 6.9351\n",
      "Epoch [415/800], Step [32/53], D_loss: 0.0303, G_loss: 3.8093\n",
      "Epoch [415/800], Step [34/53], D_loss: 0.1460, G_loss: 4.4524\n",
      "Epoch [415/800], Step [36/53], D_loss: 0.3752, G_loss: 5.1951\n",
      "Epoch [415/800], Step [38/53], D_loss: 0.0177, G_loss: 2.7699\n",
      "Epoch [415/800], Step [40/53], D_loss: 0.0715, G_loss: 5.1248\n",
      "Epoch [415/800], Step [42/53], D_loss: 0.0499, G_loss: 6.4556\n",
      "Epoch [415/800], Step [44/53], D_loss: 0.0247, G_loss: 4.5010\n",
      "Epoch [415/800], Step [46/53], D_loss: 0.0342, G_loss: 3.6457\n",
      "Epoch [415/800], Step [48/53], D_loss: 0.0272, G_loss: 4.3089\n",
      "Epoch [415/800], Step [50/53], D_loss: 0.0316, G_loss: 7.5065\n",
      "Epoch [415/800], Step [52/53], D_loss: 0.1023, G_loss: 4.6798\n",
      "Epoch [415/800], Avg D_loss: 0.1327, Avg G_loss: 4.5255\n",
      "Epoch [416/800], Step [2/53], D_loss: 0.0165, G_loss: 5.2347\n",
      "Epoch [416/800], Step [4/53], D_loss: 0.0786, G_loss: 4.3666\n",
      "Epoch [416/800], Step [6/53], D_loss: 0.0340, G_loss: 7.3968\n",
      "Epoch [416/800], Step [8/53], D_loss: 0.0807, G_loss: 5.5521\n",
      "Epoch [416/800], Step [10/53], D_loss: 0.0127, G_loss: 6.7367\n",
      "Epoch [416/800], Step [12/53], D_loss: 0.0107, G_loss: 7.3062\n",
      "Epoch [416/800], Step [14/53], D_loss: 0.0278, G_loss: 5.4805\n",
      "Epoch [416/800], Step [16/53], D_loss: 0.0115, G_loss: 7.6272\n",
      "Epoch [416/800], Step [18/53], D_loss: 0.0320, G_loss: 6.9369\n",
      "Epoch [416/800], Step [20/53], D_loss: 0.0289, G_loss: 5.9284\n",
      "Epoch [416/800], Step [22/53], D_loss: 0.0779, G_loss: 7.5904\n",
      "Epoch [416/800], Step [24/53], D_loss: 0.0591, G_loss: 7.5915\n",
      "Epoch [416/800], Step [26/53], D_loss: 0.0092, G_loss: 9.1449\n",
      "Epoch [416/800], Step [28/53], D_loss: 0.0116, G_loss: 10.0339\n",
      "Epoch [416/800], Step [30/53], D_loss: 0.0966, G_loss: 7.8930\n",
      "Epoch [416/800], Step [32/53], D_loss: 0.1921, G_loss: 6.0909\n",
      "Epoch [416/800], Step [34/53], D_loss: 0.0332, G_loss: 3.1373\n",
      "Epoch [416/800], Step [36/53], D_loss: 0.0730, G_loss: 7.1430\n",
      "Epoch [416/800], Step [38/53], D_loss: 0.0197, G_loss: 9.7037\n",
      "Epoch [416/800], Step [40/53], D_loss: 0.0102, G_loss: 5.3169\n",
      "Epoch [416/800], Step [42/53], D_loss: 0.3144, G_loss: 9.2663\n",
      "Epoch [416/800], Step [44/53], D_loss: 0.4194, G_loss: 4.5990\n",
      "Epoch [416/800], Step [46/53], D_loss: 0.0455, G_loss: 5.6066\n",
      "Epoch [416/800], Step [48/53], D_loss: 0.0854, G_loss: 6.4569\n",
      "Epoch [416/800], Step [50/53], D_loss: 0.0306, G_loss: 8.0841\n",
      "Epoch [416/800], Step [52/53], D_loss: 0.0195, G_loss: 6.3469\n",
      "Epoch [416/800], Avg D_loss: 0.0692, Avg G_loss: 6.7851\n",
      "Epoch [417/800], Step [2/53], D_loss: 0.0071, G_loss: 7.4133\n",
      "Epoch [417/800], Step [4/53], D_loss: 0.0761, G_loss: 3.3503\n",
      "Epoch [417/800], Step [6/53], D_loss: 0.0319, G_loss: 7.0608\n",
      "Epoch [417/800], Step [8/53], D_loss: 0.1604, G_loss: 6.9838\n",
      "Epoch [417/800], Step [10/53], D_loss: 0.0982, G_loss: 3.5079\n",
      "Epoch [417/800], Step [12/53], D_loss: 0.3277, G_loss: 8.1913\n",
      "Epoch [417/800], Step [14/53], D_loss: 0.0172, G_loss: 6.2493\n",
      "Epoch [417/800], Step [16/53], D_loss: 0.0787, G_loss: 6.8618\n",
      "Epoch [417/800], Step [18/53], D_loss: 0.0224, G_loss: 5.9424\n",
      "Epoch [417/800], Step [20/53], D_loss: 0.0042, G_loss: 9.3762\n",
      "Epoch [417/800], Step [22/53], D_loss: 0.0072, G_loss: 5.0504\n",
      "Epoch [417/800], Step [24/53], D_loss: 0.0013, G_loss: 6.1184\n",
      "Epoch [417/800], Step [26/53], D_loss: 0.0461, G_loss: 3.9265\n",
      "Epoch [417/800], Step [28/53], D_loss: 3.4012, G_loss: 8.3684\n",
      "Epoch [417/800], Step [30/53], D_loss: 0.1531, G_loss: 1.4125\n",
      "Epoch [417/800], Step [32/53], D_loss: 0.1990, G_loss: 3.5347\n",
      "Epoch [417/800], Step [34/53], D_loss: 0.1317, G_loss: 3.1563\n",
      "Epoch [417/800], Step [36/53], D_loss: 0.1616, G_loss: 4.3278\n",
      "Epoch [417/800], Step [38/53], D_loss: 0.1577, G_loss: 2.4695\n",
      "Epoch [417/800], Step [40/53], D_loss: 0.1049, G_loss: 1.9466\n",
      "Epoch [417/800], Step [42/53], D_loss: 0.2756, G_loss: 3.0857\n",
      "Epoch [417/800], Step [44/53], D_loss: 0.1440, G_loss: 2.0955\n",
      "Epoch [417/800], Step [46/53], D_loss: 0.1454, G_loss: 3.6170\n",
      "Epoch [417/800], Step [48/53], D_loss: 0.0508, G_loss: 7.0512\n",
      "Epoch [417/800], Step [50/53], D_loss: 0.0554, G_loss: 2.9020\n",
      "Epoch [417/800], Step [52/53], D_loss: 0.1288, G_loss: 4.3924\n",
      "Epoch [417/800], Avg D_loss: 0.2009, Avg G_loss: 5.2696\n",
      "Epoch [418/800], Step [2/53], D_loss: 0.0853, G_loss: 4.5957\n",
      "Epoch [418/800], Step [4/53], D_loss: 0.0427, G_loss: 5.7584\n",
      "Epoch [418/800], Step [6/53], D_loss: 0.0957, G_loss: 2.9112\n",
      "Epoch [418/800], Step [8/53], D_loss: 0.0099, G_loss: 2.9128\n",
      "Epoch [418/800], Step [10/53], D_loss: 0.0745, G_loss: 6.5859\n",
      "Epoch [418/800], Step [12/53], D_loss: 0.5002, G_loss: 2.8523\n",
      "Epoch [418/800], Step [14/53], D_loss: 0.4362, G_loss: 7.4136\n",
      "Epoch [418/800], Step [16/53], D_loss: 0.1370, G_loss: 5.8910\n",
      "Epoch [418/800], Step [18/53], D_loss: 0.0280, G_loss: 3.1289\n",
      "Epoch [418/800], Step [20/53], D_loss: 0.1180, G_loss: 5.1061\n",
      "Epoch [418/800], Step [22/53], D_loss: 0.0344, G_loss: 3.3555\n",
      "Epoch [418/800], Step [24/53], D_loss: 0.0086, G_loss: 4.7693\n",
      "Epoch [418/800], Step [26/53], D_loss: 0.4527, G_loss: 1.0552\n",
      "Epoch [418/800], Step [28/53], D_loss: 0.1097, G_loss: 2.8360\n",
      "Epoch [418/800], Step [30/53], D_loss: 0.0274, G_loss: 7.2188\n",
      "Epoch [418/800], Step [32/53], D_loss: 0.1240, G_loss: 7.4455\n",
      "Epoch [418/800], Step [34/53], D_loss: 0.1187, G_loss: 4.3903\n",
      "Epoch [418/800], Step [36/53], D_loss: 0.0434, G_loss: 1.4334\n",
      "Epoch [418/800], Step [38/53], D_loss: 0.0126, G_loss: 9.2660\n",
      "Epoch [418/800], Step [40/53], D_loss: 0.0293, G_loss: 3.3630\n",
      "Epoch [418/800], Step [42/53], D_loss: 0.0647, G_loss: 4.8280\n",
      "Epoch [418/800], Step [44/53], D_loss: 0.8457, G_loss: 3.5325\n",
      "Epoch [418/800], Step [46/53], D_loss: 0.1489, G_loss: 2.3138\n",
      "Epoch [418/800], Step [48/53], D_loss: 0.1167, G_loss: 4.4471\n",
      "Epoch [418/800], Step [50/53], D_loss: 0.1758, G_loss: 4.2633\n",
      "Epoch [418/800], Step [52/53], D_loss: 0.0962, G_loss: 3.1325\n",
      "Epoch [418/800], Avg D_loss: 0.1551, Avg G_loss: 4.4804\n",
      "Epoch [419/800], Step [2/53], D_loss: 0.2812, G_loss: 6.3404\n",
      "Epoch [419/800], Step [4/53], D_loss: 0.0232, G_loss: 3.9069\n",
      "Epoch [419/800], Step [6/53], D_loss: 0.0442, G_loss: 3.9528\n",
      "Epoch [419/800], Step [8/53], D_loss: 0.0419, G_loss: 4.7092\n",
      "Epoch [419/800], Step [10/53], D_loss: 0.2257, G_loss: 2.7288\n",
      "Epoch [419/800], Step [12/53], D_loss: 0.2320, G_loss: 7.2554\n",
      "Epoch [419/800], Step [14/53], D_loss: 0.0474, G_loss: 2.2911\n",
      "Epoch [419/800], Step [16/53], D_loss: 0.0654, G_loss: 3.3880\n",
      "Epoch [419/800], Step [18/53], D_loss: 0.0895, G_loss: 2.3403\n",
      "Epoch [419/800], Step [20/53], D_loss: 0.0655, G_loss: 4.2817\n",
      "Epoch [419/800], Step [22/53], D_loss: 0.1296, G_loss: 3.9783\n",
      "Epoch [419/800], Step [24/53], D_loss: 0.0667, G_loss: 5.6927\n",
      "Epoch [419/800], Step [26/53], D_loss: 0.0072, G_loss: 5.5910\n",
      "Epoch [419/800], Step [28/53], D_loss: 0.1777, G_loss: 6.3136\n",
      "Epoch [419/800], Step [30/53], D_loss: 0.0764, G_loss: 5.1324\n",
      "Epoch [419/800], Step [32/53], D_loss: 0.0167, G_loss: 6.5690\n",
      "Epoch [419/800], Step [34/53], D_loss: 0.0416, G_loss: 5.4743\n",
      "Epoch [419/800], Step [36/53], D_loss: 0.1189, G_loss: 5.9588\n",
      "Epoch [419/800], Step [38/53], D_loss: 0.4555, G_loss: 4.8222\n",
      "Epoch [419/800], Step [40/53], D_loss: 0.0962, G_loss: 8.3817\n",
      "Epoch [419/800], Step [42/53], D_loss: 0.0204, G_loss: 4.5160\n",
      "Epoch [419/800], Step [44/53], D_loss: 0.0277, G_loss: 3.2598\n",
      "Epoch [419/800], Step [46/53], D_loss: 0.0136, G_loss: 2.6315\n",
      "Epoch [419/800], Step [48/53], D_loss: 0.0413, G_loss: 3.6409\n",
      "Epoch [419/800], Step [50/53], D_loss: 0.0455, G_loss: 5.8721\n",
      "Epoch [419/800], Step [52/53], D_loss: 0.0243, G_loss: 4.9539\n",
      "Epoch [419/800], Avg D_loss: 0.1178, Avg G_loss: 4.6248\n",
      "Epoch [420/800], Step [2/53], D_loss: 0.0200, G_loss: 6.9153\n",
      "Epoch [420/800], Step [4/53], D_loss: 0.0482, G_loss: 6.1499\n",
      "Epoch [420/800], Step [6/53], D_loss: 0.2936, G_loss: 6.4337\n",
      "Epoch [420/800], Step [8/53], D_loss: 0.0324, G_loss: 5.2170\n",
      "Epoch [420/800], Step [10/53], D_loss: 0.1442, G_loss: 5.6140\n",
      "Epoch [420/800], Step [12/53], D_loss: 0.3739, G_loss: 5.7752\n",
      "Epoch [420/800], Step [14/53], D_loss: 0.0073, G_loss: 1.9995\n",
      "Epoch [420/800], Step [16/53], D_loss: 0.4612, G_loss: 8.2845\n",
      "Epoch [420/800], Step [18/53], D_loss: 0.0198, G_loss: 2.1064\n",
      "Epoch [420/800], Step [20/53], D_loss: 0.2161, G_loss: 2.4600\n",
      "Epoch [420/800], Step [22/53], D_loss: 0.0559, G_loss: 4.7432\n",
      "Epoch [420/800], Step [24/53], D_loss: 0.1542, G_loss: 6.1500\n",
      "Epoch [420/800], Step [26/53], D_loss: 0.0213, G_loss: 2.9773\n",
      "Epoch [420/800], Step [28/53], D_loss: 0.4724, G_loss: 6.8770\n",
      "Epoch [420/800], Step [30/53], D_loss: 0.1379, G_loss: 6.5083\n",
      "Epoch [420/800], Step [32/53], D_loss: 0.0317, G_loss: 2.7030\n",
      "Epoch [420/800], Step [34/53], D_loss: 0.1721, G_loss: 7.2027\n",
      "Epoch [420/800], Step [36/53], D_loss: 0.1100, G_loss: 5.3839\n",
      "Epoch [420/800], Step [38/53], D_loss: 0.0272, G_loss: 4.7629\n",
      "Epoch [420/800], Step [40/53], D_loss: 0.0434, G_loss: 5.9703\n",
      "Epoch [420/800], Step [42/53], D_loss: 0.2895, G_loss: 7.0499\n",
      "Epoch [420/800], Step [44/53], D_loss: 0.0693, G_loss: 4.5501\n",
      "Epoch [420/800], Step [46/53], D_loss: 0.0723, G_loss: 5.5234\n",
      "Epoch [420/800], Step [48/53], D_loss: 0.1476, G_loss: 5.1333\n",
      "Epoch [420/800], Step [50/53], D_loss: 0.3138, G_loss: 5.8365\n",
      "Epoch [420/800], Step [52/53], D_loss: 0.0802, G_loss: 4.4060\n",
      "Epoch [420/800], Avg D_loss: 0.1381, Avg G_loss: 5.1713\n",
      "Generated images saved as generated_images/epoch_420.png\n",
      "Epoch [421/800], Step [2/53], D_loss: 0.0226, G_loss: 8.0704\n",
      "Epoch [421/800], Step [4/53], D_loss: 0.0643, G_loss: 5.3345\n",
      "Epoch [421/800], Step [6/53], D_loss: 0.0321, G_loss: 4.5953\n",
      "Epoch [421/800], Step [8/53], D_loss: 0.0498, G_loss: 2.9969\n",
      "Epoch [421/800], Step [10/53], D_loss: 0.0057, G_loss: 3.0270\n",
      "Epoch [421/800], Step [12/53], D_loss: 0.0980, G_loss: 9.1325\n",
      "Epoch [421/800], Step [14/53], D_loss: 0.0204, G_loss: 6.0581\n",
      "Epoch [421/800], Step [16/53], D_loss: 0.1320, G_loss: 5.5175\n",
      "Epoch [421/800], Step [18/53], D_loss: 0.0240, G_loss: 6.5932\n",
      "Epoch [421/800], Step [20/53], D_loss: 0.0660, G_loss: 6.8510\n",
      "Epoch [421/800], Step [22/53], D_loss: 0.0186, G_loss: 2.6664\n",
      "Epoch [421/800], Step [24/53], D_loss: 0.1149, G_loss: 4.1520\n",
      "Epoch [421/800], Step [26/53], D_loss: 0.4075, G_loss: 2.1777\n",
      "Epoch [421/800], Step [28/53], D_loss: 0.0009, G_loss: 6.4071\n",
      "Epoch [421/800], Step [30/53], D_loss: 0.0261, G_loss: 7.5623\n",
      "Epoch [421/800], Step [32/53], D_loss: 0.0205, G_loss: 3.6581\n",
      "Epoch [421/800], Step [34/53], D_loss: 0.1813, G_loss: 5.3198\n",
      "Epoch [421/800], Step [36/53], D_loss: 0.2987, G_loss: 4.5821\n",
      "Epoch [421/800], Step [38/53], D_loss: 0.0318, G_loss: 8.5674\n",
      "Epoch [421/800], Step [40/53], D_loss: 0.1425, G_loss: 4.0821\n",
      "Epoch [421/800], Step [42/53], D_loss: 0.0312, G_loss: 4.5989\n",
      "Epoch [421/800], Step [44/53], D_loss: 0.0233, G_loss: 6.1860\n",
      "Epoch [421/800], Step [46/53], D_loss: 0.1715, G_loss: 2.6220\n",
      "Epoch [421/800], Step [48/53], D_loss: 0.1175, G_loss: 6.1386\n",
      "Epoch [421/800], Step [50/53], D_loss: 0.5123, G_loss: 0.3878\n",
      "Epoch [421/800], Step [52/53], D_loss: 0.8558, G_loss: 4.6060\n",
      "Epoch [421/800], Avg D_loss: 0.1503, Avg G_loss: 5.2008\n",
      "Epoch [422/800], Step [2/53], D_loss: 0.0573, G_loss: 5.4950\n",
      "Epoch [422/800], Step [4/53], D_loss: 0.1320, G_loss: 4.1809\n",
      "Epoch [422/800], Step [6/53], D_loss: 0.0606, G_loss: 4.5924\n",
      "Epoch [422/800], Step [8/53], D_loss: 0.2246, G_loss: 5.1460\n",
      "Epoch [422/800], Step [10/53], D_loss: 0.0265, G_loss: 2.2838\n",
      "Epoch [422/800], Step [12/53], D_loss: 0.0154, G_loss: 2.7185\n",
      "Epoch [422/800], Step [14/53], D_loss: 0.1257, G_loss: 3.2719\n",
      "Epoch [422/800], Step [16/53], D_loss: 0.0262, G_loss: 4.8561\n",
      "Epoch [422/800], Step [18/53], D_loss: 0.0086, G_loss: 5.1607\n",
      "Epoch [422/800], Step [20/53], D_loss: 0.0082, G_loss: 6.7602\n",
      "Epoch [422/800], Step [22/53], D_loss: 1.0255, G_loss: 8.1138\n",
      "Epoch [422/800], Step [24/53], D_loss: 1.0964, G_loss: 4.8744\n",
      "Epoch [422/800], Step [26/53], D_loss: 0.2668, G_loss: 3.1472\n",
      "Epoch [422/800], Step [28/53], D_loss: 0.0578, G_loss: 3.9858\n",
      "Epoch [422/800], Step [30/53], D_loss: 0.1170, G_loss: 4.1911\n",
      "Epoch [422/800], Step [32/53], D_loss: 0.0883, G_loss: 5.0904\n",
      "Epoch [422/800], Step [34/53], D_loss: 0.2236, G_loss: 4.8860\n",
      "Epoch [422/800], Step [36/53], D_loss: 0.0183, G_loss: 2.0887\n",
      "Epoch [422/800], Step [38/53], D_loss: 0.0713, G_loss: 4.6978\n",
      "Epoch [422/800], Step [40/53], D_loss: 0.0549, G_loss: 4.5500\n",
      "Epoch [422/800], Step [42/53], D_loss: 0.0599, G_loss: 4.9677\n",
      "Epoch [422/800], Step [44/53], D_loss: 0.2001, G_loss: 6.6143\n",
      "Epoch [422/800], Step [46/53], D_loss: 0.0972, G_loss: 4.7586\n",
      "Epoch [422/800], Step [48/53], D_loss: 0.1525, G_loss: 1.5627\n",
      "Epoch [422/800], Step [50/53], D_loss: 0.0620, G_loss: 3.4172\n",
      "Epoch [422/800], Step [52/53], D_loss: 0.0627, G_loss: 1.7068\n",
      "Epoch [422/800], Avg D_loss: 0.1589, Avg G_loss: 4.2418\n",
      "Epoch [423/800], Step [2/53], D_loss: 0.0606, G_loss: 3.9175\n",
      "Epoch [423/800], Step [4/53], D_loss: 0.0639, G_loss: 4.7237\n",
      "Epoch [423/800], Step [6/53], D_loss: 0.0235, G_loss: 5.1602\n",
      "Epoch [423/800], Step [8/53], D_loss: 0.0214, G_loss: 3.0296\n",
      "Epoch [423/800], Step [10/53], D_loss: 0.0356, G_loss: 4.4909\n",
      "Epoch [423/800], Step [12/53], D_loss: 0.1735, G_loss: 5.6367\n",
      "Epoch [423/800], Step [14/53], D_loss: 0.0396, G_loss: 6.0368\n",
      "Epoch [423/800], Step [16/53], D_loss: 0.0432, G_loss: 1.4636\n",
      "Epoch [423/800], Step [18/53], D_loss: 0.1133, G_loss: 4.3128\n",
      "Epoch [423/800], Step [20/53], D_loss: 0.0042, G_loss: 5.8069\n",
      "Epoch [423/800], Step [22/53], D_loss: 0.0113, G_loss: 3.2570\n",
      "Epoch [423/800], Step [24/53], D_loss: 0.0821, G_loss: 6.2377\n",
      "Epoch [423/800], Step [26/53], D_loss: 0.0253, G_loss: 5.2627\n",
      "Epoch [423/800], Step [28/53], D_loss: 0.0080, G_loss: 6.0090\n",
      "Epoch [423/800], Step [30/53], D_loss: 0.0023, G_loss: 8.6505\n",
      "Epoch [423/800], Step [32/53], D_loss: 0.0589, G_loss: 5.3155\n",
      "Epoch [423/800], Step [34/53], D_loss: 0.0119, G_loss: 6.2082\n",
      "Epoch [423/800], Step [36/53], D_loss: 0.0032, G_loss: 6.6456\n",
      "Epoch [423/800], Step [38/53], D_loss: 0.1383, G_loss: 4.6740\n",
      "Epoch [423/800], Step [40/53], D_loss: 0.1484, G_loss: 5.7499\n",
      "Epoch [423/800], Step [42/53], D_loss: 0.0116, G_loss: 7.6272\n",
      "Epoch [423/800], Step [44/53], D_loss: 0.2686, G_loss: 5.0544\n",
      "Epoch [423/800], Step [46/53], D_loss: 0.0031, G_loss: 7.4628\n",
      "Epoch [423/800], Step [48/53], D_loss: 0.0168, G_loss: 7.5637\n",
      "Epoch [423/800], Step [50/53], D_loss: 0.0151, G_loss: 4.5930\n",
      "Epoch [423/800], Step [52/53], D_loss: 0.0916, G_loss: 5.8763\n",
      "Epoch [423/800], Avg D_loss: 0.0801, Avg G_loss: 5.3926\n",
      "Epoch [424/800], Step [2/53], D_loss: 0.0024, G_loss: 5.9529\n",
      "Epoch [424/800], Step [4/53], D_loss: 0.5160, G_loss: 9.9802\n",
      "Epoch [424/800], Step [6/53], D_loss: 0.1210, G_loss: 4.2805\n",
      "Epoch [424/800], Step [8/53], D_loss: 0.1935, G_loss: 5.4640\n",
      "Epoch [424/800], Step [10/53], D_loss: 0.1047, G_loss: 5.7130\n",
      "Epoch [424/800], Step [12/53], D_loss: 0.0727, G_loss: 7.1670\n",
      "Epoch [424/800], Step [14/53], D_loss: 0.0328, G_loss: 6.0529\n",
      "Epoch [424/800], Step [16/53], D_loss: 0.0625, G_loss: 3.3084\n",
      "Epoch [424/800], Step [18/53], D_loss: 0.1918, G_loss: 5.6145\n",
      "Epoch [424/800], Step [20/53], D_loss: 0.0596, G_loss: 6.3034\n",
      "Epoch [424/800], Step [22/53], D_loss: 0.1605, G_loss: 5.2212\n",
      "Epoch [424/800], Step [24/53], D_loss: 0.6350, G_loss: 7.3906\n",
      "Epoch [424/800], Step [26/53], D_loss: 0.0094, G_loss: 2.5100\n",
      "Epoch [424/800], Step [28/53], D_loss: 0.0851, G_loss: 3.6044\n",
      "Epoch [424/800], Step [30/53], D_loss: 0.1825, G_loss: 4.3612\n",
      "Epoch [424/800], Step [32/53], D_loss: 0.0897, G_loss: 4.1392\n",
      "Epoch [424/800], Step [34/53], D_loss: 0.2023, G_loss: 2.3029\n",
      "Epoch [424/800], Step [36/53], D_loss: 0.0738, G_loss: 5.1333\n",
      "Epoch [424/800], Step [38/53], D_loss: 0.1703, G_loss: 3.2328\n",
      "Epoch [424/800], Step [40/53], D_loss: 0.0624, G_loss: 6.1460\n",
      "Epoch [424/800], Step [42/53], D_loss: 0.2146, G_loss: 5.6258\n",
      "Epoch [424/800], Step [44/53], D_loss: 0.0332, G_loss: 5.2713\n",
      "Epoch [424/800], Step [46/53], D_loss: 0.0116, G_loss: 4.2337\n",
      "Epoch [424/800], Step [48/53], D_loss: 0.0308, G_loss: 5.6513\n",
      "Epoch [424/800], Step [50/53], D_loss: 0.0134, G_loss: 3.5868\n",
      "Epoch [424/800], Step [52/53], D_loss: 0.1376, G_loss: 5.7600\n",
      "Epoch [424/800], Avg D_loss: 0.1449, Avg G_loss: 5.2793\n",
      "Epoch [425/800], Step [2/53], D_loss: 0.3211, G_loss: 7.5869\n",
      "Epoch [425/800], Step [4/53], D_loss: 0.1395, G_loss: 3.9696\n",
      "Epoch [425/800], Step [6/53], D_loss: 0.1788, G_loss: 2.7849\n",
      "Epoch [425/800], Step [8/53], D_loss: 0.0375, G_loss: 7.2356\n",
      "Epoch [425/800], Step [10/53], D_loss: 0.0290, G_loss: 4.8769\n",
      "Epoch [425/800], Step [12/53], D_loss: 0.1114, G_loss: 5.6557\n",
      "Epoch [425/800], Step [14/53], D_loss: 0.0444, G_loss: 7.2680\n",
      "Epoch [425/800], Step [16/53], D_loss: 0.0131, G_loss: 5.3835\n",
      "Epoch [425/800], Step [18/53], D_loss: 0.0288, G_loss: 5.2842\n",
      "Epoch [425/800], Step [20/53], D_loss: 0.1413, G_loss: 6.3814\n",
      "Epoch [425/800], Step [22/53], D_loss: 0.1873, G_loss: 9.2929\n",
      "Epoch [425/800], Step [24/53], D_loss: 0.2228, G_loss: 2.9667\n",
      "Epoch [425/800], Step [26/53], D_loss: 0.0574, G_loss: 1.3868\n",
      "Epoch [425/800], Step [28/53], D_loss: 0.0619, G_loss: 3.8053\n",
      "Epoch [425/800], Step [30/53], D_loss: 1.5184, G_loss: 7.9654\n",
      "Epoch [425/800], Step [32/53], D_loss: 0.0625, G_loss: 3.1712\n",
      "Epoch [425/800], Step [34/53], D_loss: 0.6766, G_loss: 4.1721\n",
      "Epoch [425/800], Step [36/53], D_loss: 0.6314, G_loss: 3.1718\n",
      "Epoch [425/800], Step [38/53], D_loss: 0.1523, G_loss: 2.7419\n",
      "Epoch [425/800], Step [40/53], D_loss: 0.1123, G_loss: 3.6770\n",
      "Epoch [425/800], Step [42/53], D_loss: 0.2323, G_loss: 2.4390\n",
      "Epoch [425/800], Step [44/53], D_loss: 0.1948, G_loss: 3.1923\n",
      "Epoch [425/800], Step [46/53], D_loss: 0.0867, G_loss: 3.2090\n",
      "Epoch [425/800], Step [48/53], D_loss: 0.0381, G_loss: 6.1178\n",
      "Epoch [425/800], Step [50/53], D_loss: 0.0573, G_loss: 3.3026\n",
      "Epoch [425/800], Step [52/53], D_loss: 0.1690, G_loss: 2.8085\n",
      "Epoch [425/800], Avg D_loss: 0.1907, Avg G_loss: 4.7573\n",
      "Epoch [426/800], Step [2/53], D_loss: 0.0077, G_loss: 5.0858\n",
      "Epoch [426/800], Step [4/53], D_loss: 0.1877, G_loss: 5.3121\n",
      "Epoch [426/800], Step [6/53], D_loss: 0.0202, G_loss: 2.2486\n",
      "Epoch [426/800], Step [8/53], D_loss: 0.0827, G_loss: 3.6948\n",
      "Epoch [426/800], Step [10/53], D_loss: 0.2017, G_loss: 3.4338\n",
      "Epoch [426/800], Step [12/53], D_loss: 0.1002, G_loss: 6.1024\n",
      "Epoch [426/800], Step [14/53], D_loss: 0.0349, G_loss: 4.3948\n",
      "Epoch [426/800], Step [16/53], D_loss: 0.0542, G_loss: 3.8207\n",
      "Epoch [426/800], Step [18/53], D_loss: 0.0876, G_loss: 2.7725\n",
      "Epoch [426/800], Step [20/53], D_loss: 0.2294, G_loss: 9.1789\n",
      "Epoch [426/800], Step [22/53], D_loss: 0.0697, G_loss: 0.5383\n",
      "Epoch [426/800], Step [24/53], D_loss: 0.1784, G_loss: 7.3048\n",
      "Epoch [426/800], Step [26/53], D_loss: 0.0662, G_loss: 3.5813\n",
      "Epoch [426/800], Step [28/53], D_loss: 0.1012, G_loss: 3.3952\n",
      "Epoch [426/800], Step [30/53], D_loss: 0.1109, G_loss: 5.0146\n",
      "Epoch [426/800], Step [32/53], D_loss: 0.3653, G_loss: 7.8403\n",
      "Epoch [426/800], Step [34/53], D_loss: 0.0106, G_loss: 5.2528\n",
      "Epoch [426/800], Step [36/53], D_loss: 0.1877, G_loss: 2.4932\n",
      "Epoch [426/800], Step [38/53], D_loss: 0.1012, G_loss: 6.0167\n",
      "Epoch [426/800], Step [40/53], D_loss: 0.0234, G_loss: 2.7178\n",
      "Epoch [426/800], Step [42/53], D_loss: 0.0961, G_loss: 3.1487\n",
      "Epoch [426/800], Step [44/53], D_loss: 0.0363, G_loss: 2.9632\n",
      "Epoch [426/800], Step [46/53], D_loss: 0.1070, G_loss: 5.9540\n",
      "Epoch [426/800], Step [48/53], D_loss: 0.2240, G_loss: 8.1906\n",
      "Epoch [426/800], Step [50/53], D_loss: 0.1563, G_loss: 3.7599\n",
      "Epoch [426/800], Step [52/53], D_loss: 0.0068, G_loss: 5.1277\n",
      "Epoch [426/800], Avg D_loss: 0.1407, Avg G_loss: 4.6379\n",
      "Epoch [427/800], Step [2/53], D_loss: 0.1158, G_loss: 4.2292\n",
      "Epoch [427/800], Step [4/53], D_loss: 0.1049, G_loss: 4.4563\n",
      "Epoch [427/800], Step [6/53], D_loss: 0.0120, G_loss: 3.7201\n",
      "Epoch [427/800], Step [8/53], D_loss: 0.3115, G_loss: 6.6959\n",
      "Epoch [427/800], Step [10/53], D_loss: 0.0626, G_loss: 7.4328\n",
      "Epoch [427/800], Step [12/53], D_loss: 0.4978, G_loss: 9.6632\n",
      "Epoch [427/800], Step [14/53], D_loss: 1.0896, G_loss: 5.5443\n",
      "Epoch [427/800], Step [16/53], D_loss: 0.2704, G_loss: 2.4749\n",
      "Epoch [427/800], Step [18/53], D_loss: 0.2746, G_loss: 2.8891\n",
      "Epoch [427/800], Step [20/53], D_loss: 0.1908, G_loss: 4.8397\n",
      "Epoch [427/800], Step [22/53], D_loss: 0.0486, G_loss: 6.0881\n",
      "Epoch [427/800], Step [24/53], D_loss: 0.1934, G_loss: 2.7574\n",
      "Epoch [427/800], Step [26/53], D_loss: 0.2741, G_loss: 5.3389\n",
      "Epoch [427/800], Step [28/53], D_loss: 0.0136, G_loss: 5.2205\n",
      "Epoch [427/800], Step [30/53], D_loss: 0.1130, G_loss: 1.5846\n",
      "Epoch [427/800], Step [32/53], D_loss: 0.1872, G_loss: 2.5928\n",
      "Epoch [427/800], Step [34/53], D_loss: 0.0933, G_loss: 5.4248\n",
      "Epoch [427/800], Step [36/53], D_loss: 0.0052, G_loss: 5.1267\n",
      "Epoch [427/800], Step [38/53], D_loss: 0.1263, G_loss: 4.8610\n",
      "Epoch [427/800], Step [40/53], D_loss: 0.1175, G_loss: 6.7258\n",
      "Epoch [427/800], Step [42/53], D_loss: 0.0168, G_loss: 4.0790\n",
      "Epoch [427/800], Step [44/53], D_loss: 0.0572, G_loss: 5.9654\n",
      "Epoch [427/800], Step [46/53], D_loss: 0.0152, G_loss: 6.0204\n",
      "Epoch [427/800], Step [48/53], D_loss: 0.0289, G_loss: 3.5780\n",
      "Epoch [427/800], Step [50/53], D_loss: 0.3068, G_loss: 8.7278\n",
      "Epoch [427/800], Step [52/53], D_loss: 0.0166, G_loss: 0.1965\n",
      "Epoch [427/800], Avg D_loss: 0.1797, Avg G_loss: 5.0274\n",
      "Epoch [428/800], Step [2/53], D_loss: 0.2374, G_loss: 3.4262\n",
      "Epoch [428/800], Step [4/53], D_loss: 0.1827, G_loss: 1.9304\n",
      "Epoch [428/800], Step [6/53], D_loss: 0.1954, G_loss: 4.0903\n",
      "Epoch [428/800], Step [8/53], D_loss: 0.4449, G_loss: 2.9914\n",
      "Epoch [428/800], Step [10/53], D_loss: 0.1685, G_loss: 4.7638\n",
      "Epoch [428/800], Step [12/53], D_loss: 0.1148, G_loss: 3.4818\n",
      "Epoch [428/800], Step [14/53], D_loss: 0.1483, G_loss: 2.8969\n",
      "Epoch [428/800], Step [16/53], D_loss: 0.1922, G_loss: 4.2614\n",
      "Epoch [428/800], Step [18/53], D_loss: 0.0976, G_loss: 1.7382\n",
      "Epoch [428/800], Step [20/53], D_loss: 0.0553, G_loss: 5.6500\n",
      "Epoch [428/800], Step [22/53], D_loss: 0.7198, G_loss: 0.6998\n",
      "Epoch [428/800], Step [24/53], D_loss: 0.7311, G_loss: 6.8518\n",
      "Epoch [428/800], Step [26/53], D_loss: 0.2729, G_loss: 3.4754\n",
      "Epoch [428/800], Step [28/53], D_loss: 0.2035, G_loss: 2.1852\n",
      "Epoch [428/800], Step [30/53], D_loss: 0.0791, G_loss: 3.5582\n",
      "Epoch [428/800], Step [32/53], D_loss: 0.3362, G_loss: 5.1279\n",
      "Epoch [428/800], Step [34/53], D_loss: 0.2620, G_loss: 4.7609\n",
      "Epoch [428/800], Step [36/53], D_loss: 0.2543, G_loss: 4.0558\n",
      "Epoch [428/800], Step [38/53], D_loss: 0.7659, G_loss: 2.3332\n",
      "Epoch [428/800], Step [40/53], D_loss: 0.1320, G_loss: 2.0455\n",
      "Epoch [428/800], Step [42/53], D_loss: 0.1326, G_loss: 6.9052\n",
      "Epoch [428/800], Step [44/53], D_loss: 0.1265, G_loss: 4.4010\n",
      "Epoch [428/800], Step [46/53], D_loss: 0.0478, G_loss: 3.6441\n",
      "Epoch [428/800], Step [48/53], D_loss: 0.0757, G_loss: 5.0651\n",
      "Epoch [428/800], Step [50/53], D_loss: 0.1904, G_loss: 4.8075\n",
      "Epoch [428/800], Step [52/53], D_loss: 0.1045, G_loss: 5.4419\n",
      "Epoch [428/800], Avg D_loss: 0.2191, Avg G_loss: 3.9284\n",
      "Epoch [429/800], Step [2/53], D_loss: 0.1326, G_loss: 2.8352\n",
      "Epoch [429/800], Step [4/53], D_loss: 0.0176, G_loss: 6.1256\n",
      "Epoch [429/800], Step [6/53], D_loss: 0.0702, G_loss: 4.1951\n",
      "Epoch [429/800], Step [8/53], D_loss: 0.0955, G_loss: 5.5682\n",
      "Epoch [429/800], Step [10/53], D_loss: 0.0890, G_loss: 6.5068\n",
      "Epoch [429/800], Step [12/53], D_loss: 0.0318, G_loss: 5.6938\n",
      "Epoch [429/800], Step [14/53], D_loss: 0.2032, G_loss: 5.9022\n",
      "Epoch [429/800], Step [16/53], D_loss: 0.8081, G_loss: 9.3992\n",
      "Epoch [429/800], Step [18/53], D_loss: 0.0748, G_loss: 7.8184\n",
      "Epoch [429/800], Step [20/53], D_loss: 0.0382, G_loss: 4.0337\n",
      "Epoch [429/800], Step [22/53], D_loss: 0.0340, G_loss: 3.4649\n",
      "Epoch [429/800], Step [24/53], D_loss: 0.2370, G_loss: 7.7369\n",
      "Epoch [429/800], Step [26/53], D_loss: 0.0294, G_loss: 3.6476\n",
      "Epoch [429/800], Step [28/53], D_loss: 0.1284, G_loss: 2.7240\n",
      "Epoch [429/800], Step [30/53], D_loss: 0.0340, G_loss: 6.2189\n",
      "Epoch [429/800], Step [32/53], D_loss: 0.1237, G_loss: 7.1786\n",
      "Epoch [429/800], Step [34/53], D_loss: 0.0431, G_loss: 3.9438\n",
      "Epoch [429/800], Step [36/53], D_loss: 0.1391, G_loss: 7.1200\n",
      "Epoch [429/800], Step [38/53], D_loss: 0.0444, G_loss: 3.6535\n",
      "Epoch [429/800], Step [40/53], D_loss: 0.0310, G_loss: 3.8438\n",
      "Epoch [429/800], Step [42/53], D_loss: 0.1872, G_loss: 7.8011\n",
      "Epoch [429/800], Step [44/53], D_loss: 0.0630, G_loss: 1.3064\n",
      "Epoch [429/800], Step [46/53], D_loss: 0.2820, G_loss: 6.1162\n",
      "Epoch [429/800], Step [48/53], D_loss: 0.1874, G_loss: 4.2747\n",
      "Epoch [429/800], Step [50/53], D_loss: 0.1314, G_loss: 4.6796\n",
      "Epoch [429/800], Step [52/53], D_loss: 0.0768, G_loss: 4.5343\n",
      "Epoch [429/800], Avg D_loss: 0.1579, Avg G_loss: 5.1438\n",
      "Epoch [430/800], Step [2/53], D_loss: 0.2012, G_loss: 4.0549\n",
      "Epoch [430/800], Step [4/53], D_loss: 0.0416, G_loss: 3.0102\n",
      "Epoch [430/800], Step [6/53], D_loss: 0.0199, G_loss: 5.0579\n",
      "Epoch [430/800], Step [8/53], D_loss: 0.1025, G_loss: 1.8472\n",
      "Epoch [430/800], Step [10/53], D_loss: 0.2070, G_loss: 7.1533\n",
      "Epoch [430/800], Step [12/53], D_loss: 0.0539, G_loss: 3.6768\n",
      "Epoch [430/800], Step [14/53], D_loss: 0.0858, G_loss: 2.5390\n",
      "Epoch [430/800], Step [16/53], D_loss: 0.1162, G_loss: 3.5963\n",
      "Epoch [430/800], Step [18/53], D_loss: 0.2020, G_loss: 3.2164\n",
      "Epoch [430/800], Step [20/53], D_loss: 1.4025, G_loss: 9.9950\n",
      "Epoch [430/800], Step [22/53], D_loss: 0.8407, G_loss: 5.4962\n",
      "Epoch [430/800], Step [24/53], D_loss: 0.1293, G_loss: 4.3677\n",
      "Epoch [430/800], Step [26/53], D_loss: 0.0765, G_loss: 4.3673\n",
      "Epoch [430/800], Step [28/53], D_loss: 0.0835, G_loss: 3.8829\n",
      "Epoch [430/800], Step [30/53], D_loss: 0.0830, G_loss: 3.7880\n",
      "Epoch [430/800], Step [32/53], D_loss: 0.2169, G_loss: 4.2469\n",
      "Epoch [430/800], Step [34/53], D_loss: 0.0619, G_loss: 3.1694\n",
      "Epoch [430/800], Step [36/53], D_loss: 0.2410, G_loss: 2.5264\n",
      "Epoch [430/800], Step [38/53], D_loss: 0.0750, G_loss: 6.5303\n",
      "Epoch [430/800], Step [40/53], D_loss: 0.1453, G_loss: 4.3045\n",
      "Epoch [430/800], Step [42/53], D_loss: 0.1582, G_loss: 4.0553\n",
      "Epoch [430/800], Step [44/53], D_loss: 0.0702, G_loss: 4.6114\n",
      "Epoch [430/800], Step [46/53], D_loss: 0.0344, G_loss: 5.5033\n",
      "Epoch [430/800], Step [48/53], D_loss: 0.0093, G_loss: 4.3090\n",
      "Epoch [430/800], Step [50/53], D_loss: 0.0258, G_loss: 3.2541\n",
      "Epoch [430/800], Step [52/53], D_loss: 0.0160, G_loss: 5.8407\n",
      "Epoch [430/800], Avg D_loss: 0.1556, Avg G_loss: 4.3955\n",
      "Epoch [431/800], Step [2/53], D_loss: 0.2997, G_loss: 5.0532\n",
      "Epoch [431/800], Step [4/53], D_loss: 0.0321, G_loss: 5.9463\n",
      "Epoch [431/800], Step [6/53], D_loss: 0.0099, G_loss: 6.1880\n",
      "Epoch [431/800], Step [8/53], D_loss: 0.1970, G_loss: 8.1404\n",
      "Epoch [431/800], Step [10/53], D_loss: 0.0211, G_loss: 7.0919\n",
      "Epoch [431/800], Step [12/53], D_loss: 0.0227, G_loss: 6.6083\n",
      "Epoch [431/800], Step [14/53], D_loss: 0.0069, G_loss: 3.0119\n",
      "Epoch [431/800], Step [16/53], D_loss: 0.1690, G_loss: 4.1120\n",
      "Epoch [431/800], Step [18/53], D_loss: 0.0410, G_loss: 9.0102\n",
      "Epoch [431/800], Step [20/53], D_loss: 0.0109, G_loss: 4.6693\n",
      "Epoch [431/800], Step [22/53], D_loss: 0.0505, G_loss: 5.1633\n",
      "Epoch [431/800], Step [24/53], D_loss: 0.5760, G_loss: 12.8653\n",
      "Epoch [431/800], Step [26/53], D_loss: 0.8160, G_loss: 2.7995\n",
      "Epoch [431/800], Step [28/53], D_loss: 0.0167, G_loss: 5.7853\n",
      "Epoch [431/800], Step [30/53], D_loss: 0.0507, G_loss: 3.0441\n",
      "Epoch [431/800], Step [32/53], D_loss: 0.0194, G_loss: 5.8299\n",
      "Epoch [431/800], Step [34/53], D_loss: 0.0329, G_loss: 6.9868\n",
      "Epoch [431/800], Step [36/53], D_loss: 0.0470, G_loss: 3.7485\n",
      "Epoch [431/800], Step [38/53], D_loss: 0.0185, G_loss: 4.1941\n",
      "Epoch [431/800], Step [40/53], D_loss: 0.0117, G_loss: 6.7174\n",
      "Epoch [431/800], Step [42/53], D_loss: 0.0568, G_loss: 3.8701\n",
      "Epoch [431/800], Step [44/53], D_loss: 0.0058, G_loss: 5.6745\n",
      "Epoch [431/800], Step [46/53], D_loss: 0.0412, G_loss: 3.6861\n",
      "Epoch [431/800], Step [48/53], D_loss: 0.0660, G_loss: 7.5631\n",
      "Epoch [431/800], Step [50/53], D_loss: 0.2477, G_loss: 3.8189\n",
      "Epoch [431/800], Step [52/53], D_loss: 0.0455, G_loss: 6.5469\n",
      "Epoch [431/800], Avg D_loss: 0.0978, Avg G_loss: 6.0310\n",
      "Epoch [432/800], Step [2/53], D_loss: 0.1722, G_loss: 8.5180\n",
      "Epoch [432/800], Step [4/53], D_loss: 0.2026, G_loss: 8.8308\n",
      "Epoch [432/800], Step [6/53], D_loss: 0.0609, G_loss: 6.1946\n",
      "Epoch [432/800], Step [8/53], D_loss: 0.0127, G_loss: 6.4257\n",
      "Epoch [432/800], Step [10/53], D_loss: 0.0188, G_loss: 7.2289\n",
      "Epoch [432/800], Step [12/53], D_loss: 0.0704, G_loss: 8.6101\n",
      "Epoch [432/800], Step [14/53], D_loss: 0.0126, G_loss: 7.3257\n",
      "Epoch [432/800], Step [16/53], D_loss: 0.0750, G_loss: 5.1638\n",
      "Epoch [432/800], Step [18/53], D_loss: 0.0369, G_loss: 6.7197\n",
      "Epoch [432/800], Step [20/53], D_loss: 0.0039, G_loss: 6.8322\n",
      "Epoch [432/800], Step [22/53], D_loss: 0.0124, G_loss: 6.9553\n",
      "Epoch [432/800], Step [24/53], D_loss: 0.0324, G_loss: 6.7984\n",
      "Epoch [432/800], Step [26/53], D_loss: 0.0039, G_loss: 10.2171\n",
      "Epoch [432/800], Step [28/53], D_loss: 0.0051, G_loss: 4.3254\n",
      "Epoch [432/800], Step [30/53], D_loss: 0.0152, G_loss: 6.5732\n",
      "Epoch [432/800], Step [32/53], D_loss: 0.0119, G_loss: 7.9444\n",
      "Epoch [432/800], Step [34/53], D_loss: 0.0041, G_loss: 5.2481\n",
      "Epoch [432/800], Step [36/53], D_loss: 0.0047, G_loss: 9.8198\n",
      "Epoch [432/800], Step [38/53], D_loss: 0.0100, G_loss: 6.5635\n",
      "Epoch [432/800], Step [40/53], D_loss: 0.0624, G_loss: 8.0613\n",
      "Epoch [432/800], Step [42/53], D_loss: 0.0033, G_loss: 7.0710\n",
      "Epoch [432/800], Step [44/53], D_loss: 0.1758, G_loss: 1.9101\n",
      "Epoch [432/800], Step [46/53], D_loss: 0.2107, G_loss: 5.6129\n",
      "Epoch [432/800], Step [48/53], D_loss: 0.0301, G_loss: 6.3946\n",
      "Epoch [432/800], Step [50/53], D_loss: 0.0177, G_loss: 9.1111\n",
      "Epoch [432/800], Step [52/53], D_loss: 0.0595, G_loss: 4.2623\n",
      "Epoch [432/800], Avg D_loss: 0.0528, Avg G_loss: 6.8764\n",
      "Epoch [433/800], Step [2/53], D_loss: 0.0307, G_loss: 8.0855\n",
      "Epoch [433/800], Step [4/53], D_loss: 0.0276, G_loss: 8.8941\n",
      "Epoch [433/800], Step [6/53], D_loss: 0.1302, G_loss: 7.1016\n",
      "Epoch [433/800], Step [8/53], D_loss: 0.2369, G_loss: 8.0086\n",
      "Epoch [433/800], Step [10/53], D_loss: 0.3373, G_loss: 2.1281\n",
      "Epoch [433/800], Step [12/53], D_loss: 0.0998, G_loss: 8.1616\n",
      "Epoch [433/800], Step [14/53], D_loss: 0.0098, G_loss: 8.6320\n",
      "Epoch [433/800], Step [16/53], D_loss: 0.0159, G_loss: 7.2440\n",
      "Epoch [433/800], Step [18/53], D_loss: 0.0302, G_loss: 10.7204\n",
      "Epoch [433/800], Step [20/53], D_loss: 0.0007, G_loss: 8.8644\n",
      "Epoch [433/800], Step [22/53], D_loss: 0.0971, G_loss: 8.1842\n",
      "Epoch [433/800], Step [24/53], D_loss: 0.0033, G_loss: 4.7335\n",
      "Epoch [433/800], Step [26/53], D_loss: 0.0323, G_loss: 6.0796\n",
      "Epoch [433/800], Step [28/53], D_loss: 0.0008, G_loss: 13.4084\n",
      "Epoch [433/800], Step [30/53], D_loss: 0.0358, G_loss: 8.5406\n",
      "Epoch [433/800], Step [32/53], D_loss: 0.0016, G_loss: 8.8436\n",
      "Epoch [433/800], Step [34/53], D_loss: 0.2030, G_loss: 11.2108\n",
      "Epoch [433/800], Step [36/53], D_loss: 0.0444, G_loss: 8.5766\n",
      "Epoch [433/800], Step [38/53], D_loss: 0.0037, G_loss: 6.3665\n",
      "Epoch [433/800], Step [40/53], D_loss: 0.0006, G_loss: 6.6295\n",
      "Epoch [433/800], Step [42/53], D_loss: 0.0729, G_loss: 4.6874\n",
      "Epoch [433/800], Step [44/53], D_loss: 0.0027, G_loss: 7.2443\n",
      "Epoch [433/800], Step [46/53], D_loss: 0.1027, G_loss: 8.7940\n",
      "Epoch [433/800], Step [48/53], D_loss: 0.0067, G_loss: 7.1041\n",
      "Epoch [433/800], Step [50/53], D_loss: 0.0166, G_loss: 6.0390\n",
      "Epoch [433/800], Step [52/53], D_loss: 0.0688, G_loss: 7.2319\n",
      "Epoch [433/800], Avg D_loss: 0.0528, Avg G_loss: 7.4486\n",
      "Epoch [434/800], Step [2/53], D_loss: 0.0152, G_loss: 6.4695\n",
      "Epoch [434/800], Step [4/53], D_loss: 0.0129, G_loss: 5.0949\n",
      "Epoch [434/800], Step [6/53], D_loss: 0.1297, G_loss: 7.0149\n",
      "Epoch [434/800], Step [8/53], D_loss: 0.0306, G_loss: 11.0142\n",
      "Epoch [434/800], Step [10/53], D_loss: 0.0060, G_loss: 11.2943\n",
      "Epoch [434/800], Step [12/53], D_loss: 0.0578, G_loss: 7.6553\n",
      "Epoch [434/800], Step [14/53], D_loss: 0.0288, G_loss: 6.9751\n",
      "Epoch [434/800], Step [16/53], D_loss: 0.0070, G_loss: 5.3553\n",
      "Epoch [434/800], Step [18/53], D_loss: 0.0116, G_loss: 10.2663\n",
      "Epoch [434/800], Step [20/53], D_loss: 0.0134, G_loss: 10.9009\n",
      "Epoch [434/800], Step [22/53], D_loss: 0.0075, G_loss: 5.9778\n",
      "Epoch [434/800], Step [24/53], D_loss: 0.0006, G_loss: 9.1494\n",
      "Epoch [434/800], Step [26/53], D_loss: 0.1946, G_loss: 7.4756\n",
      "Epoch [434/800], Step [28/53], D_loss: 0.2072, G_loss: 6.7163\n",
      "Epoch [434/800], Step [30/53], D_loss: 0.0184, G_loss: 12.0057\n",
      "Epoch [434/800], Step [32/53], D_loss: 0.0216, G_loss: 5.3216\n",
      "Epoch [434/800], Step [34/53], D_loss: 0.2043, G_loss: 6.9953\n",
      "Epoch [434/800], Step [36/53], D_loss: 0.0417, G_loss: 8.8937\n",
      "Epoch [434/800], Step [38/53], D_loss: 0.2508, G_loss: 9.0574\n",
      "Epoch [434/800], Step [40/53], D_loss: 0.0114, G_loss: 5.2212\n",
      "Epoch [434/800], Step [42/53], D_loss: 0.0240, G_loss: 6.7125\n",
      "Epoch [434/800], Step [44/53], D_loss: 0.0436, G_loss: 5.4350\n",
      "Epoch [434/800], Step [46/53], D_loss: 0.0006, G_loss: 10.3827\n",
      "Epoch [434/800], Step [48/53], D_loss: 0.0157, G_loss: 10.8102\n",
      "Epoch [434/800], Step [50/53], D_loss: 0.0214, G_loss: 5.8063\n",
      "Epoch [434/800], Step [52/53], D_loss: 0.0649, G_loss: 9.3631\n",
      "Epoch [434/800], Avg D_loss: 0.0557, Avg G_loss: 8.2663\n",
      "Epoch [435/800], Step [2/53], D_loss: 0.0248, G_loss: 3.6653\n",
      "Epoch [435/800], Step [4/53], D_loss: 0.0004, G_loss: 8.5245\n",
      "Epoch [435/800], Step [6/53], D_loss: 0.0055, G_loss: 6.4474\n",
      "Epoch [435/800], Step [8/53], D_loss: 0.1843, G_loss: 8.4210\n",
      "Epoch [435/800], Step [10/53], D_loss: 0.1687, G_loss: 7.8455\n",
      "Epoch [435/800], Step [12/53], D_loss: 0.0020, G_loss: 8.4575\n",
      "Epoch [435/800], Step [14/53], D_loss: 0.0065, G_loss: 9.0275\n",
      "Epoch [435/800], Step [16/53], D_loss: 0.0124, G_loss: 10.8287\n",
      "Epoch [435/800], Step [18/53], D_loss: 0.0235, G_loss: 6.7888\n",
      "Epoch [435/800], Step [20/53], D_loss: 0.0004, G_loss: 11.4733\n",
      "Epoch [435/800], Step [22/53], D_loss: 0.0425, G_loss: 8.4462\n",
      "Epoch [435/800], Step [24/53], D_loss: 0.0716, G_loss: 6.3251\n",
      "Epoch [435/800], Step [26/53], D_loss: 0.0067, G_loss: 11.4949\n",
      "Epoch [435/800], Step [28/53], D_loss: 0.0043, G_loss: 8.9263\n",
      "Epoch [435/800], Step [30/53], D_loss: 0.1007, G_loss: 9.0916\n",
      "Epoch [435/800], Step [32/53], D_loss: 0.0774, G_loss: 9.7953\n",
      "Epoch [435/800], Step [34/53], D_loss: 0.0831, G_loss: 5.8071\n",
      "Epoch [435/800], Step [36/53], D_loss: 0.0008, G_loss: 7.3975\n",
      "Epoch [435/800], Step [38/53], D_loss: 0.0055, G_loss: 9.7541\n",
      "Epoch [435/800], Step [40/53], D_loss: 0.0007, G_loss: 5.4814\n",
      "Epoch [435/800], Step [42/53], D_loss: 0.0485, G_loss: 5.2107\n",
      "Epoch [435/800], Step [44/53], D_loss: 0.1159, G_loss: 11.8571\n",
      "Epoch [435/800], Step [46/53], D_loss: 0.0020, G_loss: 11.3378\n",
      "Epoch [435/800], Step [48/53], D_loss: 0.0037, G_loss: 8.8683\n",
      "Epoch [435/800], Step [50/53], D_loss: 0.0225, G_loss: 10.6591\n",
      "Epoch [435/800], Step [52/53], D_loss: 0.0113, G_loss: 12.5267\n",
      "Epoch [435/800], Avg D_loss: 0.0342, Avg G_loss: 8.5291\n",
      "Epoch [436/800], Step [2/53], D_loss: 0.0299, G_loss: 9.0649\n",
      "Epoch [436/800], Step [4/53], D_loss: 0.1426, G_loss: 9.3053\n",
      "Epoch [436/800], Step [6/53], D_loss: 0.0030, G_loss: 6.2452\n",
      "Epoch [436/800], Step [8/53], D_loss: 0.0264, G_loss: 11.9659\n",
      "Epoch [436/800], Step [10/53], D_loss: 0.1039, G_loss: 11.7719\n",
      "Epoch [436/800], Step [12/53], D_loss: 0.0139, G_loss: 10.9807\n",
      "Epoch [436/800], Step [14/53], D_loss: 0.0026, G_loss: 10.1739\n",
      "Epoch [436/800], Step [16/53], D_loss: 0.0110, G_loss: 10.9616\n",
      "Epoch [436/800], Step [18/53], D_loss: 0.0702, G_loss: 9.5314\n",
      "Epoch [436/800], Step [20/53], D_loss: 0.0082, G_loss: 10.1211\n",
      "Epoch [436/800], Step [22/53], D_loss: 0.0517, G_loss: 6.6211\n",
      "Epoch [436/800], Step [24/53], D_loss: 0.0250, G_loss: 6.4279\n",
      "Epoch [436/800], Step [26/53], D_loss: 0.1262, G_loss: 14.7825\n",
      "Epoch [436/800], Step [28/53], D_loss: 0.7829, G_loss: 0.4640\n",
      "Epoch [436/800], Step [30/53], D_loss: 1.1863, G_loss: 11.7011\n",
      "Epoch [436/800], Step [32/53], D_loss: 1.3201, G_loss: 7.6830\n",
      "Epoch [436/800], Step [34/53], D_loss: 0.1690, G_loss: 3.6106\n",
      "Epoch [436/800], Step [36/53], D_loss: 0.0622, G_loss: 6.1899\n",
      "Epoch [436/800], Step [38/53], D_loss: 0.0365, G_loss: 2.7110\n",
      "Epoch [436/800], Step [40/53], D_loss: 0.0294, G_loss: 3.2967\n",
      "Epoch [436/800], Step [42/53], D_loss: 0.3029, G_loss: 7.3023\n",
      "Epoch [436/800], Step [44/53], D_loss: 0.9407, G_loss: 3.1750\n",
      "Epoch [436/800], Step [46/53], D_loss: 0.1636, G_loss: 2.5097\n",
      "Epoch [436/800], Step [48/53], D_loss: 0.0481, G_loss: 6.0372\n",
      "Epoch [436/800], Step [50/53], D_loss: 0.1192, G_loss: 2.2818\n",
      "Epoch [436/800], Step [52/53], D_loss: 0.0509, G_loss: 4.8707\n",
      "Epoch [436/800], Avg D_loss: 0.1402, Avg G_loss: 7.6820\n",
      "Epoch [437/800], Step [2/53], D_loss: 0.0046, G_loss: 6.3391\n",
      "Epoch [437/800], Step [4/53], D_loss: 0.0533, G_loss: 4.3007\n",
      "Epoch [437/800], Step [6/53], D_loss: 0.1764, G_loss: 6.2169\n",
      "Epoch [437/800], Step [8/53], D_loss: 0.1098, G_loss: 6.0732\n",
      "Epoch [437/800], Step [10/53], D_loss: 0.0861, G_loss: 3.8483\n",
      "Epoch [437/800], Step [12/53], D_loss: 0.0205, G_loss: 2.9425\n",
      "Epoch [437/800], Step [14/53], D_loss: 0.0459, G_loss: 9.3033\n",
      "Epoch [437/800], Step [16/53], D_loss: 0.1091, G_loss: 4.6710\n",
      "Epoch [437/800], Step [18/53], D_loss: 0.0363, G_loss: 3.4634\n",
      "Epoch [437/800], Step [20/53], D_loss: 0.0260, G_loss: 5.2847\n",
      "Epoch [437/800], Step [22/53], D_loss: 0.0145, G_loss: 9.0670\n",
      "Epoch [437/800], Step [24/53], D_loss: 0.0333, G_loss: 5.6431\n",
      "Epoch [437/800], Step [26/53], D_loss: 0.0109, G_loss: 7.6644\n",
      "Epoch [437/800], Step [28/53], D_loss: 0.0160, G_loss: 3.7467\n",
      "Epoch [437/800], Step [30/53], D_loss: 0.2248, G_loss: 3.0232\n",
      "Epoch [437/800], Step [32/53], D_loss: 0.1432, G_loss: 6.4475\n",
      "Epoch [437/800], Step [34/53], D_loss: 0.0055, G_loss: 6.1046\n",
      "Epoch [437/800], Step [36/53], D_loss: 0.0490, G_loss: 4.0155\n",
      "Epoch [437/800], Step [38/53], D_loss: 0.0223, G_loss: 8.2731\n",
      "Epoch [437/800], Step [40/53], D_loss: 0.0119, G_loss: 4.5428\n",
      "Epoch [437/800], Step [42/53], D_loss: 0.0137, G_loss: 4.1399\n",
      "Epoch [437/800], Step [44/53], D_loss: 0.1156, G_loss: 7.5069\n",
      "Epoch [437/800], Step [46/53], D_loss: 0.0047, G_loss: 6.7272\n",
      "Epoch [437/800], Step [48/53], D_loss: 0.0118, G_loss: 6.3304\n",
      "Epoch [437/800], Step [50/53], D_loss: 0.5469, G_loss: 13.0360\n",
      "Epoch [437/800], Step [52/53], D_loss: 0.4954, G_loss: 6.3242\n",
      "Epoch [437/800], Avg D_loss: 0.0892, Avg G_loss: 5.9615\n",
      "Epoch [438/800], Step [2/53], D_loss: 0.1130, G_loss: 9.2073\n",
      "Epoch [438/800], Step [4/53], D_loss: 0.0350, G_loss: 5.9421\n",
      "Epoch [438/800], Step [6/53], D_loss: 0.1142, G_loss: 4.0532\n",
      "Epoch [438/800], Step [8/53], D_loss: 0.1614, G_loss: 3.4280\n",
      "Epoch [438/800], Step [10/53], D_loss: 0.0391, G_loss: 8.4651\n",
      "Epoch [438/800], Step [12/53], D_loss: 0.0080, G_loss: 6.5241\n",
      "Epoch [438/800], Step [14/53], D_loss: 0.0994, G_loss: 9.3440\n",
      "Epoch [438/800], Step [16/53], D_loss: 0.0286, G_loss: 3.6188\n",
      "Epoch [438/800], Step [18/53], D_loss: 0.0173, G_loss: 4.2640\n",
      "Epoch [438/800], Step [20/53], D_loss: 0.0070, G_loss: 5.3567\n",
      "Epoch [438/800], Step [22/53], D_loss: 0.0528, G_loss: 7.1395\n",
      "Epoch [438/800], Step [24/53], D_loss: 0.0674, G_loss: 8.9807\n",
      "Epoch [438/800], Step [26/53], D_loss: 0.1661, G_loss: 7.2899\n",
      "Epoch [438/800], Step [28/53], D_loss: 0.0099, G_loss: 7.4580\n",
      "Epoch [438/800], Step [30/53], D_loss: 0.0100, G_loss: 7.4325\n",
      "Epoch [438/800], Step [32/53], D_loss: 0.0061, G_loss: 9.6543\n",
      "Epoch [438/800], Step [34/53], D_loss: 0.0496, G_loss: 4.8092\n",
      "Epoch [438/800], Step [36/53], D_loss: 0.0350, G_loss: 7.2456\n",
      "Epoch [438/800], Step [38/53], D_loss: 0.0689, G_loss: 6.7179\n",
      "Epoch [438/800], Step [40/53], D_loss: 0.1951, G_loss: 7.5379\n",
      "Epoch [438/800], Step [42/53], D_loss: 0.0242, G_loss: 4.3172\n",
      "Epoch [438/800], Step [44/53], D_loss: 0.0424, G_loss: 9.9614\n",
      "Epoch [438/800], Step [46/53], D_loss: 0.0092, G_loss: 7.1671\n",
      "Epoch [438/800], Step [48/53], D_loss: 0.0173, G_loss: 6.7387\n",
      "Epoch [438/800], Step [50/53], D_loss: 0.0008, G_loss: 7.0490\n",
      "Epoch [438/800], Step [52/53], D_loss: 0.0104, G_loss: 13.0811\n",
      "Epoch [438/800], Avg D_loss: 0.0848, Avg G_loss: 6.6046\n",
      "Epoch [439/800], Step [2/53], D_loss: 0.7717, G_loss: 9.5260\n",
      "Epoch [439/800], Step [4/53], D_loss: 0.3631, G_loss: 6.2171\n",
      "Epoch [439/800], Step [6/53], D_loss: 0.0802, G_loss: 6.7369\n",
      "Epoch [439/800], Step [8/53], D_loss: 0.0314, G_loss: 5.8808\n",
      "Epoch [439/800], Step [10/53], D_loss: 0.0283, G_loss: 4.3710\n",
      "Epoch [439/800], Step [12/53], D_loss: 0.0402, G_loss: 2.7246\n",
      "Epoch [439/800], Step [14/53], D_loss: 0.0410, G_loss: 3.8668\n",
      "Epoch [439/800], Step [16/53], D_loss: 0.0102, G_loss: 5.5737\n",
      "Epoch [439/800], Step [18/53], D_loss: 0.0158, G_loss: 4.3165\n",
      "Epoch [439/800], Step [20/53], D_loss: 0.1161, G_loss: 3.2534\n",
      "Epoch [439/800], Step [22/53], D_loss: 0.0331, G_loss: 4.2364\n",
      "Epoch [439/800], Step [24/53], D_loss: 0.0045, G_loss: 7.1214\n",
      "Epoch [439/800], Step [26/53], D_loss: 0.0577, G_loss: 4.3313\n",
      "Epoch [439/800], Step [28/53], D_loss: 0.0186, G_loss: 9.6500\n",
      "Epoch [439/800], Step [30/53], D_loss: 0.1365, G_loss: 6.7680\n",
      "Epoch [439/800], Step [32/53], D_loss: 0.0147, G_loss: 4.5424\n",
      "Epoch [439/800], Step [34/53], D_loss: 0.0217, G_loss: 8.5221\n",
      "Epoch [439/800], Step [36/53], D_loss: 0.1244, G_loss: 3.5218\n",
      "Epoch [439/800], Step [38/53], D_loss: 0.0382, G_loss: 4.8116\n",
      "Epoch [439/800], Step [40/53], D_loss: 0.0115, G_loss: 9.9697\n",
      "Epoch [439/800], Step [42/53], D_loss: 0.0490, G_loss: 6.3411\n",
      "Epoch [439/800], Step [44/53], D_loss: 0.0443, G_loss: 8.8179\n",
      "Epoch [439/800], Step [46/53], D_loss: 0.0068, G_loss: 6.0417\n",
      "Epoch [439/800], Step [48/53], D_loss: 0.0126, G_loss: 10.2664\n",
      "Epoch [439/800], Step [50/53], D_loss: 0.2686, G_loss: 5.0397\n",
      "Epoch [439/800], Step [52/53], D_loss: 0.0065, G_loss: 10.6291\n",
      "Epoch [439/800], Avg D_loss: 0.0897, Avg G_loss: 6.1385\n",
      "Epoch [440/800], Step [2/53], D_loss: 0.0117, G_loss: 8.0652\n",
      "Epoch [440/800], Step [4/53], D_loss: 0.0092, G_loss: 9.2399\n",
      "Epoch [440/800], Step [6/53], D_loss: 0.0101, G_loss: 7.6769\n",
      "Epoch [440/800], Step [8/53], D_loss: 0.1248, G_loss: 4.5131\n",
      "Epoch [440/800], Step [10/53], D_loss: 0.0118, G_loss: 10.4832\n",
      "Epoch [440/800], Step [12/53], D_loss: 0.0109, G_loss: 6.6421\n",
      "Epoch [440/800], Step [14/53], D_loss: 0.0228, G_loss: 8.1206\n",
      "Epoch [440/800], Step [16/53], D_loss: 0.0114, G_loss: 7.6562\n",
      "Epoch [440/800], Step [18/53], D_loss: 0.0144, G_loss: 6.9545\n",
      "Epoch [440/800], Step [20/53], D_loss: 0.0204, G_loss: 7.7347\n",
      "Epoch [440/800], Step [22/53], D_loss: 0.0220, G_loss: 9.1049\n",
      "Epoch [440/800], Step [24/53], D_loss: 0.0023, G_loss: 10.0734\n",
      "Epoch [440/800], Step [26/53], D_loss: 0.0086, G_loss: 7.1128\n",
      "Epoch [440/800], Step [28/53], D_loss: 0.0099, G_loss: 5.9713\n",
      "Epoch [440/800], Step [30/53], D_loss: 0.0611, G_loss: 7.1511\n",
      "Epoch [440/800], Step [32/53], D_loss: 0.0022, G_loss: 8.9299\n",
      "Epoch [440/800], Step [34/53], D_loss: 0.0340, G_loss: 6.6608\n",
      "Epoch [440/800], Step [36/53], D_loss: 0.0260, G_loss: 8.9855\n",
      "Epoch [440/800], Step [38/53], D_loss: 0.0112, G_loss: 7.4439\n",
      "Epoch [440/800], Step [40/53], D_loss: 0.0026, G_loss: 9.5289\n",
      "Epoch [440/800], Step [42/53], D_loss: 0.0113, G_loss: 6.7988\n",
      "Epoch [440/800], Step [44/53], D_loss: 0.0012, G_loss: 7.5313\n",
      "Epoch [440/800], Step [46/53], D_loss: 0.0005, G_loss: 8.7993\n",
      "Epoch [440/800], Step [48/53], D_loss: 0.0024, G_loss: 6.5519\n",
      "Epoch [440/800], Step [50/53], D_loss: 0.0053, G_loss: 8.4323\n",
      "Epoch [440/800], Step [52/53], D_loss: 0.0221, G_loss: 4.9127\n",
      "Epoch [440/800], Avg D_loss: 0.0298, Avg G_loss: 7.5544\n",
      "Generated images saved as generated_images/epoch_440.png\n",
      "Epoch [441/800], Step [2/53], D_loss: 0.0042, G_loss: 11.8820\n",
      "Epoch [441/800], Step [4/53], D_loss: 0.0043, G_loss: 10.4550\n",
      "Epoch [441/800], Step [6/53], D_loss: 0.0039, G_loss: 7.4670\n",
      "Epoch [441/800], Step [8/53], D_loss: 0.0167, G_loss: 10.9638\n",
      "Epoch [441/800], Step [10/53], D_loss: 0.2396, G_loss: 12.1301\n",
      "Epoch [441/800], Step [12/53], D_loss: 0.0021, G_loss: 12.8593\n",
      "Epoch [441/800], Step [14/53], D_loss: 0.1287, G_loss: 1.2465\n",
      "Epoch [441/800], Step [16/53], D_loss: 0.3455, G_loss: 6.4356\n",
      "Epoch [441/800], Step [18/53], D_loss: 0.0344, G_loss: 10.2597\n",
      "Epoch [441/800], Step [20/53], D_loss: 0.0462, G_loss: 5.3067\n",
      "Epoch [441/800], Step [22/53], D_loss: 0.0399, G_loss: 7.7738\n",
      "Epoch [441/800], Step [24/53], D_loss: 0.0058, G_loss: 7.8884\n",
      "Epoch [441/800], Step [26/53], D_loss: 0.0018, G_loss: 13.5946\n",
      "Epoch [441/800], Step [28/53], D_loss: 0.0187, G_loss: 6.6066\n",
      "Epoch [441/800], Step [30/53], D_loss: 0.0028, G_loss: 8.1176\n",
      "Epoch [441/800], Step [32/53], D_loss: 0.0074, G_loss: 10.2657\n",
      "Epoch [441/800], Step [34/53], D_loss: 0.0064, G_loss: 6.9413\n",
      "Epoch [441/800], Step [36/53], D_loss: 0.0009, G_loss: 8.2807\n",
      "Epoch [441/800], Step [38/53], D_loss: 0.0092, G_loss: 5.1838\n",
      "Epoch [441/800], Step [40/53], D_loss: 0.1319, G_loss: 12.4552\n",
      "Epoch [441/800], Step [42/53], D_loss: 0.0694, G_loss: 10.2782\n",
      "Epoch [441/800], Step [44/53], D_loss: 0.1477, G_loss: 4.4885\n",
      "Epoch [441/800], Step [46/53], D_loss: 0.2756, G_loss: 13.9319\n",
      "Epoch [441/800], Step [48/53], D_loss: 0.0842, G_loss: 9.3336\n",
      "Epoch [441/800], Step [50/53], D_loss: 0.0002, G_loss: 8.4932\n",
      "Epoch [441/800], Step [52/53], D_loss: 0.0040, G_loss: 7.9532\n",
      "Epoch [441/800], Avg D_loss: 0.0520, Avg G_loss: 8.7242\n",
      "Epoch [442/800], Step [2/53], D_loss: 0.1878, G_loss: 7.7896\n",
      "Epoch [442/800], Step [4/53], D_loss: 0.0116, G_loss: 10.4117\n",
      "Epoch [442/800], Step [6/53], D_loss: 0.0616, G_loss: 9.4067\n",
      "Epoch [442/800], Step [8/53], D_loss: 0.0732, G_loss: 5.1621\n",
      "Epoch [442/800], Step [10/53], D_loss: 0.0961, G_loss: 9.9698\n",
      "Epoch [442/800], Step [12/53], D_loss: 0.0079, G_loss: 8.9790\n",
      "Epoch [442/800], Step [14/53], D_loss: 0.2068, G_loss: 13.5674\n",
      "Epoch [442/800], Step [16/53], D_loss: 0.0408, G_loss: 5.1073\n",
      "Epoch [442/800], Step [18/53], D_loss: 0.0144, G_loss: 12.1962\n",
      "Epoch [442/800], Step [20/53], D_loss: 0.0086, G_loss: 4.7611\n",
      "Epoch [442/800], Step [22/53], D_loss: 0.0504, G_loss: 4.5264\n",
      "Epoch [442/800], Step [24/53], D_loss: 0.1043, G_loss: 9.0591\n",
      "Epoch [442/800], Step [26/53], D_loss: 0.0231, G_loss: 5.8626\n",
      "Epoch [442/800], Step [28/53], D_loss: 0.0045, G_loss: 3.3523\n",
      "Epoch [442/800], Step [30/53], D_loss: 0.0122, G_loss: 7.6346\n",
      "Epoch [442/800], Step [32/53], D_loss: 0.0005, G_loss: 9.7040\n",
      "Epoch [442/800], Step [34/53], D_loss: 0.0657, G_loss: 2.9375\n",
      "Epoch [442/800], Step [36/53], D_loss: 0.0172, G_loss: 5.4502\n",
      "Epoch [442/800], Step [38/53], D_loss: 0.0015, G_loss: 7.7650\n",
      "Epoch [442/800], Step [40/53], D_loss: 0.0141, G_loss: 7.8296\n",
      "Epoch [442/800], Step [42/53], D_loss: 0.0115, G_loss: 7.2869\n",
      "Epoch [442/800], Step [44/53], D_loss: 0.0249, G_loss: 7.9894\n",
      "Epoch [442/800], Step [46/53], D_loss: 0.0117, G_loss: 5.4168\n",
      "Epoch [442/800], Step [48/53], D_loss: 0.9214, G_loss: 11.9553\n",
      "Epoch [442/800], Step [50/53], D_loss: 0.1158, G_loss: 14.1226\n",
      "Epoch [442/800], Step [52/53], D_loss: 0.0069, G_loss: 8.2736\n",
      "Epoch [442/800], Avg D_loss: 0.0922, Avg G_loss: 8.0291\n",
      "Epoch [443/800], Step [2/53], D_loss: 0.0411, G_loss: 11.7656\n",
      "Epoch [443/800], Step [4/53], D_loss: 0.1156, G_loss: 7.6878\n",
      "Epoch [443/800], Step [6/53], D_loss: 0.0363, G_loss: 8.3395\n",
      "Epoch [443/800], Step [8/53], D_loss: 0.0128, G_loss: 5.2968\n",
      "Epoch [443/800], Step [10/53], D_loss: 0.0513, G_loss: 8.2611\n",
      "Epoch [443/800], Step [12/53], D_loss: 0.0449, G_loss: 7.1428\n",
      "Epoch [443/800], Step [14/53], D_loss: 0.0404, G_loss: 6.6490\n",
      "Epoch [443/800], Step [16/53], D_loss: 0.0072, G_loss: 7.0983\n",
      "Epoch [443/800], Step [18/53], D_loss: 0.1261, G_loss: 6.5110\n",
      "Epoch [443/800], Step [20/53], D_loss: 0.0161, G_loss: 9.5157\n",
      "Epoch [443/800], Step [22/53], D_loss: 0.0484, G_loss: 6.4210\n",
      "Epoch [443/800], Step [24/53], D_loss: 0.0773, G_loss: 8.6678\n",
      "Epoch [443/800], Step [26/53], D_loss: 0.0077, G_loss: 6.2305\n",
      "Epoch [443/800], Step [28/53], D_loss: 0.0281, G_loss: 7.2392\n",
      "Epoch [443/800], Step [30/53], D_loss: 0.1628, G_loss: 5.4467\n",
      "Epoch [443/800], Step [32/53], D_loss: 0.0576, G_loss: 6.9093\n",
      "Epoch [443/800], Step [34/53], D_loss: 0.0073, G_loss: 6.1800\n",
      "Epoch [443/800], Step [36/53], D_loss: 0.0298, G_loss: 7.9963\n",
      "Epoch [443/800], Step [38/53], D_loss: 0.0128, G_loss: 11.5943\n",
      "Epoch [443/800], Step [40/53], D_loss: 0.0334, G_loss: 8.1039\n",
      "Epoch [443/800], Step [42/53], D_loss: 0.0214, G_loss: 10.0960\n",
      "Epoch [443/800], Step [44/53], D_loss: 0.1554, G_loss: 7.6754\n",
      "Epoch [443/800], Step [46/53], D_loss: 0.1694, G_loss: 12.1317\n",
      "Epoch [443/800], Step [48/53], D_loss: 0.0016, G_loss: 13.4628\n",
      "Epoch [443/800], Step [50/53], D_loss: 0.0015, G_loss: 10.3805\n",
      "Epoch [443/800], Step [52/53], D_loss: 0.0186, G_loss: 7.5452\n",
      "Epoch [443/800], Avg D_loss: 0.0554, Avg G_loss: 7.8567\n",
      "Epoch [444/800], Step [2/53], D_loss: 0.0029, G_loss: 5.6244\n",
      "Epoch [444/800], Step [4/53], D_loss: 0.0460, G_loss: 8.9118\n",
      "Epoch [444/800], Step [6/53], D_loss: 0.0020, G_loss: 8.7026\n",
      "Epoch [444/800], Step [8/53], D_loss: 0.0120, G_loss: 3.8483\n",
      "Epoch [444/800], Step [10/53], D_loss: 0.0316, G_loss: 9.2806\n",
      "Epoch [444/800], Step [12/53], D_loss: 0.0162, G_loss: 7.5754\n",
      "Epoch [444/800], Step [14/53], D_loss: 0.0211, G_loss: 11.4649\n",
      "Epoch [444/800], Step [16/53], D_loss: 0.0842, G_loss: 9.0587\n",
      "Epoch [444/800], Step [18/53], D_loss: 0.0641, G_loss: 12.9080\n",
      "Epoch [444/800], Step [20/53], D_loss: 0.0556, G_loss: 5.6147\n",
      "Epoch [444/800], Step [22/53], D_loss: 0.1860, G_loss: 9.5633\n",
      "Epoch [444/800], Step [24/53], D_loss: 0.0084, G_loss: 11.0707\n",
      "Epoch [444/800], Step [26/53], D_loss: 0.0038, G_loss: 10.1895\n",
      "Epoch [444/800], Step [28/53], D_loss: 0.0036, G_loss: 6.6429\n",
      "Epoch [444/800], Step [30/53], D_loss: 0.0286, G_loss: 6.1763\n",
      "Epoch [444/800], Step [32/53], D_loss: 0.0269, G_loss: 6.0841\n",
      "Epoch [444/800], Step [34/53], D_loss: 0.0009, G_loss: 11.2825\n",
      "Epoch [444/800], Step [36/53], D_loss: 0.0054, G_loss: 10.2405\n",
      "Epoch [444/800], Step [38/53], D_loss: 0.0217, G_loss: 4.5150\n",
      "Epoch [444/800], Step [40/53], D_loss: 0.0241, G_loss: 9.6699\n",
      "Epoch [444/800], Step [42/53], D_loss: 0.0264, G_loss: 6.0876\n",
      "Epoch [444/800], Step [44/53], D_loss: 0.0194, G_loss: 7.0361\n",
      "Epoch [444/800], Step [46/53], D_loss: 0.3227, G_loss: 2.9597\n",
      "Epoch [444/800], Step [48/53], D_loss: 0.0070, G_loss: 8.1320\n",
      "Epoch [444/800], Step [50/53], D_loss: 0.0391, G_loss: 9.0275\n",
      "Epoch [444/800], Step [52/53], D_loss: 0.0999, G_loss: 7.6361\n",
      "Epoch [444/800], Avg D_loss: 0.0441, Avg G_loss: 8.3312\n",
      "Epoch [445/800], Step [2/53], D_loss: 0.0069, G_loss: 7.6364\n",
      "Epoch [445/800], Step [4/53], D_loss: 0.0601, G_loss: 9.7290\n",
      "Epoch [445/800], Step [6/53], D_loss: 0.0596, G_loss: 8.3524\n",
      "Epoch [445/800], Step [8/53], D_loss: 0.0007, G_loss: 6.3269\n",
      "Epoch [445/800], Step [10/53], D_loss: 0.0105, G_loss: 8.1887\n",
      "Epoch [445/800], Step [12/53], D_loss: 0.0630, G_loss: 9.6524\n",
      "Epoch [445/800], Step [14/53], D_loss: 0.0673, G_loss: 11.0049\n",
      "Epoch [445/800], Step [16/53], D_loss: 0.0523, G_loss: 10.1706\n",
      "Epoch [445/800], Step [18/53], D_loss: 0.1348, G_loss: 7.9136\n",
      "Epoch [445/800], Step [20/53], D_loss: 0.0009, G_loss: 9.9051\n",
      "Epoch [445/800], Step [22/53], D_loss: 0.1763, G_loss: 7.4407\n",
      "Epoch [445/800], Step [24/53], D_loss: 0.4481, G_loss: 7.0273\n",
      "Epoch [445/800], Step [26/53], D_loss: 0.1242, G_loss: 8.8201\n",
      "Epoch [445/800], Step [28/53], D_loss: 0.0049, G_loss: 8.7893\n",
      "Epoch [445/800], Step [30/53], D_loss: 0.0440, G_loss: 9.1472\n",
      "Epoch [445/800], Step [32/53], D_loss: 0.0066, G_loss: 4.2209\n",
      "Epoch [445/800], Step [34/53], D_loss: 0.4442, G_loss: 10.7328\n",
      "Epoch [445/800], Step [36/53], D_loss: 0.5552, G_loss: 7.2723\n",
      "Epoch [445/800], Step [38/53], D_loss: 0.0666, G_loss: 2.4171\n",
      "Epoch [445/800], Step [40/53], D_loss: 1.0714, G_loss: 7.5092\n",
      "Epoch [445/800], Step [42/53], D_loss: 0.1045, G_loss: 3.9098\n",
      "Epoch [445/800], Step [44/53], D_loss: 0.0283, G_loss: 5.4097\n",
      "Epoch [445/800], Step [46/53], D_loss: 0.0933, G_loss: 3.5260\n",
      "Epoch [445/800], Step [48/53], D_loss: 0.0610, G_loss: 4.7034\n",
      "Epoch [445/800], Step [50/53], D_loss: 0.0266, G_loss: 5.8072\n",
      "Epoch [445/800], Step [52/53], D_loss: 0.0683, G_loss: 5.7020\n",
      "Epoch [445/800], Avg D_loss: 0.1262, Avg G_loss: 7.3874\n",
      "Epoch [446/800], Step [2/53], D_loss: 0.3473, G_loss: 5.9921\n",
      "Epoch [446/800], Step [4/53], D_loss: 0.0455, G_loss: 6.7205\n",
      "Epoch [446/800], Step [6/53], D_loss: 0.0121, G_loss: 3.3070\n",
      "Epoch [446/800], Step [8/53], D_loss: 0.0737, G_loss: 4.8824\n",
      "Epoch [446/800], Step [10/53], D_loss: 0.0479, G_loss: 6.5724\n",
      "Epoch [446/800], Step [12/53], D_loss: 0.0147, G_loss: 4.8180\n",
      "Epoch [446/800], Step [14/53], D_loss: 0.0534, G_loss: 2.7372\n",
      "Epoch [446/800], Step [16/53], D_loss: 0.0387, G_loss: 5.0460\n",
      "Epoch [446/800], Step [18/53], D_loss: 0.0784, G_loss: 6.7458\n",
      "Epoch [446/800], Step [20/53], D_loss: 0.0669, G_loss: 5.1579\n",
      "Epoch [446/800], Step [22/53], D_loss: 0.0067, G_loss: 4.3767\n",
      "Epoch [446/800], Step [24/53], D_loss: 0.1134, G_loss: 7.4041\n",
      "Epoch [446/800], Step [26/53], D_loss: 0.0099, G_loss: 3.4778\n",
      "Epoch [446/800], Step [28/53], D_loss: 0.0319, G_loss: 8.3350\n",
      "Epoch [446/800], Step [30/53], D_loss: 0.0756, G_loss: 3.7720\n",
      "Epoch [446/800], Step [32/53], D_loss: 0.0152, G_loss: 8.4639\n",
      "Epoch [446/800], Step [34/53], D_loss: 0.1940, G_loss: 10.0701\n",
      "Epoch [446/800], Step [36/53], D_loss: 0.0083, G_loss: 11.4395\n",
      "Epoch [446/800], Step [38/53], D_loss: 0.0191, G_loss: 2.6656\n",
      "Epoch [446/800], Step [40/53], D_loss: 0.0069, G_loss: 7.7905\n",
      "Epoch [446/800], Step [42/53], D_loss: 0.1720, G_loss: 9.0751\n",
      "Epoch [446/800], Step [44/53], D_loss: 0.0188, G_loss: 4.8898\n",
      "Epoch [446/800], Step [46/53], D_loss: 0.0052, G_loss: 8.6501\n",
      "Epoch [446/800], Step [48/53], D_loss: 0.0112, G_loss: 10.0733\n",
      "Epoch [446/800], Step [50/53], D_loss: 0.0278, G_loss: 6.9319\n",
      "Epoch [446/800], Step [52/53], D_loss: 0.0082, G_loss: 8.1715\n",
      "Epoch [446/800], Avg D_loss: 0.0773, Avg G_loss: 6.6442\n",
      "Epoch [447/800], Step [2/53], D_loss: 0.0243, G_loss: 5.0874\n",
      "Epoch [447/800], Step [4/53], D_loss: 0.0071, G_loss: 6.1584\n",
      "Epoch [447/800], Step [6/53], D_loss: 0.0534, G_loss: 6.9366\n",
      "Epoch [447/800], Step [8/53], D_loss: 0.0202, G_loss: 9.5230\n",
      "Epoch [447/800], Step [10/53], D_loss: 0.0240, G_loss: 0.5684\n",
      "Epoch [447/800], Step [12/53], D_loss: 0.0914, G_loss: 11.8996\n",
      "Epoch [447/800], Step [14/53], D_loss: 0.0293, G_loss: 6.1336\n",
      "Epoch [447/800], Step [16/53], D_loss: 0.0779, G_loss: 6.9542\n",
      "Epoch [447/800], Step [18/53], D_loss: 0.0756, G_loss: 4.3061\n",
      "Epoch [447/800], Step [20/53], D_loss: 0.0295, G_loss: 7.3858\n",
      "Epoch [447/800], Step [22/53], D_loss: 0.0033, G_loss: 8.2160\n",
      "Epoch [447/800], Step [24/53], D_loss: 0.0096, G_loss: 10.2192\n",
      "Epoch [447/800], Step [26/53], D_loss: 0.0348, G_loss: 5.8526\n",
      "Epoch [447/800], Step [28/53], D_loss: 0.2125, G_loss: 9.3061\n",
      "Epoch [447/800], Step [30/53], D_loss: 0.1420, G_loss: 6.3272\n",
      "Epoch [447/800], Step [32/53], D_loss: 0.2011, G_loss: 4.1379\n",
      "Epoch [447/800], Step [34/53], D_loss: 0.0423, G_loss: 3.6128\n",
      "Epoch [447/800], Step [36/53], D_loss: 0.5710, G_loss: 11.5343\n",
      "Epoch [447/800], Step [38/53], D_loss: 0.1329, G_loss: 5.8034\n",
      "Epoch [447/800], Step [40/53], D_loss: 0.0692, G_loss: 2.5607\n",
      "Epoch [447/800], Step [42/53], D_loss: 0.0157, G_loss: 5.0929\n",
      "Epoch [447/800], Step [44/53], D_loss: 0.1405, G_loss: 1.6307\n",
      "Epoch [447/800], Step [46/53], D_loss: 0.1129, G_loss: 2.6989\n",
      "Epoch [447/800], Step [48/53], D_loss: 0.0867, G_loss: 6.4259\n",
      "Epoch [447/800], Step [50/53], D_loss: 0.0431, G_loss: 1.1285\n",
      "Epoch [447/800], Step [52/53], D_loss: 0.0156, G_loss: 7.3077\n",
      "Epoch [447/800], Avg D_loss: 0.1469, Avg G_loss: 5.9124\n",
      "Epoch [448/800], Step [2/53], D_loss: 0.0939, G_loss: 6.2963\n",
      "Epoch [448/800], Step [4/53], D_loss: 0.8674, G_loss: 3.6908\n",
      "Epoch [448/800], Step [6/53], D_loss: 0.4079, G_loss: 3.4304\n",
      "Epoch [448/800], Step [8/53], D_loss: 0.2014, G_loss: 2.5560\n",
      "Epoch [448/800], Step [10/53], D_loss: 0.2731, G_loss: 3.2086\n",
      "Epoch [448/800], Step [12/53], D_loss: 0.1063, G_loss: 2.6073\n",
      "Epoch [448/800], Step [14/53], D_loss: 0.3182, G_loss: 3.6245\n",
      "Epoch [448/800], Step [16/53], D_loss: 0.1252, G_loss: 4.7194\n",
      "Epoch [448/800], Step [18/53], D_loss: 0.2218, G_loss: 2.7175\n",
      "Epoch [448/800], Step [20/53], D_loss: 0.5241, G_loss: 4.8851\n",
      "Epoch [448/800], Step [22/53], D_loss: 0.0941, G_loss: 6.5743\n",
      "Epoch [448/800], Step [24/53], D_loss: 0.0283, G_loss: 4.8800\n",
      "Epoch [448/800], Step [26/53], D_loss: 0.4643, G_loss: 3.8230\n",
      "Epoch [448/800], Step [28/53], D_loss: 0.0693, G_loss: 3.1881\n",
      "Epoch [448/800], Step [30/53], D_loss: 0.0223, G_loss: 6.5198\n",
      "Epoch [448/800], Step [32/53], D_loss: 0.1592, G_loss: 3.6027\n",
      "Epoch [448/800], Step [34/53], D_loss: 0.0719, G_loss: 7.3065\n",
      "Epoch [448/800], Step [36/53], D_loss: 0.0152, G_loss: 4.1343\n",
      "Epoch [448/800], Step [38/53], D_loss: 0.0414, G_loss: 6.1152\n",
      "Epoch [448/800], Step [40/53], D_loss: 0.2213, G_loss: 3.6666\n",
      "Epoch [448/800], Step [42/53], D_loss: 0.8964, G_loss: 9.0794\n",
      "Epoch [448/800], Step [44/53], D_loss: 0.1956, G_loss: 4.4933\n",
      "Epoch [448/800], Step [46/53], D_loss: 0.1057, G_loss: 6.7724\n",
      "Epoch [448/800], Step [48/53], D_loss: 0.1404, G_loss: 5.3201\n",
      "Epoch [448/800], Step [50/53], D_loss: 0.1734, G_loss: 1.3688\n",
      "Epoch [448/800], Step [52/53], D_loss: 0.4724, G_loss: 5.1892\n",
      "Epoch [448/800], Avg D_loss: 0.2006, Avg G_loss: 4.4274\n",
      "Epoch [449/800], Step [2/53], D_loss: 0.2932, G_loss: 5.8911\n",
      "Epoch [449/800], Step [4/53], D_loss: 0.0128, G_loss: 3.0686\n",
      "Epoch [449/800], Step [6/53], D_loss: 0.0189, G_loss: 2.9741\n",
      "Epoch [449/800], Step [8/53], D_loss: 0.0250, G_loss: 5.6795\n",
      "Epoch [449/800], Step [10/53], D_loss: 0.4218, G_loss: 2.8217\n",
      "Epoch [449/800], Step [12/53], D_loss: 0.1938, G_loss: 4.1994\n",
      "Epoch [449/800], Step [14/53], D_loss: 0.1395, G_loss: 8.1589\n",
      "Epoch [449/800], Step [16/53], D_loss: 0.3071, G_loss: 4.9399\n",
      "Epoch [449/800], Step [18/53], D_loss: 0.0245, G_loss: 6.4288\n",
      "Epoch [449/800], Step [20/53], D_loss: 0.0641, G_loss: 3.8836\n",
      "Epoch [449/800], Step [22/53], D_loss: 0.1513, G_loss: 8.0049\n",
      "Epoch [449/800], Step [24/53], D_loss: 0.0845, G_loss: 6.8169\n",
      "Epoch [449/800], Step [26/53], D_loss: 0.0055, G_loss: 4.9996\n",
      "Epoch [449/800], Step [28/53], D_loss: 0.0977, G_loss: 5.6336\n",
      "Epoch [449/800], Step [30/53], D_loss: 0.0101, G_loss: 4.6074\n",
      "Epoch [449/800], Step [32/53], D_loss: 0.0343, G_loss: 4.7649\n",
      "Epoch [449/800], Step [34/53], D_loss: 0.0215, G_loss: 6.9721\n",
      "Epoch [449/800], Step [36/53], D_loss: 0.5379, G_loss: 4.6086\n",
      "Epoch [449/800], Step [38/53], D_loss: 0.0108, G_loss: 5.9871\n",
      "Epoch [449/800], Step [40/53], D_loss: 0.0841, G_loss: 4.4677\n",
      "Epoch [449/800], Step [42/53], D_loss: 0.1120, G_loss: 5.6850\n",
      "Epoch [449/800], Step [44/53], D_loss: 0.1495, G_loss: 2.1957\n",
      "Epoch [449/800], Step [46/53], D_loss: 0.0018, G_loss: 4.0205\n",
      "Epoch [449/800], Step [48/53], D_loss: 0.0393, G_loss: 4.9083\n",
      "Epoch [449/800], Step [50/53], D_loss: 0.0401, G_loss: 3.7382\n",
      "Epoch [449/800], Step [52/53], D_loss: 0.0230, G_loss: 3.1864\n",
      "Epoch [449/800], Avg D_loss: 0.1319, Avg G_loss: 4.9074\n",
      "Epoch [450/800], Step [2/53], D_loss: 0.1825, G_loss: 4.7109\n",
      "Epoch [450/800], Step [4/53], D_loss: 0.1340, G_loss: 3.1616\n",
      "Epoch [450/800], Step [6/53], D_loss: 0.1437, G_loss: 4.7902\n",
      "Epoch [450/800], Step [8/53], D_loss: 0.0828, G_loss: 7.5770\n",
      "Epoch [450/800], Step [10/53], D_loss: 0.0360, G_loss: 1.6997\n",
      "Epoch [450/800], Step [12/53], D_loss: 0.0094, G_loss: 6.5506\n",
      "Epoch [450/800], Step [14/53], D_loss: 0.3273, G_loss: 5.1508\n",
      "Epoch [450/800], Step [16/53], D_loss: 0.0371, G_loss: 2.4193\n",
      "Epoch [450/800], Step [18/53], D_loss: 0.0106, G_loss: 3.8587\n",
      "Epoch [450/800], Step [20/53], D_loss: 0.0197, G_loss: 5.3794\n",
      "Epoch [450/800], Step [22/53], D_loss: 0.0550, G_loss: 4.3131\n",
      "Epoch [450/800], Step [24/53], D_loss: 0.5718, G_loss: 6.9622\n",
      "Epoch [450/800], Step [26/53], D_loss: 0.0886, G_loss: 6.3623\n",
      "Epoch [450/800], Step [28/53], D_loss: 0.2632, G_loss: 7.0285\n",
      "Epoch [450/800], Step [30/53], D_loss: 0.0489, G_loss: 3.4499\n",
      "Epoch [450/800], Step [32/53], D_loss: 0.0646, G_loss: 4.2142\n",
      "Epoch [450/800], Step [34/53], D_loss: 0.0754, G_loss: 7.0758\n",
      "Epoch [450/800], Step [36/53], D_loss: 0.0738, G_loss: 4.2782\n",
      "Epoch [450/800], Step [38/53], D_loss: 0.1261, G_loss: 6.9501\n",
      "Epoch [450/800], Step [40/53], D_loss: 0.1220, G_loss: 5.4129\n",
      "Epoch [450/800], Step [42/53], D_loss: 0.1866, G_loss: 5.1420\n",
      "Epoch [450/800], Step [44/53], D_loss: 0.1945, G_loss: 9.1609\n",
      "Epoch [450/800], Step [46/53], D_loss: 0.2996, G_loss: 5.2286\n",
      "Epoch [450/800], Step [48/53], D_loss: 0.1280, G_loss: 3.8178\n",
      "Epoch [450/800], Step [50/53], D_loss: 0.0400, G_loss: 4.0431\n",
      "Epoch [450/800], Step [52/53], D_loss: 0.0069, G_loss: 5.1313\n",
      "Epoch [450/800], Avg D_loss: 0.1373, Avg G_loss: 5.1316\n",
      "Models saved for epoch 450\n",
      "Epoch [451/800], Step [2/53], D_loss: 0.7195, G_loss: 8.0689\n",
      "Epoch [451/800], Step [4/53], D_loss: 0.0606, G_loss: 2.9507\n",
      "Epoch [451/800], Step [6/53], D_loss: 0.0755, G_loss: 2.2389\n",
      "Epoch [451/800], Step [8/53], D_loss: 0.2530, G_loss: 1.7906\n",
      "Epoch [451/800], Step [10/53], D_loss: 0.0548, G_loss: 4.4459\n",
      "Epoch [451/800], Step [12/53], D_loss: 0.2440, G_loss: 4.2388\n",
      "Epoch [451/800], Step [14/53], D_loss: 0.0883, G_loss: 4.7685\n",
      "Epoch [451/800], Step [16/53], D_loss: 0.0092, G_loss: 3.5990\n",
      "Epoch [451/800], Step [18/53], D_loss: 0.1251, G_loss: 4.6334\n",
      "Epoch [451/800], Step [20/53], D_loss: 2.0304, G_loss: 9.5500\n",
      "Epoch [451/800], Step [22/53], D_loss: 0.5929, G_loss: 2.2838\n",
      "Epoch [451/800], Step [24/53], D_loss: 0.3114, G_loss: 1.5294\n",
      "Epoch [451/800], Step [26/53], D_loss: 0.3093, G_loss: 2.1706\n",
      "Epoch [451/800], Step [28/53], D_loss: 0.2193, G_loss: 3.2799\n",
      "Epoch [451/800], Step [30/53], D_loss: 0.0775, G_loss: 3.0703\n",
      "Epoch [451/800], Step [32/53], D_loss: 0.1597, G_loss: 3.4276\n",
      "Epoch [451/800], Step [34/53], D_loss: 0.2232, G_loss: 2.7105\n",
      "Epoch [451/800], Step [36/53], D_loss: 0.0533, G_loss: 2.9132\n",
      "Epoch [451/800], Step [38/53], D_loss: 0.0644, G_loss: 4.1053\n",
      "Epoch [451/800], Step [40/53], D_loss: 0.0422, G_loss: 3.5601\n",
      "Epoch [451/800], Step [42/53], D_loss: 0.0087, G_loss: 5.2234\n",
      "Epoch [451/800], Step [44/53], D_loss: 0.0684, G_loss: 5.0980\n",
      "Epoch [451/800], Step [46/53], D_loss: 0.0526, G_loss: 8.7166\n",
      "Epoch [451/800], Step [48/53], D_loss: 0.0040, G_loss: 6.1085\n",
      "Epoch [451/800], Step [50/53], D_loss: 0.0017, G_loss: 5.2221\n",
      "Epoch [451/800], Step [52/53], D_loss: 0.0034, G_loss: 6.9527\n",
      "Epoch [451/800], Avg D_loss: 0.2230, Avg G_loss: 4.5693\n",
      "Epoch [452/800], Step [2/53], D_loss: 0.0002, G_loss: 12.2888\n",
      "Epoch [452/800], Step [4/53], D_loss: 0.0021, G_loss: 8.7020\n",
      "Epoch [452/800], Step [6/53], D_loss: 0.0039, G_loss: 6.7924\n",
      "Epoch [452/800], Step [8/53], D_loss: 0.0061, G_loss: 5.4721\n",
      "Epoch [452/800], Step [10/53], D_loss: 0.0893, G_loss: 6.7376\n",
      "Epoch [452/800], Step [12/53], D_loss: 0.0387, G_loss: 7.6650\n",
      "Epoch [452/800], Step [14/53], D_loss: 0.0656, G_loss: 5.3940\n",
      "Epoch [452/800], Step [16/53], D_loss: 0.0360, G_loss: 7.7593\n",
      "Epoch [452/800], Step [18/53], D_loss: 0.0214, G_loss: 6.7283\n",
      "Epoch [452/800], Step [20/53], D_loss: 0.2149, G_loss: 9.2912\n",
      "Epoch [452/800], Step [22/53], D_loss: 0.0599, G_loss: 7.3933\n",
      "Epoch [452/800], Step [24/53], D_loss: 0.0256, G_loss: 8.3958\n",
      "Epoch [452/800], Step [26/53], D_loss: 0.3006, G_loss: 2.7040\n",
      "Epoch [452/800], Step [28/53], D_loss: 1.3924, G_loss: 10.2836\n",
      "Epoch [452/800], Step [30/53], D_loss: 0.0935, G_loss: 6.4039\n",
      "Epoch [452/800], Step [32/53], D_loss: 0.0312, G_loss: 5.6762\n",
      "Epoch [452/800], Step [34/53], D_loss: 0.0684, G_loss: 5.0014\n",
      "Epoch [452/800], Step [36/53], D_loss: 0.0829, G_loss: 5.2799\n",
      "Epoch [452/800], Step [38/53], D_loss: 0.0726, G_loss: 5.6039\n",
      "Epoch [452/800], Step [40/53], D_loss: 0.1399, G_loss: 2.9488\n",
      "Epoch [452/800], Step [42/53], D_loss: 0.0681, G_loss: 6.7837\n",
      "Epoch [452/800], Step [44/53], D_loss: 0.0299, G_loss: 5.3313\n",
      "Epoch [452/800], Step [46/53], D_loss: 0.1587, G_loss: 7.2773\n",
      "Epoch [452/800], Step [48/53], D_loss: 0.0429, G_loss: 3.4182\n",
      "Epoch [452/800], Step [50/53], D_loss: 0.0570, G_loss: 5.4435\n",
      "Epoch [452/800], Step [52/53], D_loss: 0.0848, G_loss: 5.8596\n",
      "Epoch [452/800], Avg D_loss: 0.0898, Avg G_loss: 6.3860\n",
      "Epoch [453/800], Step [2/53], D_loss: 0.0645, G_loss: 9.2659\n",
      "Epoch [453/800], Step [4/53], D_loss: 0.0759, G_loss: 4.1882\n",
      "Epoch [453/800], Step [6/53], D_loss: 0.1121, G_loss: 6.8515\n",
      "Epoch [453/800], Step [8/53], D_loss: 0.2275, G_loss: 8.9754\n",
      "Epoch [453/800], Step [10/53], D_loss: 0.5838, G_loss: 5.4797\n",
      "Epoch [453/800], Step [12/53], D_loss: 0.0100, G_loss: 6.1536\n",
      "Epoch [453/800], Step [14/53], D_loss: 0.0339, G_loss: 5.9072\n",
      "Epoch [453/800], Step [16/53], D_loss: 0.1300, G_loss: 6.3579\n",
      "Epoch [453/800], Step [18/53], D_loss: 0.0402, G_loss: 7.0019\n",
      "Epoch [453/800], Step [20/53], D_loss: 0.0746, G_loss: 6.8394\n",
      "Epoch [453/800], Step [22/53], D_loss: 0.2034, G_loss: 8.0831\n",
      "Epoch [453/800], Step [24/53], D_loss: 0.0035, G_loss: 2.4317\n",
      "Epoch [453/800], Step [26/53], D_loss: 0.0299, G_loss: 7.9479\n",
      "Epoch [453/800], Step [28/53], D_loss: 0.0236, G_loss: 5.2264\n",
      "Epoch [453/800], Step [30/53], D_loss: 0.0031, G_loss: 11.0295\n",
      "Epoch [453/800], Step [32/53], D_loss: 0.0156, G_loss: 6.4571\n",
      "Epoch [453/800], Step [34/53], D_loss: 0.0117, G_loss: 5.6427\n",
      "Epoch [453/800], Step [36/53], D_loss: 0.0470, G_loss: 8.6284\n",
      "Epoch [453/800], Step [38/53], D_loss: 0.0155, G_loss: 8.8420\n",
      "Epoch [453/800], Step [40/53], D_loss: 0.0058, G_loss: 8.8047\n",
      "Epoch [453/800], Step [42/53], D_loss: 0.0021, G_loss: 9.9523\n",
      "Epoch [453/800], Step [44/53], D_loss: 0.0026, G_loss: 9.2771\n",
      "Epoch [453/800], Step [46/53], D_loss: 0.0355, G_loss: 8.9125\n",
      "Epoch [453/800], Step [48/53], D_loss: 0.0454, G_loss: 7.0870\n",
      "Epoch [453/800], Step [50/53], D_loss: 0.0058, G_loss: 6.5605\n",
      "Epoch [453/800], Step [52/53], D_loss: 0.0118, G_loss: 6.2686\n",
      "Epoch [453/800], Avg D_loss: 0.0612, Avg G_loss: 6.9586\n",
      "Epoch [454/800], Step [2/53], D_loss: 0.0616, G_loss: 9.1295\n",
      "Epoch [454/800], Step [4/53], D_loss: 0.0027, G_loss: 11.8558\n",
      "Epoch [454/800], Step [6/53], D_loss: 0.0008, G_loss: 12.4606\n",
      "Epoch [454/800], Step [8/53], D_loss: 0.0124, G_loss: 9.9409\n",
      "Epoch [454/800], Step [10/53], D_loss: 0.0190, G_loss: 10.1830\n",
      "Epoch [454/800], Step [12/53], D_loss: 0.0224, G_loss: 10.6415\n",
      "Epoch [454/800], Step [14/53], D_loss: 0.0187, G_loss: 8.2579\n",
      "Epoch [454/800], Step [16/53], D_loss: 0.0253, G_loss: 7.1936\n",
      "Epoch [454/800], Step [18/53], D_loss: 0.0336, G_loss: 9.0873\n",
      "Epoch [454/800], Step [20/53], D_loss: 0.0991, G_loss: 10.8862\n",
      "Epoch [454/800], Step [22/53], D_loss: 0.0157, G_loss: 13.9328\n",
      "Epoch [454/800], Step [24/53], D_loss: 0.0293, G_loss: 12.8817\n",
      "Epoch [454/800], Step [26/53], D_loss: 0.0037, G_loss: 12.0338\n",
      "Epoch [454/800], Step [28/53], D_loss: 0.0138, G_loss: 6.1970\n",
      "Epoch [454/800], Step [30/53], D_loss: 0.0024, G_loss: 8.2427\n",
      "Epoch [454/800], Step [32/53], D_loss: 0.0957, G_loss: 10.9746\n",
      "Epoch [454/800], Step [34/53], D_loss: 0.0299, G_loss: 12.2982\n",
      "Epoch [454/800], Step [36/53], D_loss: 0.1089, G_loss: 6.7865\n",
      "Epoch [454/800], Step [38/53], D_loss: 0.0141, G_loss: 7.2029\n",
      "Epoch [454/800], Step [40/53], D_loss: 0.0005, G_loss: 8.1350\n",
      "Epoch [454/800], Step [42/53], D_loss: 0.0394, G_loss: 11.1513\n",
      "Epoch [454/800], Step [44/53], D_loss: 0.0004, G_loss: 11.6641\n",
      "Epoch [454/800], Step [46/53], D_loss: 0.0070, G_loss: 3.5366\n",
      "Epoch [454/800], Step [48/53], D_loss: 0.4938, G_loss: 13.8189\n",
      "Epoch [454/800], Step [50/53], D_loss: 0.0506, G_loss: 5.6497\n",
      "Epoch [454/800], Step [52/53], D_loss: 0.0627, G_loss: 7.1452\n",
      "Epoch [454/800], Avg D_loss: 0.0613, Avg G_loss: 9.6479\n",
      "Epoch [455/800], Step [2/53], D_loss: 0.0272, G_loss: 6.1882\n",
      "Epoch [455/800], Step [4/53], D_loss: 0.0047, G_loss: 5.8948\n",
      "Epoch [455/800], Step [6/53], D_loss: 0.1070, G_loss: 8.0960\n",
      "Epoch [455/800], Step [8/53], D_loss: 0.0124, G_loss: 8.8890\n",
      "Epoch [455/800], Step [10/53], D_loss: 0.0050, G_loss: 6.4594\n",
      "Epoch [455/800], Step [12/53], D_loss: 0.0060, G_loss: 6.8669\n",
      "Epoch [455/800], Step [14/53], D_loss: 0.0026, G_loss: 6.8071\n",
      "Epoch [455/800], Step [16/53], D_loss: 0.0530, G_loss: 6.8687\n",
      "Epoch [455/800], Step [18/53], D_loss: 0.0007, G_loss: 8.9117\n",
      "Epoch [455/800], Step [20/53], D_loss: 0.0337, G_loss: 9.1156\n",
      "Epoch [455/800], Step [22/53], D_loss: 0.1008, G_loss: 7.9429\n",
      "Epoch [455/800], Step [24/53], D_loss: 0.0091, G_loss: 9.9392\n",
      "Epoch [455/800], Step [26/53], D_loss: 0.0086, G_loss: 10.1770\n",
      "Epoch [455/800], Step [28/53], D_loss: 0.0026, G_loss: 11.5218\n",
      "Epoch [455/800], Step [30/53], D_loss: 0.0343, G_loss: 8.8593\n",
      "Epoch [455/800], Step [32/53], D_loss: 0.0198, G_loss: 7.5324\n",
      "Epoch [455/800], Step [34/53], D_loss: 0.0011, G_loss: 8.7809\n",
      "Epoch [455/800], Step [36/53], D_loss: 0.0087, G_loss: 9.9087\n",
      "Epoch [455/800], Step [38/53], D_loss: 0.0258, G_loss: 10.6062\n",
      "Epoch [455/800], Step [40/53], D_loss: 0.0123, G_loss: 5.3780\n",
      "Epoch [455/800], Step [42/53], D_loss: 0.0002, G_loss: 9.6854\n",
      "Epoch [455/800], Step [44/53], D_loss: 0.0167, G_loss: 11.4961\n",
      "Epoch [455/800], Step [46/53], D_loss: 0.0024, G_loss: 11.5526\n",
      "Epoch [455/800], Step [48/53], D_loss: 0.0324, G_loss: 8.9965\n",
      "Epoch [455/800], Step [50/53], D_loss: 0.0226, G_loss: 9.4055\n",
      "Epoch [455/800], Step [52/53], D_loss: 0.0026, G_loss: 6.7762\n",
      "Epoch [455/800], Avg D_loss: 0.0234, Avg G_loss: 8.6724\n",
      "Epoch [456/800], Step [2/53], D_loss: 0.0082, G_loss: 8.7795\n",
      "Epoch [456/800], Step [4/53], D_loss: 0.0519, G_loss: 12.1560\n",
      "Epoch [456/800], Step [6/53], D_loss: 0.0035, G_loss: 7.9207\n",
      "Epoch [456/800], Step [8/53], D_loss: 0.0037, G_loss: 10.7532\n",
      "Epoch [456/800], Step [10/53], D_loss: 0.0225, G_loss: 3.9020\n",
      "Epoch [456/800], Step [12/53], D_loss: 0.0023, G_loss: 8.2646\n",
      "Epoch [456/800], Step [14/53], D_loss: 0.0008, G_loss: 10.8814\n",
      "Epoch [456/800], Step [16/53], D_loss: 0.0007, G_loss: 10.8151\n",
      "Epoch [456/800], Step [18/53], D_loss: 0.0236, G_loss: 9.2212\n",
      "Epoch [456/800], Step [20/53], D_loss: 0.0115, G_loss: 10.8127\n",
      "Epoch [456/800], Step [22/53], D_loss: 0.0072, G_loss: 8.9873\n",
      "Epoch [456/800], Step [24/53], D_loss: 0.0384, G_loss: 5.9162\n",
      "Epoch [456/800], Step [26/53], D_loss: 0.0299, G_loss: 7.0577\n",
      "Epoch [456/800], Step [28/53], D_loss: 0.0186, G_loss: 8.5970\n",
      "Epoch [456/800], Step [30/53], D_loss: 0.0321, G_loss: 9.0662\n",
      "Epoch [456/800], Step [32/53], D_loss: 0.0139, G_loss: 6.5421\n",
      "Epoch [456/800], Step [34/53], D_loss: 0.0786, G_loss: 10.3984\n",
      "Epoch [456/800], Step [36/53], D_loss: 0.0011, G_loss: 10.1753\n",
      "Epoch [456/800], Step [38/53], D_loss: 0.0181, G_loss: 10.9809\n",
      "Epoch [456/800], Step [40/53], D_loss: 0.0006, G_loss: 10.3796\n",
      "Epoch [456/800], Step [42/53], D_loss: 0.0063, G_loss: 9.5137\n",
      "Epoch [456/800], Step [44/53], D_loss: 0.1911, G_loss: 5.4587\n",
      "Epoch [456/800], Step [46/53], D_loss: 0.0914, G_loss: 8.2141\n",
      "Epoch [456/800], Step [48/53], D_loss: 0.0201, G_loss: 9.8177\n",
      "Epoch [456/800], Step [50/53], D_loss: 0.0128, G_loss: 9.4836\n",
      "Epoch [456/800], Step [52/53], D_loss: 0.0031, G_loss: 8.2546\n",
      "Epoch [456/800], Avg D_loss: 0.0393, Avg G_loss: 9.0460\n",
      "Epoch [457/800], Step [2/53], D_loss: 0.0467, G_loss: 6.0617\n",
      "Epoch [457/800], Step [4/53], D_loss: 0.0502, G_loss: 6.6387\n",
      "Epoch [457/800], Step [6/53], D_loss: 0.0030, G_loss: 10.3523\n",
      "Epoch [457/800], Step [8/53], D_loss: 0.0041, G_loss: 6.2175\n",
      "Epoch [457/800], Step [10/53], D_loss: 0.1286, G_loss: 8.3723\n",
      "Epoch [457/800], Step [12/53], D_loss: 0.0117, G_loss: 11.0959\n",
      "Epoch [457/800], Step [14/53], D_loss: 0.1238, G_loss: 8.3625\n",
      "Epoch [457/800], Step [16/53], D_loss: 0.0470, G_loss: 6.3074\n",
      "Epoch [457/800], Step [18/53], D_loss: 0.0373, G_loss: 10.2493\n",
      "Epoch [457/800], Step [20/53], D_loss: 0.0166, G_loss: 7.6602\n",
      "Epoch [457/800], Step [22/53], D_loss: 0.0545, G_loss: 8.3720\n",
      "Epoch [457/800], Step [24/53], D_loss: 0.0080, G_loss: 8.8843\n",
      "Epoch [457/800], Step [26/53], D_loss: 0.0477, G_loss: 12.0098\n",
      "Epoch [457/800], Step [28/53], D_loss: 0.0389, G_loss: 6.8614\n",
      "Epoch [457/800], Step [30/53], D_loss: 0.0099, G_loss: 12.4807\n",
      "Epoch [457/800], Step [32/53], D_loss: 0.0471, G_loss: 16.3241\n",
      "Epoch [457/800], Step [34/53], D_loss: 0.0108, G_loss: 11.6122\n",
      "Epoch [457/800], Step [36/53], D_loss: 0.0059, G_loss: 7.7978\n",
      "Epoch [457/800], Step [38/53], D_loss: 0.0000, G_loss: 9.6948\n",
      "Epoch [457/800], Step [40/53], D_loss: 0.0042, G_loss: 10.3123\n",
      "Epoch [457/800], Step [42/53], D_loss: 0.0091, G_loss: 9.4167\n",
      "Epoch [457/800], Step [44/53], D_loss: 0.0011, G_loss: 9.7190\n",
      "Epoch [457/800], Step [46/53], D_loss: 0.0103, G_loss: 9.9515\n",
      "Epoch [457/800], Step [48/53], D_loss: 0.0830, G_loss: 13.4155\n",
      "Epoch [457/800], Step [50/53], D_loss: 0.5410, G_loss: 6.8918\n",
      "Epoch [457/800], Step [52/53], D_loss: 0.0582, G_loss: 11.1666\n",
      "Epoch [457/800], Avg D_loss: 0.0488, Avg G_loss: 9.2601\n",
      "Epoch [458/800], Step [2/53], D_loss: 0.0008, G_loss: 8.2901\n",
      "Epoch [458/800], Step [4/53], D_loss: 0.0017, G_loss: 9.3548\n",
      "Epoch [458/800], Step [6/53], D_loss: 0.0495, G_loss: 10.1708\n",
      "Epoch [458/800], Step [8/53], D_loss: 0.0093, G_loss: 5.5174\n",
      "Epoch [458/800], Step [10/53], D_loss: 0.0067, G_loss: 10.4109\n",
      "Epoch [458/800], Step [12/53], D_loss: 0.1399, G_loss: 4.9807\n",
      "Epoch [458/800], Step [14/53], D_loss: 0.3069, G_loss: 12.6836\n",
      "Epoch [458/800], Step [16/53], D_loss: 0.0282, G_loss: 14.3091\n",
      "Epoch [458/800], Step [18/53], D_loss: 0.0096, G_loss: 3.6775\n",
      "Epoch [458/800], Step [20/53], D_loss: 0.0203, G_loss: 11.8420\n",
      "Epoch [458/800], Step [22/53], D_loss: 0.0602, G_loss: 14.6165\n",
      "Epoch [458/800], Step [24/53], D_loss: 0.0382, G_loss: 6.6899\n",
      "Epoch [458/800], Step [26/53], D_loss: 0.0238, G_loss: 7.5071\n",
      "Epoch [458/800], Step [28/53], D_loss: 0.0448, G_loss: 7.2374\n",
      "Epoch [458/800], Step [30/53], D_loss: 0.0036, G_loss: 10.0951\n",
      "Epoch [458/800], Step [32/53], D_loss: 0.0028, G_loss: 8.6695\n",
      "Epoch [458/800], Step [34/53], D_loss: 0.1470, G_loss: 8.6120\n",
      "Epoch [458/800], Step [36/53], D_loss: 0.0634, G_loss: 8.8101\n",
      "Epoch [458/800], Step [38/53], D_loss: 0.0033, G_loss: 10.7618\n",
      "Epoch [458/800], Step [40/53], D_loss: 0.0109, G_loss: 7.7182\n",
      "Epoch [458/800], Step [42/53], D_loss: 0.0014, G_loss: 9.0229\n",
      "Epoch [458/800], Step [44/53], D_loss: 0.1712, G_loss: 15.6033\n",
      "Epoch [458/800], Step [46/53], D_loss: 0.0041, G_loss: 13.4883\n",
      "Epoch [458/800], Step [48/53], D_loss: 0.0075, G_loss: 12.3864\n",
      "Epoch [458/800], Step [50/53], D_loss: 0.1475, G_loss: 8.1691\n",
      "Epoch [458/800], Step [52/53], D_loss: 0.0153, G_loss: 4.1597\n",
      "Epoch [458/800], Avg D_loss: 0.0591, Avg G_loss: 9.3847\n",
      "Epoch [459/800], Step [2/53], D_loss: 0.1123, G_loss: 11.7384\n",
      "Epoch [459/800], Step [4/53], D_loss: 0.0045, G_loss: 10.9681\n",
      "Epoch [459/800], Step [6/53], D_loss: 0.0179, G_loss: 10.0336\n",
      "Epoch [459/800], Step [8/53], D_loss: 0.0107, G_loss: 11.5460\n",
      "Epoch [459/800], Step [10/53], D_loss: 0.0039, G_loss: 9.6857\n",
      "Epoch [459/800], Step [12/53], D_loss: 0.0278, G_loss: 6.7056\n",
      "Epoch [459/800], Step [14/53], D_loss: 0.3785, G_loss: 13.3795\n",
      "Epoch [459/800], Step [16/53], D_loss: 0.0253, G_loss: 10.6499\n",
      "Epoch [459/800], Step [18/53], D_loss: 0.0008, G_loss: 10.3557\n",
      "Epoch [459/800], Step [20/53], D_loss: 0.0323, G_loss: 7.9527\n",
      "Epoch [459/800], Step [22/53], D_loss: 0.0181, G_loss: 3.9287\n",
      "Epoch [459/800], Step [24/53], D_loss: 0.2344, G_loss: 9.7781\n",
      "Epoch [459/800], Step [26/53], D_loss: 0.3276, G_loss: 2.8890\n",
      "Epoch [459/800], Step [28/53], D_loss: 0.2035, G_loss: 6.0814\n",
      "Epoch [459/800], Step [30/53], D_loss: 0.1229, G_loss: 4.6442\n",
      "Epoch [459/800], Step [32/53], D_loss: 0.0120, G_loss: 7.2943\n",
      "Epoch [459/800], Step [34/53], D_loss: 0.0027, G_loss: 6.2100\n",
      "Epoch [459/800], Step [36/53], D_loss: 0.0016, G_loss: 5.9564\n",
      "Epoch [459/800], Step [38/53], D_loss: 0.0195, G_loss: 8.9228\n",
      "Epoch [459/800], Step [40/53], D_loss: 0.0308, G_loss: 6.3576\n",
      "Epoch [459/800], Step [42/53], D_loss: 0.0826, G_loss: 14.0675\n",
      "Epoch [459/800], Step [44/53], D_loss: 0.2156, G_loss: 3.7043\n",
      "Epoch [459/800], Step [46/53], D_loss: 0.0936, G_loss: 7.6992\n",
      "Epoch [459/800], Step [48/53], D_loss: 0.1119, G_loss: 6.7123\n",
      "Epoch [459/800], Step [50/53], D_loss: 0.0289, G_loss: 2.8338\n",
      "Epoch [459/800], Step [52/53], D_loss: 0.0862, G_loss: 5.2084\n",
      "Epoch [459/800], Avg D_loss: 0.0762, Avg G_loss: 8.2166\n",
      "Epoch [460/800], Step [2/53], D_loss: 0.0180, G_loss: 3.6863\n",
      "Epoch [460/800], Step [4/53], D_loss: 0.0332, G_loss: 8.3841\n",
      "Epoch [460/800], Step [6/53], D_loss: 0.2498, G_loss: 9.2455\n",
      "Epoch [460/800], Step [8/53], D_loss: 0.2203, G_loss: 4.6511\n",
      "Epoch [460/800], Step [10/53], D_loss: 0.0132, G_loss: 4.5937\n",
      "Epoch [460/800], Step [12/53], D_loss: 0.0215, G_loss: 6.9723\n",
      "Epoch [460/800], Step [14/53], D_loss: 0.0719, G_loss: 2.2595\n",
      "Epoch [460/800], Step [16/53], D_loss: 0.0953, G_loss: 4.0759\n",
      "Epoch [460/800], Step [18/53], D_loss: 0.4302, G_loss: 3.1113\n",
      "Epoch [460/800], Step [20/53], D_loss: 0.0334, G_loss: 3.5205\n",
      "Epoch [460/800], Step [22/53], D_loss: 0.0016, G_loss: 7.9533\n",
      "Epoch [460/800], Step [24/53], D_loss: 0.0220, G_loss: 6.9732\n",
      "Epoch [460/800], Step [26/53], D_loss: 0.1297, G_loss: 4.1758\n",
      "Epoch [460/800], Step [28/53], D_loss: 0.0078, G_loss: 3.9189\n",
      "Epoch [460/800], Step [30/53], D_loss: 0.0086, G_loss: 8.4144\n",
      "Epoch [460/800], Step [32/53], D_loss: 0.0003, G_loss: 2.6739\n",
      "Epoch [460/800], Step [34/53], D_loss: 0.1503, G_loss: 8.3881\n",
      "Epoch [460/800], Step [36/53], D_loss: 0.0131, G_loss: 5.8233\n",
      "Epoch [460/800], Step [38/53], D_loss: 0.0189, G_loss: 5.2775\n",
      "Epoch [460/800], Step [40/53], D_loss: 0.1057, G_loss: 7.8993\n",
      "Epoch [460/800], Step [42/53], D_loss: 0.0317, G_loss: 7.7886\n",
      "Epoch [460/800], Step [44/53], D_loss: 0.0155, G_loss: 6.7668\n",
      "Epoch [460/800], Step [46/53], D_loss: 0.0219, G_loss: 6.4162\n",
      "Epoch [460/800], Step [48/53], D_loss: 0.1126, G_loss: 8.3644\n",
      "Epoch [460/800], Step [50/53], D_loss: 0.0035, G_loss: 8.7484\n",
      "Epoch [460/800], Step [52/53], D_loss: 0.0352, G_loss: 7.0302\n",
      "Epoch [460/800], Avg D_loss: 0.0833, Avg G_loss: 6.1227\n",
      "Generated images saved as generated_images/epoch_460.png\n",
      "Epoch [461/800], Step [2/53], D_loss: 0.0042, G_loss: 10.5763\n",
      "Epoch [461/800], Step [4/53], D_loss: 0.0718, G_loss: 9.9836\n",
      "Epoch [461/800], Step [6/53], D_loss: 0.1365, G_loss: 7.8530\n",
      "Epoch [461/800], Step [8/53], D_loss: 0.0437, G_loss: 3.9991\n",
      "Epoch [461/800], Step [10/53], D_loss: 0.1463, G_loss: 7.8548\n",
      "Epoch [461/800], Step [12/53], D_loss: 0.0794, G_loss: 9.0169\n",
      "Epoch [461/800], Step [14/53], D_loss: 0.0131, G_loss: 8.7474\n",
      "Epoch [461/800], Step [16/53], D_loss: 0.0081, G_loss: 11.3970\n",
      "Epoch [461/800], Step [18/53], D_loss: 0.0046, G_loss: 7.2635\n",
      "Epoch [461/800], Step [20/53], D_loss: 0.0025, G_loss: 9.3765\n",
      "Epoch [461/800], Step [22/53], D_loss: 0.0271, G_loss: 4.7832\n",
      "Epoch [461/800], Step [24/53], D_loss: 0.0082, G_loss: 7.2071\n",
      "Epoch [461/800], Step [26/53], D_loss: 0.0056, G_loss: 9.9781\n",
      "Epoch [461/800], Step [28/53], D_loss: 0.0387, G_loss: 4.8011\n",
      "Epoch [461/800], Step [30/53], D_loss: 0.0038, G_loss: 6.7110\n",
      "Epoch [461/800], Step [32/53], D_loss: 0.1046, G_loss: 2.2972\n",
      "Epoch [461/800], Step [34/53], D_loss: 0.2118, G_loss: 8.5178\n",
      "Epoch [461/800], Step [36/53], D_loss: 0.0088, G_loss: 10.2891\n",
      "Epoch [461/800], Step [38/53], D_loss: 0.0206, G_loss: 7.9303\n",
      "Epoch [461/800], Step [40/53], D_loss: 0.0115, G_loss: 4.7033\n",
      "Epoch [461/800], Step [42/53], D_loss: 0.0391, G_loss: 10.2033\n",
      "Epoch [461/800], Step [44/53], D_loss: 0.0012, G_loss: 9.1345\n",
      "Epoch [461/800], Step [46/53], D_loss: 0.1044, G_loss: 3.8843\n",
      "Epoch [461/800], Step [48/53], D_loss: 0.2046, G_loss: 1.2684\n",
      "Epoch [461/800], Step [50/53], D_loss: 0.4064, G_loss: 11.1388\n",
      "Epoch [461/800], Step [52/53], D_loss: 0.1233, G_loss: 9.1567\n",
      "Epoch [461/800], Avg D_loss: 0.0653, Avg G_loss: 7.9272\n",
      "Epoch [462/800], Step [2/53], D_loss: 0.1674, G_loss: 6.9100\n",
      "Epoch [462/800], Step [4/53], D_loss: 0.0067, G_loss: 4.6212\n",
      "Epoch [462/800], Step [6/53], D_loss: 0.0127, G_loss: 6.7976\n",
      "Epoch [462/800], Step [8/53], D_loss: 0.0153, G_loss: 10.4631\n",
      "Epoch [462/800], Step [10/53], D_loss: 0.0019, G_loss: 5.4160\n",
      "Epoch [462/800], Step [12/53], D_loss: 0.0117, G_loss: 8.5295\n",
      "Epoch [462/800], Step [14/53], D_loss: 0.0694, G_loss: 6.1595\n",
      "Epoch [462/800], Step [16/53], D_loss: 0.0674, G_loss: 7.6753\n",
      "Epoch [462/800], Step [18/53], D_loss: 0.0307, G_loss: 6.6066\n",
      "Epoch [462/800], Step [20/53], D_loss: 0.0097, G_loss: 8.7273\n",
      "Epoch [462/800], Step [22/53], D_loss: 0.0983, G_loss: 7.6799\n",
      "Epoch [462/800], Step [24/53], D_loss: 0.0519, G_loss: 10.4921\n",
      "Epoch [462/800], Step [26/53], D_loss: 0.0064, G_loss: 7.0589\n",
      "Epoch [462/800], Step [28/53], D_loss: 0.0611, G_loss: 8.9828\n",
      "Epoch [462/800], Step [30/53], D_loss: 0.0054, G_loss: 8.3050\n",
      "Epoch [462/800], Step [32/53], D_loss: 0.0065, G_loss: 3.9554\n",
      "Epoch [462/800], Step [34/53], D_loss: 0.0689, G_loss: 13.7148\n",
      "Epoch [462/800], Step [36/53], D_loss: 0.1019, G_loss: 4.0020\n",
      "Epoch [462/800], Step [38/53], D_loss: 0.7542, G_loss: 18.0003\n",
      "Epoch [462/800], Step [40/53], D_loss: 0.0641, G_loss: 3.6845\n",
      "Epoch [462/800], Step [42/53], D_loss: 0.0899, G_loss: 7.9356\n",
      "Epoch [462/800], Step [44/53], D_loss: 0.0170, G_loss: 6.9117\n",
      "Epoch [462/800], Step [46/53], D_loss: 0.3122, G_loss: 8.3108\n",
      "Epoch [462/800], Step [48/53], D_loss: 0.1059, G_loss: 2.5296\n",
      "Epoch [462/800], Step [50/53], D_loss: 0.9044, G_loss: 9.8663\n",
      "Epoch [462/800], Step [52/53], D_loss: 0.3791, G_loss: 4.4593\n",
      "Epoch [462/800], Avg D_loss: 0.1366, Avg G_loss: 7.2653\n",
      "Epoch [463/800], Step [2/53], D_loss: 0.0665, G_loss: 4.4121\n",
      "Epoch [463/800], Step [4/53], D_loss: 0.1391, G_loss: 4.7267\n",
      "Epoch [463/800], Step [6/53], D_loss: 0.0410, G_loss: 3.0500\n",
      "Epoch [463/800], Step [8/53], D_loss: 0.0743, G_loss: 5.4908\n",
      "Epoch [463/800], Step [10/53], D_loss: 0.1904, G_loss: 4.4386\n",
      "Epoch [463/800], Step [12/53], D_loss: 0.0259, G_loss: 4.3958\n",
      "Epoch [463/800], Step [14/53], D_loss: 0.0151, G_loss: 4.8342\n",
      "Epoch [463/800], Step [16/53], D_loss: 0.0537, G_loss: 4.2050\n",
      "Epoch [463/800], Step [18/53], D_loss: 0.1340, G_loss: 6.7497\n",
      "Epoch [463/800], Step [20/53], D_loss: 0.2040, G_loss: 2.3954\n",
      "Epoch [463/800], Step [22/53], D_loss: 0.0446, G_loss: 6.2915\n",
      "Epoch [463/800], Step [24/53], D_loss: 0.0601, G_loss: 7.1517\n",
      "Epoch [463/800], Step [26/53], D_loss: 0.0266, G_loss: 4.1793\n",
      "Epoch [463/800], Step [28/53], D_loss: 0.0134, G_loss: 1.9402\n",
      "Epoch [463/800], Step [30/53], D_loss: 0.5514, G_loss: 11.7658\n",
      "Epoch [463/800], Step [32/53], D_loss: 0.1921, G_loss: 2.2931\n",
      "Epoch [463/800], Step [34/53], D_loss: 0.1557, G_loss: 2.4914\n",
      "Epoch [463/800], Step [36/53], D_loss: 0.1369, G_loss: 2.6677\n",
      "Epoch [463/800], Step [38/53], D_loss: 0.1647, G_loss: 4.6942\n",
      "Epoch [463/800], Step [40/53], D_loss: 0.2749, G_loss: 4.2323\n",
      "Epoch [463/800], Step [42/53], D_loss: 0.1461, G_loss: 5.5282\n",
      "Epoch [463/800], Step [44/53], D_loss: 0.0678, G_loss: 1.4902\n",
      "Epoch [463/800], Step [46/53], D_loss: 0.0389, G_loss: 4.9195\n",
      "Epoch [463/800], Step [48/53], D_loss: 0.0211, G_loss: 4.8333\n",
      "Epoch [463/800], Step [50/53], D_loss: 0.0315, G_loss: 5.5876\n",
      "Epoch [463/800], Step [52/53], D_loss: 0.2064, G_loss: 4.7329\n",
      "Epoch [463/800], Avg D_loss: 0.1886, Avg G_loss: 4.3363\n",
      "Epoch [464/800], Step [2/53], D_loss: 0.0862, G_loss: 4.7097\n",
      "Epoch [464/800], Step [4/53], D_loss: 0.2215, G_loss: 5.9293\n",
      "Epoch [464/800], Step [6/53], D_loss: 0.0642, G_loss: 9.2738\n",
      "Epoch [464/800], Step [8/53], D_loss: 0.1403, G_loss: 5.6571\n",
      "Epoch [464/800], Step [10/53], D_loss: 0.0445, G_loss: 6.8526\n",
      "Epoch [464/800], Step [12/53], D_loss: 0.0086, G_loss: 6.4395\n",
      "Epoch [464/800], Step [14/53], D_loss: 0.0522, G_loss: 5.0467\n",
      "Epoch [464/800], Step [16/53], D_loss: 0.0336, G_loss: 4.6847\n",
      "Epoch [464/800], Step [18/53], D_loss: 0.0072, G_loss: 4.2866\n",
      "Epoch [464/800], Step [20/53], D_loss: 0.0552, G_loss: 4.4024\n",
      "Epoch [464/800], Step [22/53], D_loss: 0.0088, G_loss: 7.4982\n",
      "Epoch [464/800], Step [24/53], D_loss: 0.0071, G_loss: 6.1225\n",
      "Epoch [464/800], Step [26/53], D_loss: 0.0236, G_loss: 7.0772\n",
      "Epoch [464/800], Step [28/53], D_loss: 0.0185, G_loss: 7.5978\n",
      "Epoch [464/800], Step [30/53], D_loss: 0.0019, G_loss: 6.7153\n",
      "Epoch [464/800], Step [32/53], D_loss: 0.0685, G_loss: 5.4803\n",
      "Epoch [464/800], Step [34/53], D_loss: 0.1086, G_loss: 6.2649\n",
      "Epoch [464/800], Step [36/53], D_loss: 0.0308, G_loss: 7.8372\n",
      "Epoch [464/800], Step [38/53], D_loss: 0.4110, G_loss: 2.2052\n",
      "Epoch [464/800], Step [40/53], D_loss: 0.5443, G_loss: 4.5146\n",
      "Epoch [464/800], Step [42/53], D_loss: 0.0884, G_loss: 8.9829\n",
      "Epoch [464/800], Step [44/53], D_loss: 0.0681, G_loss: 7.3096\n",
      "Epoch [464/800], Step [46/53], D_loss: 0.0394, G_loss: 5.2349\n",
      "Epoch [464/800], Step [48/53], D_loss: 0.0064, G_loss: 5.4277\n",
      "Epoch [464/800], Step [50/53], D_loss: 0.0532, G_loss: 6.5711\n",
      "Epoch [464/800], Step [52/53], D_loss: 0.0061, G_loss: 3.6390\n",
      "Epoch [464/800], Avg D_loss: 0.0626, Avg G_loss: 6.1665\n",
      "Epoch [465/800], Step [2/53], D_loss: 0.4334, G_loss: 10.8865\n",
      "Epoch [465/800], Step [4/53], D_loss: 0.0555, G_loss: 9.2932\n",
      "Epoch [465/800], Step [6/53], D_loss: 0.0555, G_loss: 7.1401\n",
      "Epoch [465/800], Step [8/53], D_loss: 0.0500, G_loss: 6.4356\n",
      "Epoch [465/800], Step [10/53], D_loss: 0.0956, G_loss: 8.3865\n",
      "Epoch [465/800], Step [12/53], D_loss: 0.0230, G_loss: 8.6576\n",
      "Epoch [465/800], Step [14/53], D_loss: 0.0432, G_loss: 7.8737\n",
      "Epoch [465/800], Step [16/53], D_loss: 0.0196, G_loss: 8.1729\n",
      "Epoch [465/800], Step [18/53], D_loss: 0.1062, G_loss: 7.1914\n",
      "Epoch [465/800], Step [20/53], D_loss: 0.2437, G_loss: 12.6366\n",
      "Epoch [465/800], Step [22/53], D_loss: 0.0076, G_loss: 8.0975\n",
      "Epoch [465/800], Step [24/53], D_loss: 0.0166, G_loss: 4.8648\n",
      "Epoch [465/800], Step [26/53], D_loss: 0.0396, G_loss: 6.8786\n",
      "Epoch [465/800], Step [28/53], D_loss: 0.0015, G_loss: 8.0966\n",
      "Epoch [465/800], Step [30/53], D_loss: 0.0085, G_loss: 8.6814\n",
      "Epoch [465/800], Step [32/53], D_loss: 0.0019, G_loss: 4.0247\n",
      "Epoch [465/800], Step [34/53], D_loss: 0.1222, G_loss: 4.2443\n",
      "Epoch [465/800], Step [36/53], D_loss: 0.2608, G_loss: 13.9783\n",
      "Epoch [465/800], Step [38/53], D_loss: 0.0106, G_loss: 8.2365\n",
      "Epoch [465/800], Step [40/53], D_loss: 0.0144, G_loss: 7.4892\n",
      "Epoch [465/800], Step [42/53], D_loss: 0.0441, G_loss: 8.3089\n",
      "Epoch [465/800], Step [44/53], D_loss: 0.0137, G_loss: 5.5004\n",
      "Epoch [465/800], Step [46/53], D_loss: 0.3318, G_loss: 13.6249\n",
      "Epoch [465/800], Step [48/53], D_loss: 0.0064, G_loss: 5.9417\n",
      "Epoch [465/800], Step [50/53], D_loss: 0.0535, G_loss: 6.0643\n",
      "Epoch [465/800], Step [52/53], D_loss: 0.7139, G_loss: 12.7525\n",
      "Epoch [465/800], Avg D_loss: 0.1499, Avg G_loss: 7.7336\n",
      "Epoch [466/800], Step [2/53], D_loss: 0.1155, G_loss: 3.3242\n",
      "Epoch [466/800], Step [4/53], D_loss: 0.6179, G_loss: 2.6689\n",
      "Epoch [466/800], Step [6/53], D_loss: 0.2903, G_loss: 0.9021\n",
      "Epoch [466/800], Step [8/53], D_loss: 0.3184, G_loss: 3.2636\n",
      "Epoch [466/800], Step [10/53], D_loss: 0.3360, G_loss: 2.0971\n",
      "Epoch [466/800], Step [12/53], D_loss: 0.1160, G_loss: 4.0841\n",
      "Epoch [466/800], Step [14/53], D_loss: 0.3614, G_loss: 3.4904\n",
      "Epoch [466/800], Step [16/53], D_loss: 0.1608, G_loss: 3.4304\n",
      "Epoch [466/800], Step [18/53], D_loss: 0.0490, G_loss: 3.6757\n",
      "Epoch [466/800], Step [20/53], D_loss: 0.0306, G_loss: 4.9813\n",
      "Epoch [466/800], Step [22/53], D_loss: 0.5086, G_loss: 1.1775\n",
      "Epoch [466/800], Step [24/53], D_loss: 0.0641, G_loss: 2.9264\n",
      "Epoch [466/800], Step [26/53], D_loss: 0.0183, G_loss: 3.5178\n",
      "Epoch [466/800], Step [28/53], D_loss: 0.1492, G_loss: 4.2299\n",
      "Epoch [466/800], Step [30/53], D_loss: 0.1520, G_loss: 2.3958\n",
      "Epoch [466/800], Step [32/53], D_loss: 0.0238, G_loss: 5.8570\n",
      "Epoch [466/800], Step [34/53], D_loss: 0.4316, G_loss: 2.4999\n",
      "Epoch [466/800], Step [36/53], D_loss: 0.0772, G_loss: 6.0315\n",
      "Epoch [466/800], Step [38/53], D_loss: 0.1401, G_loss: 5.2373\n",
      "Epoch [466/800], Step [40/53], D_loss: 0.1755, G_loss: 5.7673\n",
      "Epoch [466/800], Step [42/53], D_loss: 0.4691, G_loss: 5.4718\n",
      "Epoch [466/800], Step [44/53], D_loss: 0.0313, G_loss: 3.3239\n",
      "Epoch [466/800], Step [46/53], D_loss: 0.1041, G_loss: 5.0364\n",
      "Epoch [466/800], Step [48/53], D_loss: 0.0113, G_loss: 3.2276\n",
      "Epoch [466/800], Step [50/53], D_loss: 0.6134, G_loss: 8.0570\n",
      "Epoch [466/800], Step [52/53], D_loss: 0.0542, G_loss: 2.9431\n",
      "Epoch [466/800], Avg D_loss: 0.2062, Avg G_loss: 3.7428\n",
      "Epoch [467/800], Step [2/53], D_loss: 0.8285, G_loss: 8.6555\n",
      "Epoch [467/800], Step [4/53], D_loss: 0.5453, G_loss: 4.9874\n",
      "Epoch [467/800], Step [6/53], D_loss: 0.0958, G_loss: 1.5911\n",
      "Epoch [467/800], Step [8/53], D_loss: 0.5456, G_loss: 5.2983\n",
      "Epoch [467/800], Step [10/53], D_loss: 0.0666, G_loss: 5.2428\n",
      "Epoch [467/800], Step [12/53], D_loss: 0.0152, G_loss: 2.7659\n",
      "Epoch [467/800], Step [14/53], D_loss: 0.0419, G_loss: 4.2045\n",
      "Epoch [467/800], Step [16/53], D_loss: 0.0186, G_loss: 2.8495\n",
      "Epoch [467/800], Step [18/53], D_loss: 0.1367, G_loss: 2.3320\n",
      "Epoch [467/800], Step [20/53], D_loss: 0.0715, G_loss: 4.3979\n",
      "Epoch [467/800], Step [22/53], D_loss: 0.0108, G_loss: 2.4208\n",
      "Epoch [467/800], Step [24/53], D_loss: 0.1076, G_loss: 1.8093\n",
      "Epoch [467/800], Step [26/53], D_loss: 0.1124, G_loss: 6.9061\n",
      "Epoch [467/800], Step [28/53], D_loss: 0.1070, G_loss: 2.9204\n"
     ]
    }
   ],
   "source": [
    "# Load the last saved models\n",
    "last_epoch = 300 # Replace with the epoch number of your last saved model\n",
    "generator, discriminator = load_models(generator, discriminator, last_epoch)\n",
    "\n",
    "# Reset optimizers\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "\n",
    "# Continue training\n",
    "for epoch in range(last_epoch, last_epoch + num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for batch_idx, (real_images, input_ids, attention_mask) in enumerate(train_loader):\n",
    "        real_images, input_ids, attention_mask = real_images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        d_loss = discriminator_train_step(real_images.size(0), discriminator, generator, d_optimizer, criterion, real_images, input_ids, attention_mask)\n",
    "        g_loss = generator_train_step(real_images.size(0), discriminator, generator, g_optimizer, criterion, input_ids, attention_mask)\n",
    "\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        if (batch_idx + 1) % 2 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{last_epoch + num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], D_loss: {d_loss:.4f}, G_loss: {g_loss:.4f}\")\n",
    "\n",
    "    avg_d_loss = sum(d_losses) / len(d_losses)\n",
    "    avg_g_loss = sum(g_losses) / len(g_losses)\n",
    "    log_metrics(epoch + 1, avg_d_loss, avg_g_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{last_epoch + num_epochs}], Avg D_loss: {avg_d_loss:.4f}, Avg G_loss: {avg_g_loss:.4f}\")\n",
    "\n",
    "    # Generate and save images after every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            batch_size = 64\n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            text = [\"A healthy rice plant\"] * batch_size\n",
    "            input_ids = dataset.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt').input_ids.to(device)\n",
    "            attention_mask = (input_ids != dataset.tokenizer.pad_token_id).float().to(device)\n",
    "            text_embedding = discriminator.text_encoder(input_ids, attention_mask)\n",
    "            generated_images = generator(z, text_embedding)\n",
    "            save_image(generated_images, f\"generated_images/epoch_{epoch+1}.png\", nrow=8, normalize=True)\n",
    "            print(f\"Generated images saved as generated_images/epoch_{epoch+1}.png\")\n",
    "\n",
    "    # Save models every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        save_models(epoch + 1, generator, discriminator)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
